LLMUNITY: 

LLM for Unity enables seamless integration of Large Language Models (LLMs) within the Unity engine.
It allows to create intelligent characters that your players can interact with for an immersive experience.
The package also features a Retrieval-Augmented Generation (RAG) system that allows to performs semantic search across your data, which can be used to enhance the character's knowledge. LLM for Unity is built on top of the awesome llama.cpp library.

At a glance  ‚Ä¢  How to help  ‚Ä¢  Games / Projects using LLM for Unity  ‚Ä¢  Setup  ‚Ä¢  How to use  ‚Ä¢  RAG  ‚Ä¢  LLM model management  ‚Ä¢  Examples  ‚Ä¢  Options  ‚Ä¢  License
At a glance
üíª Cross-platform! Windows, Linux, macOS, iOS, Android and VisionOS
üè† Runs locally without internet access. No data ever leave the game!
‚ö° Blazing fast inference on CPU and GPU (Nvidia, AMD, Apple Metal)
ü§ó Supports all major LLM models
üîß Easy to setup, call with a single line of code
üí∞ Free to use for both personal and commercial purposes
üß™ Tested on Unity: 2021 LTS, 2022 LTS, 2023, Unity 6
üö¶ Upcoming Releases

How to help
‚≠ê Star the repo, leave us a review and spread the word about the project!
Join us at Discord and say hi.
Contribute by submitting feature requests, bugs or even your own PR.
 this work to allow even cooler features!
Games / Projects using LLM for Unity
Verbal Verdict
I, Chatbot: AISYLUM
Nameless Souls of the Void
Murder in Aisle 4
Finicky Food Delivery AI
AI Emotional Girlfriend
Case Closed
MaiMai AI Agent System
Contact us to add your project!

Setup
Method 1: Install using the asset store

Open the LLM for Unity asset page and click Add to My Assets
Open the Package Manager in Unity: Window > Package Manager
Select the Packages: My Assets option from the drop-down
Select the LLM for Unity package, click Download and then Import
Method 2: Install using the GitHub repo:

Open the Package Manager in Unity: Window > Package Manager
Click the + button and select Add package from git URL
Use the repository URL https://github.com/undreamai/LLMUnity.git and click Add
How to use


First you will setup the LLM for your game üèé:

Create an empty GameObject.
In the GameObject Inspector click Add Component and select the LLM script.
Download one of the default models with the Download Model button (~GBs).
Or load your own .gguf model with the Load model button (see LLM model management).
Then you can setup each of your characters as follows üôã‚Äç‚ôÄÔ∏è:

Create an empty GameObject for the character.
In the GameObject Inspector click Add Component and select the LLMCharacter script.
Define the role of your AI in the Prompt. You can define the name of the AI (AI Name) and the player (Player Name).
(Optional) Select the LLM constructed above in the LLM field if you have more than one LLM GameObjects.
You can also adjust the LLM and character settings according to your preference (see Options).

In your script you can then use it as follows ü¶Ñ:

using LLMUnity;

public class MyScript {
  public LLMCharacter llmCharacter;
  
  void HandleReply(string reply){
    // do something with the reply from the model
    Debug.Log(reply);
  }
  
  void Game(){
    // your game function
    ...
    string message = "Hello bot!";
    _ = llmCharacter.Chat(message, HandleReply);
    ...
  }
}
You can also specify a function to call when the model reply has been completed.
This is useful if the Stream option is enabled for continuous output from the model (default behaviour):

  void ReplyCompleted(){
    // do something when the reply from the model is complete
    Debug.Log("The AI replied");
  }
  
  void Game(){
    // your game function
    ...
    string message = "Hello bot!";
    _ = llmCharacter.Chat(message, HandleReply, ReplyCompleted);
    ...
  }
To stop the chat without waiting for its completion you can use:

    llmCharacter.CancelRequests();
Finally, in the Inspector of the GameObject of your script, select the LLMCharacter GameObject created above as the llmCharacter property.
That's all ‚ú®!

You can also:

Build a mobile app
Restrict the output of the LLM / Function calling
Access / Save / Load your chat history
Process the prompt at the beginning of your app for faster initial processing time
Decide whether or not to add the message to the chat/prompt history
Use pure text completion
Wait for the reply before proceeding to the next lines of code
Add a LLM / LLMCharacter component programmatically
Use a remote server
Compute embeddings using a LLM
A detailed documentation on function level can be found here: 

Semantic search with a Retrieval-Augmented Generation (RAG) system
LLM for Unity implements a super-fast similarity search functionality with a Retrieval-Augmented Generation (RAG) system.
It is based on the LLM functionality, and the Approximate Nearest Neighbors (ANN) search from the usearch library.
Semantic search works as follows.

Building the data You provide text inputs (a phrase, paragraph, document) to add to the data.
Each input is split into chunks (optional) and encoded into embeddings with a LLM.

Searching You can then search for a query text input.
The input is again encoded and the most similar text inputs or chunks in the data are retrieved.

To use semantic serch:

create a GameObject for the LLM as described above. Download one of the provided RAG models or load your own (good options can be found at the MTEB leaderboard).
create an empty GameObject. In the GameObject Inspector click Add Component and select the RAG script.
In the Search Type dropdown of the RAG select your preferred search method. SimpleSearch is a simple brute-force search, whileDBSearch is a fast ANN method that should be preferred in most cases.
In the Chunking Type dropdown of the RAG you can select a method for splitting the inputs into chunks. This is useful to have a more consistent meaning within each data part. Chunking methods for splitting according to tokens, words and sentences are provided.
Alternatively, you can create the RAG from code (where llm is your LLM):

  RAG rag = gameObject.AddComponent<RAG>();
  rag.Init(SearchMethods.DBSearch, ChunkingMethods.SentenceSplitter, llm);
In your script you can then use it as follows ü¶Ñ:

using LLMUnity;

public class MyScript : MonoBehaviour
{
  RAG rag;

  async void Game(){
    ...
    string[] inputs = new string[]{
      "Hi! I'm a search system.",
      "the weather is nice. I like it.",
      "I'm a RAG system"
    };
    // add the inputs to the RAG
    foreach (string input in inputs) await rag.Add(input);
    // get the 2 most similar inputs and their distance (dissimilarity) to the search query
    (string[] results, float[] distances) = await rag.Search("hello!", 2);
    // to get the most similar text parts (chnuks) you can enable the returnChunks option
    rag.ReturnChunks(true);
    (results, distances) = await rag.Search("hello!", 2);
    ...
  }
}
You can also add / search text inputs for groups of data e.g. for a specific character or scene:

    // add the inputs to the RAG for a group of data e.g. an orc character
    foreach (string input in inputs) await rag.Add(input, "orc");
    // get the 2 most similar inputs for the group of data e.g. the orc character
    (string[] results, float[] distances) = await rag.Search("how do you feel?", 2, "orc");
...

You can save the RAG state (stored in the `Assets/StreamingAssets` folder):
``` c#
rag.Save("rag.zip");
and load it from disk:

await rag.Load("rag.zip");
You can use the RAG to feed relevant data to the LLM based on a user message:

  string message = "How is the weather?";
  (string[] similarPhrases, float[] distances) = await rag.Search(message, 3);

  string prompt = "Answer the user query based on the provided data.\n\n";
  prompt += $"User query: {message}\n\n";
  prompt += $"Data:\n";
  foreach (string similarPhrase in similarPhrases) prompt += $"\n- {similarPhrase}";

  _ = llmCharacter.Chat(prompt, HandleReply, ReplyCompleted);
The RAG sample includes an example RAG implementation as well as an example RAG-LLM integration.

That's all ‚ú®!

LLM model management
LLM for Unity uses a model manager that allows to load or download LLMs and ship them directly in your game.
The model manager can be found as part of the LLM GameObject:


You can download models with the Download model button.
LLM for Unity includes different state of the art models built-in for different model sizes, quantised with the Q4_K_M method.
Alternative models can be downloaded from HuggingFace in the .gguf format.
You can download a model locally and load it with the Load model button, or copy the URL in the Download model > Custom URL field to directly download it.
If a HuggingFace model does not provide a gguf file, it can be converted to gguf with this online converter.

The chat template used for constructing the prompts is determined automatically from the model (if a relevant entry exists) or the model name.
If incorrecly identified, you can select another template from the chat template dropdown.

Models added in the model manager are copied to the game during the building process.
You can omit a model from being built in by deselecting the "Build" checkbox.
To remove the model (but not delete it from disk) you can click the bin button.
The the path and URL (if downloaded) of each added model is diplayed in the expanded view of the model manager access with the >> button:


You can create lighter builds by selecting the Download on Build option.
Using this option the models will be downloaded the first time the game starts instead of copied in the build.
If you have loaded a model locally you need to set its URL through the expanded view, otherwise it will be copied in the build.

‚ùï Before using any model make sure you check their license ‚ùï

Examples
The Samples~ folder contains several examples of interaction ü§ñ:

SimpleInteraction: Simple interaction with an AI character
MultipleCharacters: Simple interaction using multiple AI characters
FunctionCalling: Function calling sample with structured output from the LLM
RAG: Semantic search using a Retrieval Augmented Generation (RAG) system. Includes example using a RAG to feed information to a LLM
MobileDemo: Example mobile app for Android / iOS with an initial screen displaying the model download progress
ChatBot: Interaction between a player and a AI with a UI similar to a messaging app (see image below)
KnowledgeBaseGame: Simple detective game using a knowledge base to provide information to the LLM based on google/mysteryofthreebots
demo.gif


To install a sample:

Open the Package Manager: Window > Package Manager
Select the LLM for Unity Package. From the Samples Tab, click Import next to the sample you want to install.
The samples can be run with the Scene.unity scene they contain inside their folder.
In the scene, select the LLM GameObject and click the Download Model button to download a default model or Load model to load your own model (see LLM model management).
Save the scene, run and enjoy!

Options
Details on the different parameters are provided as Unity Tooltips. Previous documentation can be found here (deprecated).

License
The license of LLM for Unity is MIT (LICENSE.md) and uses third-party software with MIT and Apache licenses. Some models included in the asset define their own license terms, please review them before using each model. Third-party licenses can be found in the (Third Party Notices.md).


# -------------------- Chunking.cs --------------------

/// @file
/// @brief File implementing the chunking functionality
using System;
using System.Collections.Generic;
using System.IO.Compression;
using System.Linq;
using System.Text;
using System.Threading.Tasks;

namespace LLMUnity
{
    /// @ingroup rag
    /// <summary>
    /// Class implementing the chunking functionality
    /// </summary>
    [Serializable]
    public abstract class Chunking : SearchPlugin
    {
        protected bool returnChunks = false;
        protected Dictionary<string, List<int>> dataSplitToPhrases = new Dictionary<string, List<int>>();
        protected Dictionary<int, int[]> phraseToSentences = new Dictionary<int, int[]>();
        protected Dictionary<int, int> sentenceToPhrase = new Dictionary<int, int>();
        protected Dictionary<int, int[]> hexToPhrase = new Dictionary<int, int[]>();
        protected int nextKey = 0;

        /// <summary>
        /// Set to true to return chunks or the direct input with the Search function
        /// </summary>
        /// <param name="returnChunks">whether to return chunks</param>
        public void ReturnChunks(bool returnChunks)
        {
            this.returnChunks = returnChunks;
        }

        /// <summary>
        /// Splits the provided phrase into chunks
        /// </summary>
        /// <param name="input">phrase</param>
        /// <returns>List of start/end indices of the split chunks</returns>
        public abstract Task<List<(int, int)>> Split(string input);

        /// <summary>
        /// Retrieves the phrase with the specific id
        /// </summary>
        /// <param name="key">phrase id</param>
        /// <returns>phrase</returns>
        public override string Get(int key)
        {
            StringBuilder phraseBuilder = new StringBuilder();
            foreach (int sentenceId in phraseToSentences[key])
            {
                phraseBuilder.Append(search.Get(sentenceId));
            }
            return phraseBuilder.ToString();
        }

        /// <summary>
        /// Adds a phrase to the search after splitting it into chunks.
        /// </summary>
        /// <param name="inputString">input phrase</param>
        /// <param name="group">data group to add it to </param>
        /// <returns>phrase id</returns>
        public override async Task<int> Add(string inputString, string group = "")
        {
            int key = nextKey++;
            // sentence -> phrase
            List<int> sentenceIds = new List<int>();
            foreach ((int startIndex, int endIndex) in await Split(inputString))
            {
                string sentenceText = inputString.Substring(startIndex, endIndex - startIndex + 1);
                int sentenceId = await search.Add(sentenceText, group);
                sentenceIds.Add(sentenceId);

                sentenceToPhrase[sentenceId] = key;
            }
            // phrase -> sentence
            phraseToSentences[key] = sentenceIds.ToArray();

            // data split -> phrase
            if (!dataSplitToPhrases.ContainsKey(group)) dataSplitToPhrases[group] = new List<int>(){key};
            else dataSplitToPhrases[group].Add(key);

            // hex -> phrase
            int hash = inputString.GetHashCode();
            if (!hexToPhrase.TryGetValue(hash, out int[] entries)) entries = new int[0];
            List<int> matchingHash = new List<int>(entries);
            matchingHash.Add(key);

            hexToPhrase[hash] = matchingHash.ToArray();
            return key;
        }

        /// <summary>
        /// Removes a phrase and the phrase chunks from the search
        /// </summary>
        /// <param name="key">phrase id</param>
        public override void Remove(int key)
        {
            if (!phraseToSentences.TryGetValue(key, out int[] sentenceIds)) return;
            int hash = Get(key).GetHashCode();

            // phrase -> sentence
            phraseToSentences.Remove(key);
            foreach (int sentenceId in sentenceIds)
            {
                search.Remove(sentenceId);
                // sentence -> phrase
                sentenceToPhrase.Remove(sentenceId);
            }

            // data split -> phrase
            foreach (var dataSplitPhrases in dataSplitToPhrases.Values) dataSplitPhrases.Remove(key);

            // hex -> phrase
            if (hexToPhrase.TryGetValue(hash, out int[] phraseIds))
            {
                List<int> updatedIds = phraseIds.ToList();
                updatedIds.Remove(key);
                if (updatedIds.Count == 0) hexToPhrase.Remove(hash);
                else hexToPhrase[hash] = updatedIds.ToArray();
            }
        }

        /// <summary>
        /// Removes a phrase and the phrase chunks from the search.
        /// </summary>
        /// <param name="inputString">input phrase</param>
        /// <param name="group">data group to remove it from </param>
        /// <returns>number of removed phrases</returns>
        public override int Remove(string inputString, string group = "")
        {
            int hash = inputString.GetHashCode();
            if (!hexToPhrase.TryGetValue(hash, out int[] entries)) return 0;
            List<int> removeIds = new List<int>();
            foreach (int key in entries)
            {
                if (dataSplitToPhrases[group].Contains(key) && Get(key) == inputString) removeIds.Add(key);
            }
            foreach (int removeId in removeIds) Remove(removeId);
            return removeIds.Count;
        }

        /// <summary>
        /// Returns a count of the phrases
        /// </summary>
        /// <returns>phrase count</returns>
        public override int Count()
        {
            return phraseToSentences.Count;
        }

        /// <summary>
        /// Returns a count of the phrases in a specific data group
        /// </summary>
        /// <param name="group">data group</param>
        /// <returns>phrase count</returns>
        public override int Count(string group)
        {
            if (!dataSplitToPhrases.TryGetValue(group, out List<int> dataSplitPhrases)) return 0;
            return dataSplitPhrases.Count;
        }

        /// <summary>
        /// Allows to do search and retrieve results in batches (incremental search).
        /// </summary>
        /// <param name="queryString">search query</param>
        /// <param name="group">data group to search in</param>
        /// <returns>incremental search key</returns>
        public override async Task<int> IncrementalSearch(string queryString, string group = "")
        {
            return await search.IncrementalSearch(queryString, group);
        }

        /// <summary>
        /// Retrieves the most similar search results in batches (incremental search).
        /// The phrase/chunk keys and distances are retrieved, as well as a parameter that dictates whether the search is exhausted.
        /// The returnChunks variable defines whether to return chunks or phrases.
        /// </summary>
        /// <param name="fetchKey">incremental search key</param>
        /// <param name="k">number of results to retrieve</param>
        /// <returns>
        /// A tuple containing:
        /// <list type="bullet">
        /// <item><description>Array of retrieved keys (`int[]`).</description></item>
        /// <item><description>Array of distances for each result (`float[]`).</description></item>
        /// <item><description>`bool` indicating if the search is exhausted.</description></item>
        /// </list>
        /// </returns>
        public override ValueTuple<int[], float[], bool> IncrementalFetchKeys(int fetchKey, int k)
        {
            if (returnChunks)
            {
                return search.IncrementalFetchKeys(fetchKey, k);
            }
            else
            {
                List<int> phraseKeys = new List<int>();
                List<float> distancesList = new List<float>();
                bool done = false;
                bool completed;
                do
                {
                    int[] resultKeys;
                    float[] distancesIter;
                    (resultKeys, distancesIter, completed) = search.IncrementalFetchKeys(fetchKey, k);
                    for (int i = 0; i < resultKeys.Length; i++)
                    {
                        int phraseId = sentenceToPhrase[resultKeys[i]];
                        if (phraseKeys.Contains(phraseId)) continue;
                        phraseKeys.Add(phraseId);
                        distancesList.Add(distancesIter[i]);
                        if (phraseKeys.Count() == k)
                        {
                            done = true;
                            break;
                        }
                    }
                    if (completed) break;
                }
                while (!done);
                if (completed) IncrementalSearchComplete(fetchKey);
                return (phraseKeys.ToArray(), distancesList.ToArray(), completed);
            }
        }

        /// <summary>
        /// Retrieves the most similar search results in batches (incremental search).
        /// The phrases/chunks and their distances are retrieved, as well as a parameter that dictates whether the search is exhausted.
        /// The returnChunks variable defines whether to return chunks or phrases.
        /// </summary>
        /// <param name="fetchKey">incremental search key</param>
        /// <param name="k">number of results to retrieve</param>
        /// <returns>
        /// A tuple containing:
        /// <list type="bullet">
        /// <item><description>Array of retrieved phrases/chunks (`string[]`).</description></item>
        /// <item><description>Array of distances for each result (`float[]`).</description></item>
        /// <item><description>`bool` indicating if the search is exhausted.</description></item>
        /// </list>
        /// </returns>
        public override ValueTuple<string[], float[], bool> IncrementalFetch(int fetchKey, int k)
        {
            (int[] resultKeys, float[] distances, bool completed) = IncrementalFetchKeys(fetchKey, k);
            string[] results = new string[resultKeys.Length];
            for (int i = 0; i < resultKeys.Length; i++)
            {
                if (returnChunks) results[i] = search.Get(resultKeys[i]);
                else results[i] = Get(resultKeys[i]);
            }
            return (results, distances, completed);
        }

        /// <summary>
        /// Completes the search and clears the cached results for an incremental search
        /// </summary>
        /// <param name="fetchKey">incremental search key</param>
        public override void IncrementalSearchComplete(int fetchKey)
        {
            search.IncrementalSearchComplete(fetchKey);
        }

        /// <summary>
        /// Clears the object and the associated search object
        /// </summary>
        public override void Clear()
        {
            nextKey = 0;
            dataSplitToPhrases.Clear();
            phraseToSentences.Clear();
            sentenceToPhrase.Clear();
            hexToPhrase.Clear();
            search.Clear();
        }

        protected override void SaveInternal(ZipArchive archive)
        {
            ArchiveSaver.Save(archive, dataSplitToPhrases, GetSavePath("dataSplitToPhrases"));
            ArchiveSaver.Save(archive, phraseToSentences, GetSavePath("phraseToSentences"));
            ArchiveSaver.Save(archive, sentenceToPhrase, GetSavePath("sentenceToPhrase"));
            ArchiveSaver.Save(archive, hexToPhrase, GetSavePath("hexToPhrase"));
            ArchiveSaver.Save(archive, nextKey, GetSavePath("nextKey"));
        }

        protected override void LoadInternal(ZipArchive archive)
        {
            dataSplitToPhrases = ArchiveSaver.Load<Dictionary<string, List<int>>>(archive, GetSavePath("dataSplitToPhrases"));
            phraseToSentences = ArchiveSaver.Load<Dictionary<int, int[]>>(archive, GetSavePath("phraseToSentences"));
            sentenceToPhrase = ArchiveSaver.Load<Dictionary<int, int>>(archive, GetSavePath("sentenceToPhrase"));
            hexToPhrase = ArchiveSaver.Load<Dictionary<int, int[]>>(archive, GetSavePath("hexToPhrase"));
            nextKey = ArchiveSaver.Load<int>(archive, GetSavePath("nextKey"));
        }
    }
}


# -------------------- DBSearch.cs --------------------

/// @file
/// @brief File implementing the vector database search.
using System;
using System.Collections.Generic;
using Cloud.Unum.USearch;
using System.IO.Compression;
using UnityEngine;

namespace LLMUnity
{
    /// @ingroup rag
    /// <summary>
    /// Class implementing a search with a vector database.
    /// The search results are retrieved with Approximate Nearest Neighbor (ANN) which is much faster that SimpleSearch.
    /// </summary>
    [DefaultExecutionOrder(-2)]
    public class DBSearch : SearchMethod
    {
        protected USearchIndex index;
        /// <summary> show/hide advanced options in the GameObject </summary>
        [Tooltip("show/hide advanced options in the GameObject")]
        [HideInInspector] public bool advancedOptions = false;
        /// <summary> The quantisation type used for vector data during indexing. </summary>
        [Tooltip("The quantisation type used for vector data during indexing.")]
        [ModelAdvanced] public ScalarKind quantization = ScalarKind.Float16;
        /// <summary> The metric kind used for distance calculation between vectors. </summary>
        [Tooltip("The metric kind used for distance calculation between vectors.")]
        [ModelAdvanced] public MetricKind metricKind = MetricKind.Cos;
        /// <summary> The connectivity parameter limits the connections-per-node in the graph. </summary>
        [Tooltip("The connectivity parameter limits the connections-per-node in the graph.")]
        [ModelAdvanced] public ulong connectivity = 32;
        /// <summary> The expansion factor used for index construction when adding vectors. </summary>
        [Tooltip("The expansion factor used for index construction when adding vectors.")]
        [ModelAdvanced] public ulong expansionAdd = 40;
        /// <summary> The expansion factor used for index construction during search operations. </summary>
        [Tooltip("The expansion factor used for index construction during search operations.")]
        [ModelAdvanced] public ulong expansionSearch = 16;

        private Dictionary<int, (float[], string, List<int>)> incrementalSearchCache = new Dictionary<int, (float[], string, List<int>)>();

        /// \cond HIDE
        public new void Awake()
        {
            base.Awake();
            if (!enabled) return;
            InitIndex();
        }

        public void InitIndex()
        {
            index = new USearchIndex(metricKind, quantization, (ulong)llmEmbedder.llm.embeddingLength, connectivity, expansionAdd, expansionSearch, false);
        }

        protected override void AddInternal(int key, float[] embedding)
        {
            index.Add((ulong)key, embedding);
        }

        protected override void RemoveInternal(int key)
        {
            index.Remove((ulong)key);
        }

        protected int[] UlongToInt(ulong[] keys)
        {
            int[] intKeys = new int[keys.Length];
            for (int i = 0; i < keys.Length; i++) intKeys[i] = (int)keys[i];
            return intKeys;
        }

        public override int IncrementalSearch(float[] embedding, string group = "")
        {
            int key = nextIncrementalSearchKey++;
            incrementalSearchCache[key] = (embedding, group, new List<int>());
            return key;
        }

        public override ValueTuple<int[], float[], bool> IncrementalFetchKeys(int fetchKey, int k)
        {
            if (!incrementalSearchCache.ContainsKey(fetchKey)) throw new Exception($"There is no IncrementalSearch cached with this key: {fetchKey}");

            (float[] embedding, string group, List<int> seenKeys) = incrementalSearchCache[fetchKey];

            if (!dataSplits.TryGetValue(group, out List<int> dataSplit)) return (new int[0], new float[0], true);
            if (dataSplit.Count == 0) return (new int[0], new float[0], true);

            Func<int, int> filter = (int key) => !dataSplit.Contains(key) || seenKeys.Contains(key) ? 0 : 1;
            index.Search(embedding, k, out ulong[] keys, out float[] distances, filter);
            int[] intKeys = UlongToInt(keys);
            incrementalSearchCache[fetchKey].Item3.AddRange(intKeys);

            bool completed = intKeys.Length < k || seenKeys.Count == Count(group);
            if (completed) IncrementalSearchComplete(fetchKey);
            return (intKeys, distances, completed);
        }

        public override void IncrementalSearchComplete(int fetchKey)
        {
            incrementalSearchCache.Remove(fetchKey);
        }

        protected override void SaveInternal(ZipArchive archive)
        {
            index.Save(archive);
        }

        protected override void LoadInternal(ZipArchive archive)
        {
            index.Load(archive);
        }

        protected override void ClearInternal()
        {
            index.Dispose();
            InitIndex();
            incrementalSearchCache.Clear();
        }

        /// \endcond
    }
}


# -------------------- LLM.cs --------------------

/// @file
/// @brief File implementing the LLM.
using System;
using System.Collections.Generic;
using System.IO;
using System.Threading;
using System.Threading.Tasks;
using UnityEditor;
using UnityEngine;

namespace LLMUnity
{
    [DefaultExecutionOrder(-1)]
    /// @ingroup llm
    /// <summary>
    /// Class implementing the LLM server.
    /// </summary>
    public class LLM : MonoBehaviour
    {
        /// <summary> show/hide advanced options in the GameObject </summary>
        [Tooltip("show/hide advanced options in the GameObject")]
        [HideInInspector] public bool advancedOptions = false;
        /// <summary> enable remote server functionality </summary>
        [Tooltip("enable remote server functionality")]
        [LocalRemote] public bool remote = false;
        /// <summary> port to use for the remote LLM server </summary>
        [Tooltip("port to use for the remote LLM server")]
        [Remote] public int port = 13333;
        /// <summary> number of threads to use (-1 = all) </summary>
        [Tooltip("number of threads to use (-1 = all)")]
        [LLM] public int numThreads = -1;
        /// <summary> number of model layers to offload to the GPU (0 = GPU not used).
        /// If the user's GPU is not supported, the LLM will fall back to the CPU </summary>
        [Tooltip("number of model layers to offload to the GPU (0 = GPU not used). If the user's GPU is not supported, the LLM will fall back to the CPU")]
        [LLM] public int numGPULayers = 0;
        /// <summary> log the output of the LLM in the Unity Editor. </summary>
        [Tooltip("log the output of the LLM in the Unity Editor.")]
        [LLM] public bool debug = false;
        /// <summary> number of prompts that can happen in parallel (-1 = number of LLMCaller objects) </summary>
        [Tooltip("number of prompts that can happen in parallel (-1 = number of LLMCaller objects)")]
        [LLMAdvanced] public int parallelPrompts = -1;
        /// <summary> do not destroy the LLM GameObject when loading a new Scene. </summary>
        [Tooltip("do not destroy the LLM GameObject when loading a new Scene.")]
        [LLMAdvanced] public bool dontDestroyOnLoad = true;
        /// <summary> Size of the prompt context (0 = context size of the model).
        /// This is the number of tokens the model can take as input when generating responses. </summary>
        [Tooltip("Size of the prompt context (0 = context size of the model). This is the number of tokens the model can take as input when generating responses.")]
        [DynamicRange("minContextLength", "maxContextLength", false), Model] public int contextSize = 8192;
        /// <summary> Batch size for prompt processing. </summary>
        [Tooltip("Batch size for prompt processing.")]
        [ModelAdvanced] public int batchSize = 512;
        /// <summary> Boolean set to true if the server has started and is ready to receive requests, false otherwise. </summary>
        public bool started { get; protected set; } = false;
        /// <summary> Boolean set to true if the server has failed to start. </summary>
        public bool failed { get; protected set; } = false;
        /// <summary> Boolean set to true if the models were not downloaded successfully. </summary>
        public static bool modelSetupFailed { get; protected set; } = false;
        /// <summary> Boolean set to true if the server has started and is ready to receive requests, false otherwise. </summary>
        public static bool modelSetupComplete { get; protected set; } = false;
        /// <summary> LLM model to use (.gguf format) </summary>
        [Tooltip("LLM model to use (.gguf format)")]
        [ModelAdvanced] public string model = "";
        /// <summary> Chat template for the model </summary>
        [Tooltip("Chat template for the model")]
        [ModelAdvanced] public string chatTemplate = ChatTemplate.DefaultTemplate;
        /// <summary> LORA models to use (.gguf format) </summary>
        [Tooltip("LORA models to use (.gguf format)")]
        [ModelAdvanced] public string lora = "";
        /// <summary> the weights of the LORA models being used.</summary>
        [Tooltip("the weights of the LORA models being used.")]
        [ModelAdvanced] public string loraWeights = "";
        /// <summary> enable use of flash attention </summary>
        [Tooltip("enable use of flash attention")]
        [ModelExtras] public bool flashAttention = false;
        /// <summary> API key to use for the server </summary>
        [Tooltip("API key to use for the server")]
        public string APIKey;

        // SSL certificate
        [SerializeField]
        private string SSLCert = "";
        public string SSLCertPath = "";
        // SSL key
        [SerializeField]
        private string SSLKey = "";
        public string SSLKeyPath = "";

        /// \cond HIDE
        public int minContextLength = 0;
        public int maxContextLength = 0;
        public string architecture => llmlib.architecture;

        IntPtr LLMObject = IntPtr.Zero;
        List<LLMCaller> clients = new List<LLMCaller>();
        LLMLib llmlib;
        StreamWrapper logStreamWrapper = null;
        Thread llmThread = null;
        List<StreamWrapper> streamWrappers = new List<StreamWrapper>();
        public LLMManager llmManager = new LLMManager();
        private readonly object startLock = new object();
        static readonly object staticLock = new object();
        public LoraManager loraManager = new LoraManager();
        string loraPre = "";
        string loraWeightsPre = "";
        public bool embeddingsOnly = false;
        public int embeddingLength = 0;

        /// \endcond

        public LLM()
        {
            LLMManager.Register(this);
        }

        void OnValidate()
        {
            if (lora != loraPre || loraWeights != loraWeightsPre)
            {
                loraManager.FromStrings(lora, loraWeights);
                (loraPre, loraWeightsPre) = (lora, loraWeights);
            }
        }

        /// <summary>
        /// The Unity Awake function that starts the LLM server.
        /// </summary>
        public async void Awake()
        {
            if (!enabled) return;
#if !UNITY_EDITOR
            modelSetupFailed = !await LLMManager.Setup();
#endif
            modelSetupComplete = true;
            if (modelSetupFailed)
            {
                failed = true;
                return;
            }
            string arguments = GetLlamaccpArguments();
            if (arguments == null)
            {
                failed = true;
                return;
            }
            await Task.Run(() => StartLLMServer(arguments));
            if (!started) return;
            if (dontDestroyOnLoad) DontDestroyOnLoad(transform.root.gameObject);
        }

        /// <summary>
        /// Allows to wait until the LLM is ready
        /// </summary>
        public async Task WaitUntilReady()
        {
            while (!started) await Task.Yield();
        }

        /// <summary>
        /// Allows to wait until the LLM models are downloaded and ready
        /// </summary>
        /// <param name="downloadProgressCallback">function to call with the download progress (float)</param>
        public static async Task<bool> WaitUntilModelSetup(Callback<float> downloadProgressCallback = null)
        {
            if (downloadProgressCallback != null) LLMManager.downloadProgressCallbacks.Add(downloadProgressCallback);
            while (!modelSetupComplete) await Task.Yield();
            return !modelSetupFailed;
        }

        /// \cond HIDE
        public static string GetLLMManagerAsset(string path)
        {
#if UNITY_EDITOR
            if (!EditorApplication.isPlaying) return GetLLMManagerAssetEditor(path);
#endif
            return GetLLMManagerAssetRuntime(path);
        }

        public static string GetLLMManagerAssetEditor(string path)
        {
            // empty
            if (string.IsNullOrEmpty(path)) return path;
            // LLMManager - return location the file will be stored in StreamingAssets
            ModelEntry modelEntry = LLMManager.Get(path);
            if (modelEntry != null) return modelEntry.filename;
            // StreamingAssets - return relative location within StreamingAssets
            string assetPath = LLMUnitySetup.GetAssetPath(path); // Note: this will return the full path if a full path is passed
            string basePath = LLMUnitySetup.GetAssetPath();
            if (File.Exists(assetPath))
            {
                if (LLMUnitySetup.IsSubPath(assetPath, basePath)) return LLMUnitySetup.RelativePath(assetPath, basePath);
            }
            // full path
            if (!File.Exists(assetPath))
            {
                LLMUnitySetup.LogError($"Model {path} was not found.");
            }
            else
            {
                string errorMessage = $"The model {path} was loaded locally. You can include it in the build in one of these ways:";
                errorMessage += $"\n-Copy the model inside the StreamingAssets folder and use its StreamingAssets path";
                errorMessage += $"\n-Load the model with the model manager inside the LLM GameObject and use its filename";
                LLMUnitySetup.LogWarning(errorMessage);
            }
            return path;
        }

        public static string GetLLMManagerAssetRuntime(string path)
        {
            // empty
            if (string.IsNullOrEmpty(path)) return path;
            // LLMManager
            string managerPath = LLMManager.GetAssetPath(path);
            if (!string.IsNullOrEmpty(managerPath) && File.Exists(managerPath)) return managerPath;
            // StreamingAssets
            string assetPath = LLMUnitySetup.GetAssetPath(path);
            if (File.Exists(assetPath)) return assetPath;
            // download path
            assetPath = LLMUnitySetup.GetDownloadAssetPath(path);
            if (File.Exists(assetPath)) return assetPath;
            // give up
            return path;
        }

        /// \endcond

        /// <summary>
        /// Allows to set the model used by the LLM.
        /// The model provided is copied to the Assets/StreamingAssets folder that allows it to also work in the build.
        /// Models supported are in .gguf format.
        /// </summary>
        /// <param name="path">path to model to use (.gguf format)</param>
        public void SetModel(string path)
        {
            model = GetLLMManagerAsset(path);
            if (!string.IsNullOrEmpty(model))
            {
                ModelEntry modelEntry = LLMManager.Get(model);
                if (modelEntry == null) modelEntry = new ModelEntry(GetLLMManagerAssetRuntime(model));
                SetTemplate(modelEntry.chatTemplate);

                maxContextLength = modelEntry.contextLength;
                if (contextSize > maxContextLength) contextSize = maxContextLength;
                SetEmbeddings(modelEntry.embeddingLength, modelEntry.embeddingOnly);
                if (contextSize == 0 && modelEntry.contextLength > 32768)
                {
                    LLMUnitySetup.LogWarning($"The model {path} has very large context size ({modelEntry.contextLength}), consider setting it to a smaller value (<=32768) to avoid filling up the RAM");
                }
            }
#if UNITY_EDITOR
            if (!EditorApplication.isPlaying) EditorUtility.SetDirty(this);
#endif
        }

        /// <summary>
        /// Allows to set a LORA model to use in the LLM.
        /// The model provided is copied to the Assets/StreamingAssets folder that allows it to also work in the build.
        /// Models supported are in .gguf format.
        /// </summary>
        /// <param name="path">path to LORA model to use (.gguf format)</param>
        public void SetLora(string path, float weight = 1)
        {
            AssertNotStarted();
            loraManager.Clear();
            AddLora(path, weight);
        }

        /// <summary>
        /// Allows to add a LORA model to use in the LLM.
        /// The model provided is copied to the Assets/StreamingAssets folder that allows it to also work in the build.
        /// Models supported are in .gguf format.
        /// </summary>
        /// <param name="path">path to LORA model to use (.gguf format)</param>
        public void AddLora(string path, float weight = 1)
        {
            AssertNotStarted();
            loraManager.Add(path, weight);
            UpdateLoras();
        }

        /// <summary>
        /// Allows to remove a LORA model from the LLM.
        /// Models supported are in .gguf format.
        /// </summary>
        /// <param name="path">path to LORA model to remove (.gguf format)</param>
        public void RemoveLora(string path)
        {
            AssertNotStarted();
            loraManager.Remove(path);
            UpdateLoras();
        }

        /// <summary>
        /// Allows to remove all LORA models from the LLM.
        /// </summary>
        public void RemoveLoras()
        {
            AssertNotStarted();
            loraManager.Clear();
            UpdateLoras();
        }

        /// <summary>
        /// Allows to change the weight (scale) of a LORA model in the LLM.
        /// </summary>
        /// <param name="path">path of LORA model to change (.gguf format)</param>
        /// <param name="weight">weight of LORA</param>
        public void SetLoraWeight(string path, float weight)
        {
            loraManager.SetWeight(path, weight);
            UpdateLoras();
            if (started) ApplyLoras();
        }

        /// <summary>
        /// Allows to change the weights (scale) of the LORA models in the LLM.
        /// </summary>
        /// <param name="loraToWeight">Dictionary (string, float) mapping the path of LORA models with weights to change</param>
        public void SetLoraWeights(Dictionary<string, float> loraToWeight)
        {
            foreach (KeyValuePair<string, float> entry in loraToWeight) loraManager.SetWeight(entry.Key, entry.Value);
            UpdateLoras();
            if (started) ApplyLoras();
        }

        public void UpdateLoras()
        {
            (lora, loraWeights) = loraManager.ToStrings();
            (loraPre, loraWeightsPre) = (lora, loraWeights);
#if UNITY_EDITOR
            if (!EditorApplication.isPlaying) EditorUtility.SetDirty(this);
#endif
        }

        /// <summary>
        /// Set the chat template for the LLM.
        /// </summary>
        /// <param name="templateName">the chat template to use. The available templates can be found in the ChatTemplate.templates.Keys array </param>
        public void SetTemplate(string templateName, bool setDirty = true)
        {
            chatTemplate = templateName;
            if (started) llmlib?.LLM_SetTemplate(LLMObject, chatTemplate);
#if UNITY_EDITOR
            if (setDirty && !EditorApplication.isPlaying) EditorUtility.SetDirty(this);
#endif
        }

        /// <summary>
        /// Set LLM Embedding parameters
        /// </summary>
        /// <param name="embeddingLength"> number of embedding dimensions </param>
        /// <param name="embeddingsOnly"> if true, the LLM will be used only for embeddings </param>
        public void SetEmbeddings(int embeddingLength, bool embeddingsOnly)
        {
            this.embeddingsOnly = embeddingsOnly;
            this.embeddingLength = embeddingLength;
#if UNITY_EDITOR
            if (!EditorApplication.isPlaying) EditorUtility.SetDirty(this);
#endif
        }

        /// \cond HIDE

        string ReadFileContents(string path)
        {
            if (String.IsNullOrEmpty(path)) return "";
            else if (!File.Exists(path))
            {
                LLMUnitySetup.LogError($"File {path} not found!");
                return "";
            }
            return File.ReadAllText(path);
        }

        /// \endcond

        /// <summary>
        /// Use a SSL certificate for the LLM server.
        /// </summary>
        /// <param name="templateName">the SSL certificate path </param>
        public void SetSSLCert(string path)
        {
            SSLCertPath = path;
            SSLCert = ReadFileContents(path);
        }

        /// <summary>
        /// Use a SSL key for the LLM server.
        /// </summary>
        /// <param name="templateName">the SSL key path </param>
        public void SetSSLKey(string path)
        {
            SSLKeyPath = path;
            SSLKey = ReadFileContents(path);
        }

        /// <summary>
        /// Returns the chat template of the LLM.
        /// </summary>
        /// <returns>chat template of the LLM</returns>
        public string GetTemplate()
        {
            return chatTemplate;
        }

        protected virtual string GetLlamaccpArguments()
        {
            // Start the LLM server in a cross-platform way
            if ((SSLCert != "" && SSLKey == "") || (SSLCert == "" && SSLKey != ""))
            {
                LLMUnitySetup.LogError($"Both SSL certificate and key need to be provided!");
                return null;
            }

            if (model == "")
            {
                LLMUnitySetup.LogError("No model file provided!");
                return null;
            }
            string modelPath = GetLLMManagerAssetRuntime(model);
            if (!File.Exists(modelPath))
            {
                LLMUnitySetup.LogError($"File {modelPath} not found!");
                return null;
            }

            loraManager.FromStrings(lora, loraWeights);
            string loraArgument = "";
            foreach (string lora in loraManager.GetLoras())
            {
                string loraPath = GetLLMManagerAssetRuntime(lora);
                if (!File.Exists(loraPath))
                {
                    LLMUnitySetup.LogError($"File {loraPath} not found!");
                    return null;
                }
                loraArgument += $" --lora \"{loraPath}\"";
            }

            int numThreadsToUse = numThreads;
            if (Application.platform == RuntimePlatform.Android && numThreads <= 0) numThreadsToUse = LLMUnitySetup.AndroidGetNumBigCores();

            int slots = GetNumClients();
            string arguments = $"-m \"{modelPath}\" -c {contextSize} -b {batchSize} --log-disable -np {slots}";
            if (embeddingsOnly) arguments += " --embedding";
            if (numThreadsToUse > 0) arguments += $" -t {numThreadsToUse}";
            arguments += loraArgument;
            if (numGPULayers > 0) arguments += $" -ngl {numGPULayers}";
            if (LLMUnitySetup.FullLlamaLib && flashAttention) arguments += $" --flash-attn";
            if (remote)
            {
                arguments += $" --port {port} --host 0.0.0.0";
                if (!String.IsNullOrEmpty(APIKey)) arguments += $" --api-key {APIKey}";
            }

            // the following is the equivalent for running from command line
            string serverCommand;
            if (Application.platform == RuntimePlatform.WindowsEditor || Application.platform == RuntimePlatform.WindowsPlayer) serverCommand = "undreamai_server.exe";
            else serverCommand = "./undreamai_server";
            serverCommand += " " + arguments;
            serverCommand += $" --template \"{chatTemplate}\"";
            if (remote && SSLCert != "" && SSLKey != "") serverCommand += $" --ssl-cert-file {SSLCertPath} --ssl-key-file {SSLKeyPath}";
            LLMUnitySetup.Log($"Deploy server command: {serverCommand}");
            return arguments;
        }

        private void SetupLogging()
        {
            logStreamWrapper = ConstructStreamWrapper(LLMUnitySetup.LogWarning, true);
            llmlib?.Logging(logStreamWrapper.GetStringWrapper());
        }

        private void StopLogging()
        {
            if (logStreamWrapper == null) return;
            llmlib?.StopLogging();
            DestroyStreamWrapper(logStreamWrapper);
        }

        private void StartLLMServer(string arguments)
        {
            started = false;
            failed = false;
            bool useGPU = numGPULayers > 0;

            foreach (string arch in LLMLib.PossibleArchitectures(useGPU))
            {
                string error;
                try
                {
                    InitLib(arch);
                    InitService(arguments);
                    LLMUnitySetup.Log($"Using architecture: {arch}");
                    break;
                }
                catch (LLMException e)
                {
                    error = e.Message;
                    Destroy();
                }
                catch (DestroyException)
                {
                    break;
                }
                catch (Exception e)
                {
                    error = $"{e.GetType()}: {e.Message}";
                }
                LLMUnitySetup.Log($"Tried architecture: {arch}, error: " + error);
            }
            if (llmlib == null)
            {
                LLMUnitySetup.LogError("LLM service couldn't be created");
                failed = true;
                return;
            }
            CallWithLock(StartService);
            LLMUnitySetup.Log("LLM service created");
        }

        private void InitLib(string arch)
        {
            llmlib = new LLMLib(arch);
            CheckLLMStatus(false);
        }

        void CallWithLock(EmptyCallback fn)
        {
            lock (startLock)
            {
                if (llmlib == null) throw new DestroyException();
                fn();
            }
        }

        private void InitService(string arguments)
        {
            lock (staticLock)
            {
                if (debug) CallWithLock(SetupLogging);
                CallWithLock(() => { LLMObject = llmlib.LLM_Construct(arguments); });
                CallWithLock(() => llmlib.LLM_SetTemplate(LLMObject, chatTemplate));
                if (remote)
                {
                    if (SSLCert != "" && SSLKey != "")
                    {
                        LLMUnitySetup.Log("Using SSL");
                        CallWithLock(() => llmlib.LLM_SetSSL(LLMObject, SSLCert, SSLKey));
                    }
                    CallWithLock(() => llmlib.LLM_StartServer(LLMObject));
                }
                CallWithLock(() => CheckLLMStatus(false));
            }
        }

        private void StartService()
        {
            llmThread = new Thread(() => llmlib.LLM_Start(LLMObject));
            llmThread.Start();
            while (!llmlib.LLM_Started(LLMObject)) {}
            ApplyLoras();
            started = true;
        }

        /// <summary>
        /// Registers a local LLMCaller object.
        /// This allows to bind the LLMCaller "client" to a specific slot of the LLM.
        /// </summary>
        /// <param name="llmCaller"></param>
        /// <returns></returns>
        public int Register(LLMCaller llmCaller)
        {
            clients.Add(llmCaller);
            int index = clients.IndexOf(llmCaller);
            if (parallelPrompts != -1) return index % parallelPrompts;
            return index;
        }

        protected int GetNumClients()
        {
            return Math.Max(parallelPrompts == -1 ? clients.Count : parallelPrompts, 1);
        }

        /// \cond HIDE
        public delegate void LLMStatusCallback(IntPtr LLMObject, IntPtr stringWrapper);
        public delegate void LLMNoInputReplyCallback(IntPtr LLMObject, IntPtr stringWrapper);
        public delegate void LLMReplyCallback(IntPtr LLMObject, string json_data, IntPtr stringWrapper);
        /// \endcond

        StreamWrapper ConstructStreamWrapper(Callback<string> streamCallback = null, bool clearOnUpdate = false)
        {
            StreamWrapper streamWrapper = new StreamWrapper(llmlib, streamCallback, clearOnUpdate);
            streamWrappers.Add(streamWrapper);
            return streamWrapper;
        }

        void DestroyStreamWrapper(StreamWrapper streamWrapper)
        {
            streamWrappers.Remove(streamWrapper);
            streamWrapper.Destroy();
        }

        /// <summary>
        /// The Unity Update function. It is used to retrieve the LLM replies.
        public void Update()
        {
            foreach (StreamWrapper streamWrapper in streamWrappers) streamWrapper.Update();
        }

        void AssertStarted()
        {
            string error = null;
            if (failed) error = "LLM service couldn't be created";
            else if (!started) error = "LLM service not started";
            if (error != null)
            {
                LLMUnitySetup.LogError(error);
                throw new Exception(error);
            }
        }

        void AssertNotStarted()
        {
            if (started)
            {
                string error = "This method can't be called when the LLM has started";
                LLMUnitySetup.LogError(error);
                throw new Exception(error);
            }
        }

        void CheckLLMStatus(bool log = true)
        {
            if (llmlib == null) { return; }
            IntPtr stringWrapper = llmlib.StringWrapper_Construct();
            int status = llmlib.LLM_Status(LLMObject, stringWrapper);
            string result = llmlib.GetStringWrapperResult(stringWrapper);
            llmlib.StringWrapper_Delete(stringWrapper);
            string message = $"LLM {status}: {result}";
            if (status > 0)
            {
                if (log) LLMUnitySetup.LogError(message);
                throw new LLMException(message, status);
            }
            else if (status < 0)
            {
                if (log) LLMUnitySetup.LogWarning(message);
            }
        }

        async Task<string> LLMNoInputReply(LLMNoInputReplyCallback callback)
        {
            AssertStarted();
            IntPtr stringWrapper = llmlib.StringWrapper_Construct();
            await Task.Run(() => callback(LLMObject, stringWrapper));
            string result = llmlib?.GetStringWrapperResult(stringWrapper);
            llmlib?.StringWrapper_Delete(stringWrapper);
            CheckLLMStatus();
            return result;
        }

        async Task<string> LLMReply(LLMReplyCallback callback, string json)
        {
            AssertStarted();
            IntPtr stringWrapper = llmlib.StringWrapper_Construct();
            await Task.Run(() => callback(LLMObject, json, stringWrapper));
            string result = llmlib?.GetStringWrapperResult(stringWrapper);
            llmlib?.StringWrapper_Delete(stringWrapper);
            CheckLLMStatus();
            return result;
        }

        /// <summary>
        /// Tokenises the provided query.
        /// </summary>
        /// <param name="json">json request containing the query</param>
        /// <returns>tokenisation result</returns>
        public async Task<string> Tokenize(string json)
        {
            AssertStarted();
            LLMReplyCallback callback = (IntPtr LLMObject, string jsonData, IntPtr strWrapper) =>
            {
                llmlib.LLM_Tokenize(LLMObject, jsonData, strWrapper);
            };
            return await LLMReply(callback, json);
        }

        /// <summary>
        /// Detokenises the provided query.
        /// </summary>
        /// <param name="json">json request containing the query</param>
        /// <returns>detokenisation result</returns>
        public async Task<string> Detokenize(string json)
        {
            AssertStarted();
            LLMReplyCallback callback = (IntPtr LLMObject, string jsonData, IntPtr strWrapper) =>
            {
                llmlib.LLM_Detokenize(LLMObject, jsonData, strWrapper);
            };
            return await LLMReply(callback, json);
        }

        /// <summary>
        /// Computes the embeddings of the provided query.
        /// </summary>
        /// <param name="json">json request containing the query</param>
        /// <returns>embeddings result</returns>
        public async Task<string> Embeddings(string json)
        {
            AssertStarted();
            LLMReplyCallback callback = (IntPtr LLMObject, string jsonData, IntPtr strWrapper) =>
            {
                llmlib.LLM_Embeddings(LLMObject, jsonData, strWrapper);
            };
            return await LLMReply(callback, json);
        }

        /// <summary>
        /// Sets the lora scale, only works after the LLM service has started
        /// </summary>
        /// <returns>switch result</returns>
        public void ApplyLoras()
        {
            LoraWeightRequestList loraWeightRequest = new LoraWeightRequestList();
            loraWeightRequest.loraWeights = new List<LoraWeightRequest>();
            float[] weights = loraManager.GetWeights();
            if (weights.Length == 0) return;
            for (int i = 0; i < weights.Length; i++)
            {
                loraWeightRequest.loraWeights.Add(new LoraWeightRequest() { id = i, scale = weights[i] });
            }

            string json = JsonUtility.ToJson(loraWeightRequest);
            int startIndex = json.IndexOf("[");
            int endIndex = json.LastIndexOf("]") + 1;
            json = json.Substring(startIndex, endIndex - startIndex);

            IntPtr stringWrapper = llmlib.StringWrapper_Construct();
            llmlib.LLM_LoraWeight(LLMObject, json, stringWrapper);
            llmlib.StringWrapper_Delete(stringWrapper);
        }

        /// <summary>
        /// Gets a list of the lora adapters
        /// </summary>
        /// <returns>list of lara adapters</returns>
        public async Task<List<LoraWeightResult>> ListLoras()
        {
            AssertStarted();
            LLMNoInputReplyCallback callback = (IntPtr LLMObject, IntPtr strWrapper) =>
            {
                llmlib.LLM_LoraList(LLMObject, strWrapper);
            };
            string json = await LLMNoInputReply(callback);
            if (String.IsNullOrEmpty(json)) return null;
            LoraWeightResultList loraRequest = JsonUtility.FromJson<LoraWeightResultList>("{\"loraWeights\": " + json + "}");
            return loraRequest.loraWeights;
        }

        /// <summary>
        /// Allows to save / restore the state of a slot
        /// </summary>
        /// <param name="json">json request containing the query</param>
        /// <returns>slot result</returns>
        public async Task<string> Slot(string json)
        {
            AssertStarted();
            LLMReplyCallback callback = (IntPtr LLMObject, string jsonData, IntPtr strWrapper) =>
            {
                llmlib.LLM_Slot(LLMObject, jsonData, strWrapper);
            };
            return await LLMReply(callback, json);
        }

        /// <summary>
        /// Allows to use the chat and completion functionality of the LLM.
        /// </summary>
        /// <param name="json">json request containing the query</param>
        /// <param name="streamCallback">callback function to call with intermediate responses</param>
        /// <returns>completion result</returns>
        public async Task<string> Completion(string json, Callback<string> streamCallback = null)
        {
            AssertStarted();
            if (streamCallback == null) streamCallback = (string s) => {};
            StreamWrapper streamWrapper = ConstructStreamWrapper(streamCallback);
            await Task.Run(() => llmlib.LLM_Completion(LLMObject, json, streamWrapper.GetStringWrapper()));
            if (!started) return null;
            streamWrapper.Update();
            string result = streamWrapper.GetString();
            DestroyStreamWrapper(streamWrapper);
            CheckLLMStatus();
            return result;
        }

        /// <summary>
        /// Allows to cancel the requests in a specific slot of the LLM
        /// </summary>
        /// <param name="id_slot">slot of the LLM</param>
        public void CancelRequest(int id_slot)
        {
            AssertStarted();
            llmlib?.LLM_Cancel(LLMObject, id_slot);
            CheckLLMStatus();
        }

        /// <summary>
        /// Stops and destroys the LLM
        /// </summary>
        public void Destroy()
        {
            lock (staticLock)
                lock (startLock)
                {
                    try
                    {
                        if (llmlib != null)
                        {
                            if (LLMObject != IntPtr.Zero)
                            {
                                llmlib.LLM_Stop(LLMObject);
                                if (remote) llmlib.LLM_StopServer(LLMObject);
                                StopLogging();
                                llmThread?.Join();
                                llmlib.LLM_Delete(LLMObject);
                                LLMObject = IntPtr.Zero;
                            }
                            llmlib.Destroy();
                            llmlib = null;
                        }
                        started = false;
                        failed = false;
                    }
                    catch (Exception e)
                    {
                        LLMUnitySetup.LogError(e.Message);
                    }
                }
        }

        /// <summary>
        /// The Unity OnDestroy function called when the onbject is destroyed.
        /// The function StopProcess is called to stop the LLM server.
        /// </summary>
        public void OnDestroy()
        {
            Destroy();
            LLMManager.Unregister(this);
        }
    }
}


# -------------------- LLMBuilder.cs --------------------

/// @file
/// @brief File implementing the LLMUnity builder.
using UnityEditor;
using UnityEngine;
using System.IO;
using System.Collections.Generic;

#if UNITY_EDITOR
namespace LLMUnity
{
    /// @ingroup utils
    /// <summary>
    /// Class implementing the LLMUnity builder.
    /// </summary>
    public class LLMBuilder : AssetPostprocessor
    {
        static List<StringPair> movedPairs = new List<StringPair>();
        public static string BuildTempDir = Path.Combine(Application.temporaryCachePath, "LLMUnityBuild");
        static string movedCache = Path.Combine(BuildTempDir, "moved.json");

        [InitializeOnLoadMethod]
        private static void InitializeOnLoad()
        {
            Reset();
        }

        public static string PluginDir(string platform, bool relative = false)
        {
            string pluginDir = Path.Combine("Plugins", platform, "LLMUnity");
            if (!relative) pluginDir = Path.Combine(Application.dataPath, pluginDir);
            return pluginDir;
        }

        public static string PluginLibraryDir(string platform, bool relative = false)
        {
            return Path.Combine(PluginDir(platform, relative), LLMUnitySetup.libraryName);
        }

        /// <summary>
        /// Performs an action for a file or a directory recursively
        /// </summary>
        /// <param name="source">source file/directory</param>
        /// <param name="target">targer file/directory</param>
        /// <param name="actionCallback">action</param>
        public static void HandleActionFileRecursive(string source, string target, ActionCallback actionCallback)
        {
            if (File.Exists(source))
            {
                actionCallback(source, target);
            }
            else if (Directory.Exists(source))
            {
                Directory.CreateDirectory(target);
                List<string> filesAndDirs = new List<string>();
                filesAndDirs.AddRange(Directory.GetFiles(source));
                filesAndDirs.AddRange(Directory.GetDirectories(source));
                foreach (string path in filesAndDirs)
                {
                    HandleActionFileRecursive(path, Path.Combine(target, Path.GetFileName(path)), actionCallback);
                }
            }
        }

        /// <summary>
        /// Overwrites a target file based on the source file
        /// </summary>
        /// <param name="source">source file</param>
        /// <param name="target">target file</param>
        public static void CopyWithOverwrite(string source, string target)
        {
            File.Copy(source, target, true);
        }

        /// <summary>
        /// Copies a source file to a target file
        /// </summary>
        /// <param name="source">source file</param>
        /// <param name="target">target file</param>
        public static void CopyPath(string source, string target)
        {
            HandleActionFileRecursive(source, target, CopyWithOverwrite);
        }

        /// <summary>
        /// Moves a source file to a target file
        /// </summary>
        /// <param name="source">source file</param>
        /// <param name="target">target file</param>
        public static void MovePath(string source, string target)
        {
            HandleActionFileRecursive(source, target, File.Move);
            DeletePath(source);
        }

        /// <summary>
        /// Deletes a path after checking if we are allowed to
        /// </summary>
        /// <param name="path">path</param>
        public static bool DeletePath(string path)
        {
            string[] allowedDirs = new string[] { LLMUnitySetup.GetAssetPath(), BuildTempDir, PluginDir("Android"), PluginDir("iOS"), PluginDir("VisionOS")};
            bool deleteOK = false;
            foreach (string allowedDir in allowedDirs) deleteOK = deleteOK || LLMUnitySetup.IsSubPath(path, allowedDir);
            if (!deleteOK)
            {
                LLMUnitySetup.LogError($"Safeguard: {path} will not be deleted because it may not be safe");
                return false;
            }
            if (File.Exists(path)) File.Delete(path);
            else if (Directory.Exists(path)) Directory.Delete(path, true);
            return true;
        }

        static void AddMovedPair(string source, string target)
        {
            movedPairs.Add(new StringPair {source = source, target = target});
            File.WriteAllText(movedCache, JsonUtility.ToJson(new ListStringPair { pairs = movedPairs }, true));
        }

        static void AddTargetPair(string target)
        {
            AddMovedPair("", target);
        }

        static bool MoveAction(string source, string target, bool addEntry = true)
        {
            ActionCallback moveCallback;
            if (File.Exists(source)) moveCallback = File.Move;
            else if (Directory.Exists(source)) moveCallback = MovePath;
            else return false;

            if (addEntry) AddMovedPair(source, target);
            moveCallback(source, target);
            return true;
        }

        static bool CopyAction(string source, string target, bool addEntry = true)
        {
            ActionCallback copyCallback;
            if (File.Exists(source)) copyCallback = File.Copy;
            else if (Directory.Exists(source)) copyCallback = CopyPath;
            else return false;

            if (addEntry) AddTargetPair(target);
            copyCallback(source, target);
            return true;
        }

        static void CopyActionAddMeta(string source, string target)
        {
            CopyAction(source, target);
            AddTargetPair(target + ".meta");
        }

        static void AddActionAddMeta(string target)
        {
            AddTargetPair(target);
            AddTargetPair(target + ".meta");
        }

        /// <summary>
        /// Moves libraries in the correct place for building
        /// </summary>
        /// <param name="platform">target platform</param>
        public static void BuildLibraryPlatforms(BuildTarget buildTarget)
        {
            string platform = "";
            switch (buildTarget)
            {
                case BuildTarget.StandaloneWindows:
                case BuildTarget.StandaloneWindows64:
                    platform = "windows";
                    break;
                case BuildTarget.StandaloneLinux64:
                    platform = "linux";
                    break;
                case BuildTarget.StandaloneOSX:
                    platform = "macos";
                    break;
                case BuildTarget.Android:
                    platform = "android";
                    break;
                case BuildTarget.iOS:
                    platform = "ios";
                    break;
                case BuildTarget.VisionOS:
                    platform = "visionos";
                    break;
            }

            foreach (string source in Directory.GetDirectories(LLMUnitySetup.libraryPath))
            {
                string sourceName = Path.GetFileName(source);
                bool move = !sourceName.StartsWith(platform);
                move = move || (sourceName.Contains("cuda") && !sourceName.Contains("full") && LLMUnitySetup.FullLlamaLib);
                move = move || (sourceName.Contains("cuda") && sourceName.Contains("full") && !LLMUnitySetup.FullLlamaLib);
                if (move)
                {
                    string target = Path.Combine(BuildTempDir, sourceName);
                    MoveAction(source, target);
                    MoveAction(source + ".meta", target + ".meta");
                }
            }

            if (buildTarget == BuildTarget.Android || buildTarget == BuildTarget.iOS || buildTarget == BuildTarget.VisionOS)
            {
                string source = Path.Combine(LLMUnitySetup.libraryPath, platform);
                string target = PluginLibraryDir(buildTarget.ToString());
                string pluginDir = PluginDir(buildTarget.ToString());
                MoveAction(source, target);
                MoveAction(source + ".meta", target + ".meta");
                AddActionAddMeta(pluginDir);
            }
        }

        static void OnPostprocessAllAssets(string[] importedAssets, string[] deletedAssets, string[] movedAssets, string[] movedFromAssetPaths, bool didDomainReload)
        {
            foreach (BuildTarget buildTarget in new BuildTarget[]{BuildTarget.iOS, BuildTarget.VisionOS})
            {
                string pathToPlugin = Path.Combine("Assets", PluginLibraryDir(buildTarget.ToString(), true), $"libundreamai_{buildTarget.ToString().ToLower()}.a");
                for (int i = 0; i < movedAssets.Length; i++)
                {
                    if (movedAssets[i] == pathToPlugin)
                    {
                        var importer = AssetImporter.GetAtPath(pathToPlugin) as PluginImporter;
                        if (importer != null && importer.isNativePlugin)
                        {
                            importer.SetCompatibleWithPlatform(buildTarget, true);
                            importer.SetPlatformData(buildTarget, "CPU", "ARM64");
                            AssetDatabase.ImportAsset(pathToPlugin);
                        }
                    }
                }
            }
        }

        /// <summary>
        /// Bundles the model information
        /// </summary>
        public static void BuildModels()
        {
            LLMManager.Build(CopyActionAddMeta);
            if (File.Exists(LLMUnitySetup.LLMManagerPath)) AddActionAddMeta(LLMUnitySetup.LLMManagerPath);
        }

        /// <summary>
        /// Bundles the models and libraries
        /// </summary>
        public static void Build(BuildTarget buildTarget)
        {
            DeletePath(BuildTempDir);
            Directory.CreateDirectory(BuildTempDir);
            BuildLibraryPlatforms(buildTarget);
            BuildModels();
        }

        /// <summary>
        /// Resets the libraries back to their original state
        /// </summary>
        public static void Reset()
        {
            if (!File.Exists(movedCache)) return;
            List<StringPair> movedPairs = JsonUtility.FromJson<ListStringPair>(File.ReadAllText(movedCache)).pairs;
            if (movedPairs == null) return;

            bool refresh = false;
            foreach (var pair in movedPairs)
            {
                if (pair.source == "") refresh |= DeletePath(pair.target);
                else refresh |= MoveAction(pair.target, pair.source, false);
            }
            if (refresh) AssetDatabase.Refresh();
            DeletePath(movedCache);
        }
    }
}
#endif


# -------------------- LLMCaller.cs --------------------

/// @file
/// @brief File implementing the basic functionality for LLM callers.
using System;
using System.Collections.Generic;
using System.Threading.Tasks;
using UnityEngine;
using UnityEngine.Networking;

namespace LLMUnity
{
    [DefaultExecutionOrder(-2)]
    /// @ingroup llm
    /// <summary>
    /// Class implementing calling of LLM functions (local and remote).
    /// </summary>
    public class LLMCaller : MonoBehaviour
    {
        /// <summary> show/hide advanced options in the GameObject </summary>
        [Tooltip("show/hide advanced options in the GameObject")]
        [HideInInspector] public bool advancedOptions = false;
        /// <summary> use remote LLM server </summary>
        [Tooltip("use remote LLM server")]
        [LocalRemote] public bool remote = false;
        /// <summary> LLM GameObject to use </summary>
        [Tooltip("LLM GameObject to use")] // Tooltip: ignore
        [Local, SerializeField] protected LLM _llm;
        public LLM llm
        {
            get => _llm;//whatever
            set => SetLLM(value);
        }
        /// <summary> API key for the remote server </summary>
        [Tooltip("API key for the remote server")]
        [Remote] public string APIKey;
        /// <summary> host of the remote LLM server </summary>
        [Tooltip("host of the remote LLM server")]
        [Remote] public string host = "localhost";
        /// <summary> port of the remote LLM server </summary>
        [Tooltip("port of the remote LLM server")]
        [Remote] public int port = 13333;
        /// <summary> number of retries to use for the remote LLM server requests (-1 = infinite) </summary>
        [Tooltip("number of retries to use for the remote LLM server requests (-1 = infinite)")]
        [Remote] public int numRetries = 10;

        protected LLM _prellm;
        protected List<(string, string)> requestHeaders;
        protected List<UnityWebRequest> WIPRequests = new List<UnityWebRequest>();

        /// <summary>
        /// The Unity Awake function that initializes the state before the application starts.
        /// The following actions are executed:
        /// - the corresponding LLM server is defined (if ran locally)
        /// - the grammar is set based on the grammar file
        /// - the prompt and chat history are initialised
        /// - the chat template is constructed
        /// - the number of tokens to keep are based on the system prompt (if setNKeepToPrompt=true)
        /// </summary>
        public virtual void Awake()
        {
            // Start the LLM server in a cross-platform way
            if (!enabled) return;

            requestHeaders = new List<(string, string)> { ("Content-Type", "application/json") };
            if (!remote)
            {
                AssignLLM();
                if (llm == null)
                {
                    string error = $"No LLM assigned or detected for LLMCharacter {name}!";
                    LLMUnitySetup.LogError(error);
                    throw new Exception(error);
                }
            }
            else
            {
                if (!String.IsNullOrEmpty(APIKey)) requestHeaders.Add(("Authorization", "Bearer " + APIKey));
            }
        }

        /// <summary>
        /// Sets the LLM object of the LLMCaller
        /// </summary>
        /// <param name="llmSet">LLM object</param>
        protected virtual void SetLLM(LLM llmSet)
        {
            if (llmSet != null && !IsValidLLM(llmSet))
            {
                LLMUnitySetup.LogError(NotValidLLMError());
                llmSet = null;
            }
            _llm = llmSet;
            _prellm = _llm;
        }

        /// <summary>
        /// Checks if a LLM is valid for the LLMCaller
        /// </summary>
        /// <param name="llmSet">LLM object</param>
        /// <returns>bool specifying whether the LLM is valid</returns>
        public virtual bool IsValidLLM(LLM llmSet)
        {
            return true;
        }

        /// <summary>
        /// Checks if a LLM can be auto-assigned if the LLM of the LLMCaller is null
        /// </summary>
        /// <param name="llmSet"LLM object></param>
        /// <returns>bool specifying whether the LLM can be auto-assigned</returns>
        public virtual bool IsAutoAssignableLLM(LLM llmSet)
        {
            return true;
        }

        protected virtual string NotValidLLMError()
        {
            return $"Can't set LLM {llm.name} to {name}";
        }

        protected virtual void OnValidate()
        {
            if (_llm != _prellm) SetLLM(_llm);
            AssignLLM();
        }

        protected virtual void Reset()
        {
            AssignLLM();
        }

        protected virtual void AssignLLM()
        {
            if (remote || llm != null) return;

            List<LLM> validLLMs = new List<LLM>();
#if UNITY_6000_0_OR_NEWER
            foreach (LLM foundllm in FindObjectsByType(typeof(LLM), FindObjectsSortMode.None))
#else
            foreach (LLM foundllm in FindObjectsOfType<LLM>())
#endif
            {
                if (IsValidLLM(foundllm) && IsAutoAssignableLLM(foundllm)) validLLMs.Add(foundllm);
            }
            if (validLLMs.Count == 0) return;

            llm = SortLLMsByBestMatching(validLLMs.ToArray())[0];
            string msg = $"Assigning LLM {llm.name} to {GetType()} {name}";
            if (llm.gameObject.scene != gameObject.scene) msg += $" from scene {llm.gameObject.scene}";
            LLMUnitySetup.Log(msg);
        }

        protected virtual LLM[] SortLLMsByBestMatching(LLM[] arrayIn)
        {
            LLM[] array = (LLM[])arrayIn.Clone();
            for (int i = 0; i < array.Length - 1; i++)
            {
                bool swapped = false;
                for (int j = 0; j < array.Length - i - 1; j++)
                {
                    bool sameScene = array[j].gameObject.scene == array[j + 1].gameObject.scene;
                    bool swap = (
                        (!sameScene && array[j + 1].gameObject.scene == gameObject.scene) ||
                        (sameScene && array[j].transform.GetSiblingIndex() > array[j + 1].transform.GetSiblingIndex())
                    );
                    if (swap)
                    {
                        LLM temp = array[j];
                        array[j] = array[j + 1];
                        array[j + 1] = temp;
                        swapped = true;
                    }
                }
                if (!swapped) break;
            }
            return array;
        }

        protected virtual List<int> TokenizeContent(TokenizeResult result)
        {
            // get the tokens from a tokenize result received from the endpoint
            return result.tokens;
        }

        protected virtual string DetokenizeContent(TokenizeRequest result)
        {
            // get content from a chat result received from the endpoint
            return result.content;
        }

        protected virtual List<float> EmbeddingsContent(EmbeddingsResult result)
        {
            // get content from a chat result received from the endpoint
            return result.embedding;
        }

        protected virtual Ret ConvertContent<Res, Ret>(string response, ContentCallback<Res, Ret> getContent = null)
        {
            // template function to convert the json received and get the content
            if (response == null) return default;
            response = response.Trim();
            if (response.StartsWith("data: "))
            {
                string responseArray = "";
                foreach (string responsePart in response.Replace("\n\n", "").Split("data: "))
                {
                    if (responsePart == "") continue;
                    if (responseArray != "") responseArray += ",\n";
                    responseArray += responsePart;
                }
                response = $"{{\"data\": [{responseArray}]}}";
            }
            return getContent(JsonUtility.FromJson<Res>(response));
        }

        protected virtual void CancelRequestsLocal() {}

        protected virtual void CancelRequestsRemote()
        {
            foreach (UnityWebRequest request in WIPRequests)
            {
                request.Abort();
            }
            WIPRequests.Clear();
        }

        /// <summary>
        /// Cancel the ongoing requests e.g. Chat, Complete.
        /// </summary>
        // <summary>
        public virtual void CancelRequests()
        {
            if (remote) CancelRequestsRemote();
            else CancelRequestsLocal();
        }

        protected virtual async Task<Ret> PostRequestLocal<Res, Ret>(string json, string endpoint, ContentCallback<Res, Ret> getContent, Callback<Ret> callback = null)
        {
            // send a post request to the server and call the relevant callbacks to convert the received content and handle it
            // this function has streaming functionality i.e. handles the answer while it is being received
            while (!llm.failed && !llm.started) await Task.Yield();
            string callResult = null;
            switch (endpoint)
            {
                case "tokenize":
                    callResult = await llm.Tokenize(json);
                    break;
                case "detokenize":
                    callResult = await llm.Detokenize(json);
                    break;
                case "embeddings":
                    callResult = await llm.Embeddings(json);
                    break;
                case "slots":
                    callResult = await llm.Slot(json);
                    break;
                default:
                    LLMUnitySetup.LogError($"Unknown endpoint {endpoint}");
                    break;
            }

            Ret result = ConvertContent(callResult, getContent);
            callback?.Invoke(result);
            return result;
        }

        protected virtual async Task<Ret> PostRequestRemote<Res, Ret>(string json, string endpoint, ContentCallback<Res, Ret> getContent, Callback<Ret> callback = null)
        {
            // send a post request to the server and call the relevant callbacks to convert the received content and handle it
            // this function has streaming functionality i.e. handles the answer while it is being received
            if (endpoint == "slots")
            {
                LLMUnitySetup.LogError("Saving and loading is not currently supported in remote setting");
                return default;
            }

            Ret result = default;
            byte[] jsonToSend = new System.Text.UTF8Encoding().GetBytes(json);
            UnityWebRequest request = null;
            string error = null;
            int tryNr = numRetries;

            while (tryNr != 0)
            {
                using (request = UnityWebRequest.Put($"{host}:{port}/{endpoint}", jsonToSend))
                {
                    WIPRequests.Add(request);

                    request.method = "POST";
                    if (requestHeaders != null)
                    {
                        for (int i = 0; i < requestHeaders.Count; i++)
                            request.SetRequestHeader(requestHeaders[i].Item1, requestHeaders[i].Item2);
                    }

                    // Start the request asynchronously
                    UnityWebRequestAsyncOperation asyncOperation = request.SendWebRequest();
                    await Task.Yield(); // Wait for the next frame so that asyncOperation is properly registered (especially if not in main thread)

                    float lastProgress = 0f;
                    // Continue updating progress until the request is completed
                    while (!asyncOperation.isDone)
                    {
                        float currentProgress = request.downloadProgress;
                        // Check if progress has changed
                        if (currentProgress != lastProgress && callback != null)
                        {
                            callback?.Invoke(ConvertContent(request.downloadHandler.text, getContent));
                            lastProgress = currentProgress;
                        }
                        // Wait for the next frame
                        await Task.Yield();
                    }
                    WIPRequests.Remove(request);
                    if (request.result == UnityWebRequest.Result.Success)
                    {
                        result = ConvertContent(request.downloadHandler.text, getContent);
                        error = null;
                        break;
                    }
                    else
                    {
                        result = default;
                        error = request.error;
                        if (request.responseCode == (int)System.Net.HttpStatusCode.Unauthorized) break;
                    }
                }
                tryNr--;
                if (tryNr > 0) await Task.Delay(200 * (numRetries - tryNr));
            }

            if (error != null) LLMUnitySetup.LogError(error);
            callback?.Invoke(result);
            return result;
        }

        protected virtual async Task<Ret> PostRequest<Res, Ret>(string json, string endpoint, ContentCallback<Res, Ret> getContent, Callback<Ret> callback = null)
        {
            if (remote) return await PostRequestRemote(json, endpoint, getContent, callback);
            return await PostRequestLocal(json, endpoint, getContent, callback);
        }

        /// <summary>
        /// Tokenises the provided query.
        /// </summary>
        /// <param name="query">query to tokenise</param>
        /// <param name="callback">callback function called with the result tokens</param>
        /// <returns>list of the tokens</returns>
        public virtual async Task<List<int>> Tokenize(string query, Callback<List<int>> callback = null)
        {
            // handle the tokenization of a message by the user
            TokenizeRequest tokenizeRequest = new TokenizeRequest();
            tokenizeRequest.content = query;
            string json = JsonUtility.ToJson(tokenizeRequest);
            return await PostRequest<TokenizeResult, List<int>>(json, "tokenize", TokenizeContent, callback);
        }

        /// <summary>
        /// Detokenises the provided tokens to a string.
        /// </summary>
        /// <param name="tokens">tokens to detokenise</param>
        /// <param name="callback">callback function called with the result string</param>
        /// <returns>the detokenised string</returns>
        public virtual async Task<string> Detokenize(List<int> tokens, Callback<string> callback = null)
        {
            // handle the detokenization of a message by the user
            TokenizeResult tokenizeRequest = new TokenizeResult();
            tokenizeRequest.tokens = tokens;
            string json = JsonUtility.ToJson(tokenizeRequest);
            return await PostRequest<TokenizeRequest, string>(json, "detokenize", DetokenizeContent, callback);
        }

        /// <summary>
        /// Computes the embeddings of the provided input.
        /// </summary>
        /// <param name="tokens">input to compute the embeddings for</param>
        /// <param name="callback">callback function called with the result string</param>
        /// <returns>the computed embeddings</returns>
        public virtual async Task<List<float>> Embeddings(string query, Callback<List<float>> callback = null)
        {
            // handle the tokenization of a message by the user
            TokenizeRequest tokenizeRequest = new TokenizeRequest();
            tokenizeRequest.content = query;
            string json = JsonUtility.ToJson(tokenizeRequest);
            return await PostRequest<EmbeddingsResult, List<float>>(json, "embeddings", EmbeddingsContent, callback);
        }
    }
}


# -------------------- LLMCharacter.cs --------------------

/// @file
/// @brief File implementing the LLM characters.
using System;
using System.Collections.Generic;
using System.IO;
using System.Threading;
using System.Threading.Tasks;
using UnityEditor;
using UnityEngine;

namespace LLMUnity
{
    [DefaultExecutionOrder(-2)]
    /// @ingroup llm
    /// <summary>
    /// Class implementing the LLM characters.
    /// </summary>
    public class LLMCharacter : LLMCaller
    {
        /// <summary> file to save the chat history.
        /// The file will be saved within the persistentDataPath directory. </summary>
        [Tooltip("file to save the chat history. The file will be saved within the persistentDataPath directory.")]
        [LLM] public string save = "";
        /// <summary> save the LLM cache. Speeds up the prompt calculation when reloading from history but also requires ~100MB of space per character. </summary>
        [Tooltip("save the LLM cache. Speeds up the prompt calculation when reloading from history but also requires ~100MB of space per character.")]
        [LLM] public bool saveCache = false;
        /// <summary> log the constructed prompt the Unity Editor. </summary>
        [Tooltip("log the constructed prompt the Unity Editor.")]
        [LLM] public bool debugPrompt = false;
        /// <summary> maximum number of tokens that the LLM will predict (-1 = infinity). </summary>
        [Tooltip("maximum number of tokens that the LLM will predict (-1 = infinity).")]
        [Model] public int numPredict = -1;
        /// <summary> slot of the server to use for computation (affects caching) </summary>
        [Tooltip("slot of the server to use for computation (affects caching)")]
        [ModelAdvanced] public int slot = -1;
        /// <summary> grammar file used for the LLMCharacter (.gbnf format) </summary>
        [Tooltip("grammar file used for the LLMCharacter (.gbnf format)")]
        [ModelAdvanced] public string grammar = null;
        /// <summary> cache the processed prompt to avoid reprocessing the entire prompt every time (default: true, recommended!) </summary>
        [Tooltip("cache the processed prompt to avoid reprocessing the entire prompt every time (default: true, recommended!)")]
        [ModelAdvanced] public bool cachePrompt = true;
        /// <summary> seed for reproducibility (-1 = no reproducibility). </summary>
        [Tooltip("seed for reproducibility (-1 = no reproducibility).")]
        [ModelAdvanced] public int seed = 0;
        /// <summary> LLM temperature, lower values give more deterministic answers. </summary>
        [Tooltip("LLM temperature, lower values give more deterministic answers.")]
        [ModelAdvanced, Float(0f, 2f)] public float temperature = 0.2f;
        /// <summary> Top-k sampling selects the next token only from the top k most likely predicted tokens (0 = disabled).
        /// Higher values lead to more diverse text, while lower value will generate more focused and conservative text.
        /// </summary>
        [Tooltip("Top-k sampling selects the next token only from the top k most likely predicted tokens (0 = disabled). Higher values lead to more diverse text, while lower value will generate more focused and conservative text. ")]
        [ModelAdvanced, Int(-1, 100)] public int topK = 40;
        /// <summary> Top-p sampling selects the next token from a subset of tokens that together have a cumulative probability of at least p (1.0 = disabled).
        /// Higher values lead to more diverse text, while lower value will generate more focused and conservative text.
        /// </summary>
        [Tooltip("Top-p sampling selects the next token from a subset of tokens that together have a cumulative probability of at least p (1.0 = disabled). Higher values lead to more diverse text, while lower value will generate more focused and conservative text. ")]
        [ModelAdvanced, Float(0f, 1f)] public float topP = 0.9f;
        /// <summary> minimum probability for a token to be used. </summary>
        [Tooltip("minimum probability for a token to be used.")]
        [ModelAdvanced, Float(0f, 1f)] public float minP = 0.05f;
        /// <summary> Penalty based on repeated tokens to control the repetition of token sequences in the generated text. </summary>
        [Tooltip("Penalty based on repeated tokens to control the repetition of token sequences in the generated text.")]
        [ModelAdvanced, Float(0f, 2f)] public float repeatPenalty = 1.1f;
        /// <summary> Penalty based on token presence in previous responses to control the repetition of token sequences in the generated text. (0.0 = disabled). </summary>
        [Tooltip("Penalty based on token presence in previous responses to control the repetition of token sequences in the generated text. (0.0 = disabled).")]
        [ModelAdvanced, Float(0f, 1f)] public float presencePenalty = 0f;
        /// <summary> Penalty based on token frequency in previous responses to control the repetition of token sequences in the generated text. (0.0 = disabled). </summary>
        [Tooltip("Penalty based on token frequency in previous responses to control the repetition of token sequences in the generated text. (0.0 = disabled).")]
        [ModelAdvanced, Float(0f, 1f)] public float frequencyPenalty = 0f;
        /// <summary> enable locally typical sampling (1.0 = disabled). Higher values will promote more contextually coherent tokens, while  lower values will promote more diverse tokens. </summary>
        [Tooltip("enable locally typical sampling (1.0 = disabled). Higher values will promote more contextually coherent tokens, while  lower values will promote more diverse tokens.")]
        [ModelAdvanced, Float(0f, 1f)] public float typicalP = 1f;
        /// <summary> last n tokens to consider for penalizing repetition (0 = disabled, -1 = ctx-size). </summary>
        [Tooltip("last n tokens to consider for penalizing repetition (0 = disabled, -1 = ctx-size).")]
        [ModelAdvanced, Int(0, 2048)] public int repeatLastN = 64;
        /// <summary> penalize newline tokens when applying the repeat penalty. </summary>
        [Tooltip("penalize newline tokens when applying the repeat penalty.")]
        [ModelAdvanced] public bool penalizeNl = true;
        /// <summary> prompt for the purpose of the penalty evaluation. Can be either null, a string or an array of numbers representing tokens (null/'' = use original prompt) </summary>
        [Tooltip("prompt for the purpose of the penalty evaluation. Can be either null, a string or an array of numbers representing tokens (null/'' = use original prompt)")]
        [ModelAdvanced] public string penaltyPrompt;
        /// <summary> enable Mirostat sampling, controlling perplexity during text generation (0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0). </summary>
        [Tooltip("enable Mirostat sampling, controlling perplexity during text generation (0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0).")]
        [ModelAdvanced, Int(0, 2)] public int mirostat = 0;
        /// <summary> The Mirostat target entropy (tau) controls the balance between coherence and diversity in the generated text. </summary>
        [Tooltip("The Mirostat target entropy (tau) controls the balance between coherence and diversity in the generated text.")]
        [ModelAdvanced, Float(0f, 10f)] public float mirostatTau = 5f;
        /// <summary> The Mirostat learning rate (eta) controls how quickly the algorithm responds to feedback from the generated text. </summary>
        [Tooltip("The Mirostat learning rate (eta) controls how quickly the algorithm responds to feedback from the generated text.")]
        [ModelAdvanced, Float(0f, 1f)] public float mirostatEta = 0.1f;
        /// <summary> if greater than 0, the response also contains the probabilities of top N tokens for each generated token. </summary>
        [Tooltip("if greater than 0, the response also contains the probabilities of top N tokens for each generated token.")]
        [ModelAdvanced, Int(0, 10)] public int nProbs = 0;
        /// <summary> ignore end of stream token and continue generating. </summary>
        [Tooltip("ignore end of stream token and continue generating.")]
        [ModelAdvanced] public bool ignoreEos = false;
        /// <summary> number of tokens to retain from the prompt when the model runs out of context (-1 = LLMCharacter prompt tokens if setNKeepToPrompt is set to true). </summary>
        [Tooltip("number of tokens to retain from the prompt when the model runs out of context (-1 = LLMCharacter prompt tokens if setNKeepToPrompt is set to true).")]
        public int nKeep = -1;
        /// <summary> stopwords to stop the LLM in addition to the default stopwords from the chat template. </summary>
        [Tooltip("stopwords to stop the LLM in addition to the default stopwords from the chat template.")]
        public List<string> stop = new List<string>();
        /// <summary> the logit bias option allows to manually adjust the likelihood of specific tokens appearing in the generated text.
        /// By providing a token ID and a positive or negative bias value, you can increase or decrease the probability of that token being generated. </summary>
        [Tooltip("the logit bias option allows to manually adjust the likelihood of specific tokens appearing in the generated text. By providing a token ID and a positive or negative bias value, you can increase or decrease the probability of that token being generated.")]
        public Dictionary<int, string> logitBias = null;
        /// <summary> Receive the reply from the model as it is produced (recommended!).
        /// If not selected, the full reply from the model is received in one go </summary>
        [Tooltip("Receive the reply from the model as it is produced (recommended!). If not selected, the full reply from the model is received in one go")]
        [Chat] public bool stream = true;
        /// <summary> the name of the player </summary>
        [Tooltip("the name of the player")]
        [Chat] public string playerName = "user";
        /// <summary> the name of the AI </summary>
        [Tooltip("the name of the AI")]
        [Chat] public string AIName = "assistant";
        /// <summary> a description of the AI role (system prompt) </summary>
        [Tooltip("a description of the AI role (system prompt)")]
        [TextArea(5, 10), Chat] public string prompt = "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.";
        /// <summary> set the number of tokens to always retain from the prompt (nKeep) based on the LLMCharacter system prompt </summary>
        [Tooltip("set the number of tokens to always retain from the prompt (nKeep) based on the LLMCharacter system prompt")]
        public bool setNKeepToPrompt = true;
        /// <summary> the chat history as list of chat messages </summary>
        [Tooltip("the chat history as list of chat messages")]
        public List<ChatMessage> chat = new List<ChatMessage>();
        /// <summary> the grammar to use </summary>
        [Tooltip("the grammar to use")]
        public string grammarString;

        /// \cond HIDE
        protected SemaphoreSlim chatLock = new SemaphoreSlim(1, 1);
        protected string chatTemplate;
        protected ChatTemplate template = null;
        /// \endcond

        /// <summary>
        /// The Unity Awake function that initializes the state before the application starts.
        /// The following actions are executed:
        /// - the corresponding LLM server is defined (if ran locally)
        /// - the grammar is set based on the grammar file
        /// - the prompt and chat history are initialised
        /// - the chat template is constructed
        /// - the number of tokens to keep are based on the system prompt (if setNKeepToPrompt=true)
        /// </summary>
        public override void Awake()
        {
            if (!enabled) return;
            base.Awake();
            if (!remote)
            {
                int slotFromServer = llm.Register(this);
                if (slot == -1) slot = slotFromServer;
            }
            InitGrammar();
            InitHistory();
        }

        protected override void OnValidate()
        {
            base.OnValidate();
            if (llm != null && llm.parallelPrompts > -1 && (slot < -1 || slot >= llm.parallelPrompts)) LLMUnitySetup.LogError($"The slot needs to be between 0 and {llm.parallelPrompts-1}, or -1 to be automatically set");
        }

        protected override string NotValidLLMError()
        {
            return base.NotValidLLMError() + $", it is an embedding only model";
        }

        /// <summary>
        /// Checks if a LLM is valid for the LLMCaller
        /// </summary>
        /// <param name="llmSet">LLM object</param>
        /// <returns>bool specifying whether the LLM is valid</returns>
        public override bool IsValidLLM(LLM llmSet)
        {
            return !llmSet.embeddingsOnly;
        }

        protected virtual void InitHistory()
        {
            ClearChat();
            _ = LoadHistory();
        }

        protected virtual async Task LoadHistory()
        {
            if (save == "" || !File.Exists(GetJsonSavePath(save))) return;
            await chatLock.WaitAsync(); // Acquire the lock
            try
            {
                await Load(save);
            }
            finally
            {
                chatLock.Release(); // Release the lock
            }
        }

        protected virtual string GetSavePath(string filename)
        {
            return Path.Combine(Application.persistentDataPath, filename).Replace('\\', '/');
        }

        /// <summary>
        /// Allows to get the save path of the chat history based on the provided filename or relative path.
        /// </summary>
        /// <param name="filename">filename or relative path used for the save</param>
        /// <returns>save path</returns>
        public virtual string GetJsonSavePath(string filename)
        {
            return GetSavePath(filename + ".json");
        }

        /// <summary>
        /// Allows to get the save path of the LLM cache based on the provided filename or relative path.
        /// </summary>
        /// <param name="filename">filename or relative path used for the save</param>
        /// <returns>save path</returns>
        public virtual string GetCacheSavePath(string filename)
        {
            return GetSavePath(filename + ".cache");
        }

        /// <summary>
        /// Clear the chat of the LLMCharacter.
        /// </summary>
        public virtual void ClearChat()
        {
            chat.Clear();
            ChatMessage promptMessage = new ChatMessage { role = "system", content = prompt };
            chat.Add(promptMessage);
        }

        /// <summary>
        /// Set the system prompt for the LLMCharacter.
        /// </summary>
        /// <param name="newPrompt"> the system prompt </param>
        /// <param name="clearChat"> whether to clear (true) or keep (false) the current chat history on top of the system prompt. </param>
        public virtual void SetPrompt(string newPrompt, bool clearChat = true)
        {
            prompt = newPrompt;
            nKeep = -1;
            if (clearChat) ClearChat();
            else chat[0] = new ChatMessage { role = "system", content = prompt };
        }

        protected virtual bool CheckTemplate()
        {
            if (template == null)
            {
                LLMUnitySetup.LogError("Template not set!");
                return false;
            }
            return true;
        }

        protected virtual async Task<bool> InitNKeep()
        {
            if (setNKeepToPrompt && nKeep == -1)
            {
                if (!CheckTemplate()) return false;
                string systemPrompt = template.ComputePrompt(new List<ChatMessage>(){chat[0]}, playerName, "", false);
                List<int> tokens = await Tokenize(systemPrompt);
                if (tokens == null) return false;
                SetNKeep(tokens);
            }
            return true;
        }

        protected virtual void InitGrammar()
        {
            if (grammar != null && grammar != "")
            {
                grammarString = File.ReadAllText(LLMUnitySetup.GetAssetPath(grammar));
            }
        }

        protected virtual void SetNKeep(List<int> tokens)
        {
            // set the tokens to keep
            nKeep = tokens.Count;
        }

        /// <summary>
        /// Loads the chat template of the LLMCharacter.
        /// </summary>
        /// <returns></returns>
        public virtual async Task LoadTemplate()
        {
            string llmTemplate;
            if (remote)
            {
                llmTemplate = await AskTemplate();
            }
            else
            {
                llmTemplate = llm.GetTemplate();
            }
            if (llmTemplate != chatTemplate)
            {
                chatTemplate = llmTemplate;
                template = chatTemplate == null ? null : ChatTemplate.GetTemplate(chatTemplate);
                nKeep = -1;
            }
        }

        /// <summary>
        /// Sets the grammar file of the LLMCharacter
        /// </summary>
        /// <param name="path">path to the grammar file</param>
        public virtual async void SetGrammar(string path)
        {
#if UNITY_EDITOR
            if (!EditorApplication.isPlaying) path = LLMUnitySetup.AddAsset(path);
#endif
            await LLMUnitySetup.AndroidExtractAsset(path, true);
            grammar = path;
            InitGrammar();
        }

        protected virtual List<string> GetStopwords()
        {
            if (!CheckTemplate()) return null;
            List<string> stopAll = new List<string>(template.GetStop(playerName, AIName));
            if (stop != null) stopAll.AddRange(stop);
            return stopAll;
        }

        protected virtual ChatRequest GenerateRequest(string prompt)
        {
            // setup the request struct
            ChatRequest chatRequest = new ChatRequest();
            if (debugPrompt) LLMUnitySetup.Log(prompt);
            chatRequest.prompt = prompt;
            chatRequest.id_slot = slot;
            chatRequest.temperature = temperature;
            chatRequest.top_k = topK;
            chatRequest.top_p = topP;
            chatRequest.min_p = minP;
            chatRequest.n_predict = numPredict;
            chatRequest.n_keep = nKeep;
            chatRequest.stream = stream;
            chatRequest.stop = GetStopwords();
            chatRequest.typical_p = typicalP;
            chatRequest.repeat_penalty = repeatPenalty;
            chatRequest.repeat_last_n = repeatLastN;
            chatRequest.penalize_nl = penalizeNl;
            chatRequest.presence_penalty = presencePenalty;
            chatRequest.frequency_penalty = frequencyPenalty;
            chatRequest.penalty_prompt = (penaltyPrompt != null && penaltyPrompt != "") ? penaltyPrompt : null;
            chatRequest.mirostat = mirostat;
            chatRequest.mirostat_tau = mirostatTau;
            chatRequest.mirostat_eta = mirostatEta;
            chatRequest.grammar = grammarString;
            chatRequest.seed = seed;
            chatRequest.ignore_eos = ignoreEos;
            chatRequest.logit_bias = logitBias;
            chatRequest.n_probs = nProbs;
            chatRequest.cache_prompt = cachePrompt;
            return chatRequest;
        }

        /// <summary>
        /// Allows to add a message in the chat history.
        /// </summary>
        /// <param name="role">message role (e.g. playerName or AIName)</param>
        /// <param name="content">message content</param>
        public virtual void AddMessage(string role, string content)
        {
            // add the question / answer to the chat list, update prompt
            chat.Add(new ChatMessage { role = role, content = content });
        }

        /// <summary>
        /// Allows to add a player message in the chat history.
        /// </summary>
        /// <param name="content">message content</param>
        public virtual void AddPlayerMessage(string content)
        {
            AddMessage(playerName, content);
        }

        /// <summary>
        /// Allows to add a AI message in the chat history.
        /// </summary>
        /// <param name="content">message content</param>
        public virtual void AddAIMessage(string content)
        {
            AddMessage(AIName, content);
        }

        protected virtual string ChatContent(ChatResult result)
        {
            // get content from a chat result received from the endpoint
            return result.content.Trim();
        }

        protected virtual string MultiChatContent(MultiChatResult result)
        {
            // get content from a chat result received from the endpoint
            string response = "";
            foreach (ChatResult resultPart in result.data)
            {
                response += resultPart.content;
            }
            return response.Trim();
        }

        protected virtual string SlotContent(SlotResult result)
        {
            // get the tokens from a tokenize result received from the endpoint
            return result.filename;
        }

        protected virtual string TemplateContent(TemplateResult result)
        {
            // get content from a char result received from the endpoint in open AI format
            return result.template;
        }

        protected virtual async Task<string> CompletionRequest(string json, Callback<string> callback = null)
        {
            string result = "";
            if (stream)
            {
                result = await PostRequest<MultiChatResult, string>(json, "completion", MultiChatContent, callback);
            }
            else
            {
                result = await PostRequest<ChatResult, string>(json, "completion", ChatContent, callback);
            }
            return result;
        }

        protected async Task<ChatRequest> PromptWithQuery(string query)
        {
            ChatRequest result = default;
            await chatLock.WaitAsync();
            try
            {
                AddPlayerMessage(query);
                string prompt = template.ComputePrompt(chat, playerName, AIName);
                result = GenerateRequest(prompt);
                chat.RemoveAt(chat.Count - 1);
            }
            finally
            {
                chatLock.Release();
            }
            return result;
        }

        /// <summary>
        /// Chat functionality of the LLM.
        /// It calls the LLM completion based on the provided query including the previous chat history.
        /// The function allows callbacks when the response is partially or fully received.
        /// The question is added to the history if specified.
        /// </summary>
        /// <param name="query">user query</param>
        /// <param name="callback">callback function that receives the response as string</param>
        /// <param name="completionCallback">callback function called when the full response has been received</param>
        /// <param name="addToHistory">whether to add the user query to the chat history</param>
        /// <returns>the LLM response</returns>
        public virtual async Task<string> Chat(string query, Callback<string> callback = null, EmptyCallback completionCallback = null, bool addToHistory = true)
        {
            // handle a chat message by the user
            // call the callback function while the answer is received
            // call the completionCallback function when the answer is fully received
            await LoadTemplate();
            if (!CheckTemplate()) return null;
            if (!await InitNKeep()) return null;

            string json = JsonUtility.ToJson(await PromptWithQuery(query));
            string result = await CompletionRequest(json, callback);

            if (addToHistory && result != null)
            {
                await chatLock.WaitAsync();
                try
                {
                    AddPlayerMessage(query);
                    AddAIMessage(result);
                }
                finally
                {
                    chatLock.Release();
                }
                if (save != "") _ = Save(save);
            }

            completionCallback?.Invoke();
            return result;
        }

        /// <summary>
        /// Pure completion functionality of the LLM.
        /// It calls the LLM completion based solely on the provided prompt (no formatting by the chat template).
        /// The function allows callbacks when the response is partially or fully received.
        /// </summary>
        /// <param name="prompt">user query</param>
        /// <param name="callback">callback function that receives the response as string</param>
        /// <param name="completionCallback">callback function called when the full response has been received</param>
        /// <returns>the LLM response</returns>
        public virtual async Task<string> Complete(string prompt, Callback<string> callback = null, EmptyCallback completionCallback = null)
        {
            // handle a completion request by the user
            // call the callback function while the answer is received
            // call the completionCallback function when the answer is fully received
            await LoadTemplate();

            string json = JsonUtility.ToJson(GenerateRequest(prompt));
            string result = await CompletionRequest(json, callback);
            completionCallback?.Invoke();
            return result;
        }

        /// <summary>
        /// Allow to warm-up a model by processing the system prompt.
        /// The prompt processing will be cached (if cachePrompt=true) allowing for faster initialisation.
        /// The function allows a callback function for when the prompt is processed and the response received.
        /// </summary>
        /// <param name="completionCallback">callback function called when the full response has been received</param>
        /// <returns>the LLM response</returns>
        public virtual async Task Warmup(EmptyCallback completionCallback = null)
        {
            await Warmup(null, completionCallback);
        }

        /// <summary>
        /// Allow to warm-up a model by processing the provided prompt without adding it to history.
        /// The prompt processing will be cached (if cachePrompt=true) allowing for faster initialisation.
        /// The function allows a callback function for when the prompt is processed and the response received.
        ///
        /// </summary>
        /// <param name="query">user prompt used during the initialisation (not added to history)</param>
        /// <param name="completionCallback">callback function called when the full response has been received</param>
        /// <returns>the LLM response</returns>
        public virtual async Task Warmup(string query, EmptyCallback completionCallback = null)
        {
            await LoadTemplate();
            if (!CheckTemplate()) return;
            if (!await InitNKeep()) return;

            ChatRequest request;
            if (String.IsNullOrEmpty(query))
            {
                string prompt = template.ComputePrompt(chat, playerName, AIName);
                request = GenerateRequest(prompt);
            }
            else
            {
                request = await PromptWithQuery(query);
            }

            request.n_predict = 0;
            string json = JsonUtility.ToJson(request);
            await CompletionRequest(json);
            completionCallback?.Invoke();
        }

        /// <summary>
        /// Asks the LLM for the chat template to use.
        /// </summary>
        /// <returns>the chat template of the LLM</returns>
        public virtual async Task<string> AskTemplate()
        {
            return await PostRequest<TemplateResult, string>("{}", "template", TemplateContent);
        }

        protected override void CancelRequestsLocal()
        {
            if (slot >= 0) llm.CancelRequest(slot);
        }

        protected virtual async Task<string> Slot(string filepath, string action)
        {
            SlotRequest slotRequest = new SlotRequest();
            slotRequest.id_slot = slot;
            slotRequest.filepath = filepath;
            slotRequest.action = action;
            string json = JsonUtility.ToJson(slotRequest);
            return await PostRequest<SlotResult, string>(json, "slots", SlotContent);
        }

        /// <summary>
        /// Saves the chat history and cache to the provided filename / relative path.
        /// </summary>
        /// <param name="filename">filename / relative path to save the chat history</param>
        /// <returns></returns>
        public virtual async Task<string> Save(string filename)
        {
            string filepath = GetJsonSavePath(filename);
            string dirname = Path.GetDirectoryName(filepath);
            if (!Directory.Exists(dirname)) Directory.CreateDirectory(dirname);
            string json = JsonUtility.ToJson(new ChatListWrapper { chat = chat.GetRange(1, chat.Count - 1) });
            File.WriteAllText(filepath, json);

            string cachepath = GetCacheSavePath(filename);
            if (remote || !saveCache) return null;
            string result = await Slot(cachepath, "save");
            return result;
        }

        /// <summary>
        /// Load the chat history and cache from the provided filename / relative path.
        /// </summary>
        /// <param name="filename">filename / relative path to load the chat history from</param>
        /// <returns></returns>
        public virtual async Task<string> Load(string filename)
        {
            string filepath = GetJsonSavePath(filename);
            if (!File.Exists(filepath))
            {
                LLMUnitySetup.LogError($"File {filepath} does not exist.");
                return null;
            }
            string json = File.ReadAllText(filepath);
            List<ChatMessage> chatHistory = JsonUtility.FromJson<ChatListWrapper>(json).chat;
            ClearChat();
            chat.AddRange(chatHistory);
            LLMUnitySetup.Log($"Loaded {filepath}");

            string cachepath = GetCacheSavePath(filename);
            if (remote || !saveCache || !File.Exists(GetSavePath(cachepath))) return null;
            string result = await Slot(cachepath, "restore");
            return result;
        }

        protected override async Task<Ret> PostRequestLocal<Res, Ret>(string json, string endpoint, ContentCallback<Res, Ret> getContent, Callback<Ret> callback = null)
        {
            if (endpoint != "completion") return await base.PostRequestLocal(json, endpoint, getContent, callback);

            while (!llm.failed && !llm.started) await Task.Yield();

            string callResult = null;
            bool callbackCalled = false;
            if (llm.embeddingsOnly) LLMUnitySetup.LogError("The LLM can't be used for completion, only for embeddings");
            else
            {
                Callback<string> callbackString = null;
                if (stream && callback != null)
                {
                    if (typeof(Ret) == typeof(string))
                    {
                        callbackString = (strArg) =>
                        {
                            callback(ConvertContent(strArg, getContent));
                        };
                    }
                    else
                    {
                        LLMUnitySetup.LogError($"wrong callback type, should be string");
                    }
                    callbackCalled = true;
                }
                callResult = await llm.Completion(json, callbackString);
            }

            Ret result = ConvertContent(callResult, getContent);
            if (!callbackCalled) callback?.Invoke(result);
            return result;
        }
    }

    /// \cond HIDE
    [Serializable]
    public class ChatListWrapper
    {
        public List<ChatMessage> chat;
    }
    /// \endcond
}


# -------------------- LLMChatTemplates.cs --------------------

/// @file
/// @brief File implementing the chat templates.
using System.Collections.Generic;
using System.Diagnostics;
using System.IO;
using System.Text;

namespace LLMUnity
{
    /// @ingroup template
    /// <summary>
    /// Class implementing the skeleton of a chat template
    /// </summary>
    public abstract class ChatTemplate
    {
        /// <summary> the default template used when it can't be determined ("chatml") </summary>
        public static string DefaultTemplate;
        /// <summary> a dictionary from chat template name to chat template type.
        /// It can be used to get the chat template names supported with:
        /// \code
        /// ChatTemplate.templates.Keys
        /// \endcode
        /// </summary>
        public static Dictionary<string, ChatTemplate> templates;
        /// \cond HIDE
        public static ChatTemplate[] templateClasses;
        public static Dictionary<string, string> templatesDescription;
        public static Dictionary<string, string> modelTemplates;
        public static Dictionary<string, string> chatTemplates;
        /// \endcond

        static ChatTemplate()
        {
            DefaultTemplate = "chatml";

            templateClasses = new ChatTemplate[]
            {
                new ChatMLTemplate(),
                new AlpacaTemplate(),
                new GemmaTemplate(),
                new MistralChatTemplate(),
                new MistralInstructTemplate(),
                new LLama3ChatTemplate(),
                new LLama2ChatTemplate(),
                new LLama2Template(),
                new Phi4MiniTemplate(),
                new Phi4Template(),
                new Phi3_5Template(),
                new Phi3Template(),
                new Phi2Template(),
                new DeepSeekR1Template(),
                new DeepSeekV3Template(),
                new DeepSeekV2Template(),
                new VicunaTemplate(),
                new ZephyrTemplate(),
            };

            templates = new Dictionary<string, ChatTemplate>();
            templatesDescription = new Dictionary<string, string>();
            modelTemplates = new Dictionary<string, string>();
            chatTemplates = new Dictionary<string, string>();
            foreach (ChatTemplate template in templateClasses)
            {
                if (templates.ContainsKey(template.GetName())) LLMUnitySetup.LogError($"{template.GetName()} already in templates");
                templates[template.GetName()] = template;
                if (templatesDescription.ContainsKey(template.GetDescription())) LLMUnitySetup.LogError($"{template.GetDescription()} already in templatesDescription");
                templatesDescription[template.GetDescription()] = template.GetName();
                foreach (string match in template.GetNameMatches())
                {
                    if (modelTemplates.ContainsKey(match)) LLMUnitySetup.LogError($"Name for {template.GetName()} already in modelTemplates");
                    modelTemplates[match] = template.GetName();
                }
                foreach (string match in template.GetChatTemplateMatches())
                {
                    if (chatTemplates.ContainsKey(match)) LLMUnitySetup.LogError($"Chat template for {template.GetName()} already in chatTemplates");
                    chatTemplates[match] = template.GetName();
                }
            }
        }

        /// <summary>
        /// Determines the chat template name from a search name.
        /// It searches if any of the chat template names is a substring of the provided name.
        /// </summary>
        /// <param name="name">search name</param>
        /// <returns>chat template name</returns>
        public static string FromName(string name)
        {
            if (name == null) return null;
            string nameLower = name.ToLower();
            foreach (var pair in modelTemplates)
            {
                if (nameLower.Contains(pair.Key)) return pair.Value;
            }
            return null;
        }

        /// <summary>
        /// Determines the chat template name from a Jinja template.
        /// </summary>
        /// <param name="template">Jinja template</param>
        /// <returns>chat template name</returns>
        public static string FromTemplate(string template)
        {
            if (template == null) return null;
            string templateTrim = template.Trim();
            if (chatTemplates.TryGetValue(templateTrim, out string value))
                return value;
            return null;
        }

        /// <summary>
        /// Determines the chat template name from a GGUF file.
        /// It reads the GGUF file and then determines the chat template name based on:
        /// - the jinja template defined in the file (if it exists and matched)
        /// - the model name defined in the file (if it exists and matched)
        /// - the filename defined in the file (if matched)
        /// - otherwises uses the DefaultTemplate
        /// </summary>
        /// <param name="path">GGUF file path</param>
        /// <returns>template name</returns>
        public static string FromGGUF(string path)
        {
            return FromGGUF(new GGUFReader(path), path);
        }

        public static string FromGGUF(GGUFReader reader, string path)
        {
            string name;
            name = FromTemplate(reader.GetStringField("tokenizer.chat_template"));
            if (name != null) return name;

            name = FromName(reader.GetStringField("general.name"));
            if (name != null) return name;

            name = FromName(Path.GetFileNameWithoutExtension(path));
            if (name != null) return name;

            LLMUnitySetup.Log("No chat template could be matched, fallback to ChatML");
            return DefaultTemplate;
        }

        /// <summary>
        /// Creates the chat template based on the provided chat template name
        /// </summary>
        /// <param name="template">chat template name</param>
        /// <returns>chat template</returns>
        public static ChatTemplate GetTemplate(string template)
        {
            return templates[template];
        }

        /// <summary> Returns the chat template name </summary>
        public virtual string GetName() { return ""; }
        /// <summary> Returns the chat template description </summary>
        public virtual string GetDescription() { return ""; }
        /// <summary> Returns an array of names that can be used to match the chat template </summary>
        public virtual string[] GetNameMatches() { return new string[] {}; }
        /// <summary> Returns an array of jinja templates that can be used to match the chat template </summary>
        public virtual string[] GetChatTemplateMatches() { return new string[] {}; }
        /// <summary> Returns an array of the stopwords used by the template </summary>
        public virtual string[] GetStop(string playerName, string AIName) { return new string[] {}; }

        protected virtual string PromptPrefix() { return ""; }
        protected virtual string SystemPrefix() { return ""; }
        protected virtual string SystemSuffix() { return ""; }
        protected virtual string PlayerPrefix(string playerName) { return ""; }
        protected virtual string AIPrefix(string AIName) { return ""; }
        protected virtual string PrefixMessageSeparator() { return ""; }
        protected virtual string RequestPrefix() { return ""; }
        protected virtual string RequestSuffix() { return ""; }
        protected virtual string PairSuffix() { return ""; }

        protected virtual bool SystemPromptSupported() { return true; }

        /// <summary> Constructs the prompt using the template based on a list of ChatMessages </summary>
        /// <param name="messages"> list of ChatMessages e.g. the LLMCharacter chat </param>
        /// <param name="AIName"> the AI name </param>
        /// <param name="endWithPrefix"> whether to end the prompt with the AI prefix </param>
        /// <returns>prompt</returns>
        public virtual string ComputePrompt(List<ChatMessage> chatMessages, string playerName, string AIName, bool endWithPrefix = true)
        {
            List<ChatMessage> messages = chatMessages;
            if (!SystemPromptSupported())
            {
                if (chatMessages[0].role == "system")
                {
                    string firstUserMessage = chatMessages[0].content;
                    int newStart = 1;
                    if (chatMessages.Count > 1)
                    {
                        if (firstUserMessage != "") firstUserMessage += "\n\n";
                        firstUserMessage += chatMessages[1].content;
                        newStart = 2;
                    }
                    messages = new List<ChatMessage>(){new ChatMessage { role = playerName, content = firstUserMessage }};
                    messages.AddRange(chatMessages.GetRange(newStart, chatMessages.Count - newStart));
                }
            }

            string chatPrompt = PromptPrefix();
            int start = 0;
            if (messages[0].role == "system")
            {
                chatPrompt += RequestPrefix() + SystemPrefix() + messages[0].content + SystemSuffix();
                start = 1;
            }
            for (int i = start; i < messages.Count; i += 2)
            {
                if (i > start || start == 0) chatPrompt += RequestPrefix();
                chatPrompt += PlayerPrefix(messages[i].role) + PrefixMessageSeparator() + messages[i].content + RequestSuffix();
                if (i < messages.Count - 1)
                {
                    chatPrompt += AIPrefix(messages[i + 1].role) + PrefixMessageSeparator() + messages[i + 1].content + PairSuffix();
                }
            }
            if (endWithPrefix) chatPrompt += AIPrefix(AIName);
            return chatPrompt;
        }

        protected string[] AddStopNewlines(string[] stop)
        {
            List<string> stopWithNewLines = new List<string>();
            foreach (string stopword in stop)
            {
                stopWithNewLines.Add(stopword);
                stopWithNewLines.Add("\n" + stopword);
            }
            return stopWithNewLines.ToArray();
        }
    }

    /// @ingroup template
    /// <summary>
    /// Class implementing the ChatML template
    /// </summary>
    public class ChatMLTemplate : ChatTemplate
    {
        public override string GetName() { return "chatml"; }
        public override string GetDescription() { return "chatml (most widely used)"; }
        public override string[] GetNameMatches() { return new string[] {"chatml", "hermes", "qwen"}; }
        public override string[] GetChatTemplateMatches() { return new string[] {"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"}; }

        protected override string SystemPrefix() { return "<|im_start|>system\n"; }
        protected override string SystemSuffix() { return "<|im_end|>\n"; }
        protected override string PlayerPrefix(string playerName) { return $"<|im_start|>{playerName}\n"; }
        protected override string AIPrefix(string AIName) { return $"<|im_start|>{AIName}\n"; }
        protected override string RequestSuffix() { return "<|im_end|>\n"; }
        protected override string PairSuffix() { return "<|im_end|>\n"; }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { "<|im_start|>", "<|im_end|>" });
        }
    }

    /// @ingroup template
    /// <summary>
    /// Class implementing the LLama2 template
    /// </summary>
    public class LLama2Template : ChatTemplate
    {
        public override string GetName() { return "llama"; }
        public override string GetDescription() { return "llama 2"; }

        protected override string SystemPrefix() { return "<<SYS>>\n"; }
        protected override string SystemSuffix() { return "\n<</SYS>> "; }
        protected override string RequestPrefix() { return "<s>[INST] "; }
        protected override string RequestSuffix() { return " [/INST]"; }
        protected override string PairSuffix() { return " </s>"; }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { "[INST]", "[/INST]" });
        }
    }

    /// @ingroup template
    /// <summary>
    /// Class implementing a modified version of the LLama2 template for chat
    /// </summary>
    public class LLama2ChatTemplate : LLama2Template
    {
        public override string GetName() { return "llama chat"; }
        public override string GetDescription() { return "llama 2 (chat)"; }
        public override string[] GetNameMatches() { return new string[] {"llama-2", "llama v2"}; }

        protected override string PlayerPrefix(string playerName) { return "### " + playerName + ":"; }
        protected override string AIPrefix(string AIName) { return "### " + AIName + ":"; }
        protected override string PrefixMessageSeparator() { return " "; }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { "[INST]", "[/INST]", "###" });
        }
    }

    /// @ingroup template
    /// <summary>
    /// Class implementing the LLama3 template for chat
    /// </summary>
    public class LLama3ChatTemplate : ChatTemplate
    {
        public override string GetName() { return "llama3 chat"; }
        public override string GetDescription() { return "llama 3 (chat)"; }
        public override string[] GetNameMatches() { return new string[] {"llama-3", "llama v3"}; }
        public override string[] GetChatTemplateMatches() { return new string[] {"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}"};}

        protected override string SystemPrefix() { return "<|start_header_id|>system<|end_header_id|>\n\n"; }
        protected override string SystemSuffix() { return "<|eot_id|>"; }

        protected override string RequestSuffix() { return "<|eot_id|>"; }
        protected override string PairSuffix() { return "<|eot_id|>"; }

        protected override string PlayerPrefix(string playerName) { return $"<|start_header_id|>{playerName}<|end_header_id|>\n\n"; }
        protected override string AIPrefix(string AIName) { return $"<|start_header_id|>{AIName}<|end_header_id|>\n\n"; }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { "<|eot_id|>" });
        }
    }

    /// @ingroup template
    /// <summary>
    /// Class implementing the Mistral Instruct template
    /// </summary>
    public class MistralInstructTemplate : ChatTemplate
    {
        public override string GetName() { return "mistral instruct"; }
        public override string GetDescription() { return "mistral instruct"; }

        protected override string SystemPrefix() { return ""; }
        protected override string SystemSuffix() { return "\n\n"; }
        protected override string RequestPrefix() { return "[INST] "; }
        protected override string RequestSuffix() { return " [/INST]"; }
        protected override string PairSuffix() { return "</s>"; }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { "</s>", "[INST]", "[/INST]" });
        }
    }

    /// @ingroup template
    /// <summary>
    /// Class implementing a modified version of the Mistral Instruct template for chat
    /// </summary>
    public class MistralChatTemplate : MistralInstructTemplate
    {
        public override string GetName() { return "mistral chat"; }
        public override string GetDescription() { return "mistral (chat)"; }
        public override string[] GetNameMatches() { return new string[] {"mistral"}; }
        public override string[] GetChatTemplateMatches() { return new string[] {"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}"}; }

        protected override string PlayerPrefix(string playerName) { return "### " + playerName + ":"; }
        protected override string AIPrefix(string AIName) { return "### " + AIName + ":"; }
        protected override string PrefixMessageSeparator() { return " "; }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { "</s>", "[INST]", "[/INST]", "###" });
        }
    }

    /// @ingroup template
    /// <summary>
    /// Class implementing the Gemma template
    /// </summary>
    public class GemmaTemplate : ChatTemplate
    {
        public override string GetName() { return "gemma"; }
        public override string GetDescription() { return "gemma"; }
        public override string[] GetNameMatches() { return new string[] {"gemma"}; }
        public override string[] GetChatTemplateMatches() { return new string[] {"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n' + message['content'] | trim + '<end_of_turn>\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n'}}{% endif %}"}; }

        protected override string RequestSuffix() { return "<end_of_turn>\n"; }
        protected override string PairSuffix() { return "<end_of_turn>\n"; }

        protected override string PlayerPrefix(string playerName) { return "<start_of_turn>user\n"; }
        protected override string AIPrefix(string AIName) { return "<start_of_turn>model\n"; }

        protected override bool SystemPromptSupported() { return false; }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { "<start_of_turn>", "<end_of_turn>" });
        }
    }

    /// @ingroup template
    /// <summary>
    /// Class implementing the Alpaca template
    /// </summary>
    public class AlpacaTemplate : ChatTemplate
    {
        public override string GetName() { return "alpaca"; }
        public override string GetDescription() { return "alpaca (best alternative)"; }
        public override string[] GetNameMatches() { return new string[] {"alpaca"}; }

        protected override string SystemSuffix() { return "\n\n"; }
        protected override string RequestSuffix() { return "\n"; }
        protected override string PlayerPrefix(string playerName) { return "### " + playerName + ":"; }
        protected override string AIPrefix(string AIName) { return "### " + AIName + ":"; }
        protected override string PrefixMessageSeparator() { return " "; }
        protected override string PairSuffix() { return "\n"; }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { "###" });
        }
    }

    /// @ingroup template
    /// <summary>
    /// Class implementing the Vicuna template
    /// </summary>
    public class VicunaTemplate : ChatTemplate
    {
        public override string GetName() { return "vicuna"; }
        public override string GetDescription() { return "vicuna"; }
        public override string[] GetNameMatches() { return new string[] {"vicuna"}; }
        public override string[] GetChatTemplateMatches() { return new string[] {"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{% if message['role'] == 'system' %}{{message['content'] + ' '}}{% elif message['role'] == 'user' %}{{ 'USER: ' + message['content'] + ' '}}{% elif message['role'] == 'assistant' %}{{ 'ASSISTANT: ' + message['content'] + ' '}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'ASSISTANT: '}}{% endif %}"}; }

        protected override string SystemSuffix() { return "\n"; }
        protected override string PlayerPrefix(string playerName) { return "\n" + playerName + ":"; }
        protected override string AIPrefix(string AIName) { return "\n" + AIName + ":"; }
        protected override string PrefixMessageSeparator() { return " "; }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { playerName + ":", AIName + ":" });
        }
    }

    /// @ingroup template
    /// <summary>
    /// Class implementing the Phi-2 template
    /// </summary>
    public class Phi2Template : ChatTemplate
    {
        public override string GetName() { return "phi"; }
        public override string GetDescription() { return "phi-2"; }
        public override string[] GetNameMatches() { return new string[] {"phi-2"}; }

        protected override string SystemSuffix() { return "\n\n"; }
        protected override string RequestSuffix() { return "\n"; }
        protected override string PlayerPrefix(string playerName) { return playerName + ":"; }
        protected override string AIPrefix(string AIName) { return AIName + ":"; }
        protected override string PrefixMessageSeparator() { return " "; }
        protected override string PairSuffix() { return "\n"; }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { playerName + ":", AIName + ":" });
        }
    }

    /// @ingroup template
    /// <summary>
    /// Class implementing the Phi-3 template
    /// </summary>
    public class Phi3Template : ChatTemplate
    {
        public override string GetName() { return "phi-3"; }
        public override string GetDescription() { return "phi-3"; }
        public override string[] GetNameMatches() { return new string[] {"phi-3"}; }
        public override string[] GetChatTemplateMatches() { return new string[] {"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') %}{{'<|user|>' + '\n' + message['content'] + '<|end|>' + '\n' + '<|assistant|>' + '\n'}}{% elif (message['role'] == 'assistant') %}{{message['content'] + '<|end|>' + '\n'}}{% endif %}{% endfor %}"}; }

        protected override string PlayerPrefix(string playerName) { return $"<|user|>\n"; }
        protected override string AIPrefix(string AIName) { return $"<|assistant|>\n"; }
        protected override string RequestSuffix() { return "<|end|>\n"; }
        protected override string PairSuffix() { return "<|end|>\n"; }

        protected override bool SystemPromptSupported() { return false; }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { "<|end|>", "<|user|>", "<|assistant|>" });
        }
    }

    /// @ingroup template
    /// <summary>
    /// Class implementing the Phi-4 mini template
    /// </summary>
    public class Phi3_5Template : ChatTemplate
    {
        public override string GetName() { return "phi-3.5"; }
        public override string GetDescription() { return "phi-3.5"; }
        public override string[] GetNameMatches() { return new string[] {"phi-3.5"}; }
        public override string[] GetChatTemplateMatches() { return new string[] {"{% for message in messages %}{% if message['role'] == 'system' and message['content'] %}{{'<|system|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\n' + message['content'] + '<|end|>\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n' + message['content'] + '<|end|>\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n' }}{% else %}{{ eos_token }}{% endif %}"};}

        protected override string PlayerPrefix(string playerName) { return $"<|user|>\n"; }
        protected override string AIPrefix(string AIName) { return $"<|assistant|>\n"; }
        protected override string RequestSuffix() { return "<|end|>\n"; }
        protected override string PairSuffix() { return "<|end|>\n"; }
        protected override string SystemPrefix() { return "<|system|>\n"; }
        protected override string SystemSuffix() { return "<|end|>\n"; }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { "<|end|>", "<|user|>", "<|assistant|>" });
        }
    }

    /// @ingroup template
    /// <summary>
    /// Class implementing the Phi-4 mini template
    /// </summary>
    public class Phi4MiniTemplate : ChatTemplate
    {
        public override string GetName() { return "phi-4-mini"; }
        public override string GetDescription() { return "phi-4-mini"; }
        public override string[] GetNameMatches() { return new string[] {"phi-4-mini"}; }
        public override string[] GetChatTemplateMatches() { return new string[] {"{% for message in messages %}{% if message['role'] == 'system' and 'tools' in message and message['tools'] is not none %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|tool|>' + message['tools'] + '<|/tool|>' + '<|end|>' }}{% else %}{{ '<|' + message['role'] + '|>' + message['content'] + '<|end|>' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>' }}{% else %}{{ eos_token }}{% endif %}"};}

        protected override string PlayerPrefix(string playerName) { return $"<|user|>"; }
        protected override string AIPrefix(string AIName) { return $"<|assistant|>"; }
        protected override string RequestSuffix() { return "<|end|>"; }
        protected override string PairSuffix() { return "<|end|>"; }
        protected override string SystemPrefix() { return "<|system|>"; }
        protected override string SystemSuffix() { return "<|end|>"; }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { "<|end|>", "<|user|>", "<|assistant|>" });
        }
    }
    /// @ingroup template
    /// <summary>
    /// Class implementing the Phi-4 template
    /// </summary>
    public class Phi4Template : ChatTemplate
    {
        public override string GetName() { return "phi-4"; }
        public override string GetDescription() { return "phi-4"; }
        public override string[] GetNameMatches() { return new string[] {"phi-4"}; }
        public override string[] GetChatTemplateMatches() { return new string[] {"{% for message in messages %}{% if (message['role'] == 'system') %}{{'<|im_start|>system<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'user') %}{{'<|im_start|>user<|im_sep|>' + message['content'] + '<|im_end|>'}}{% elif (message['role'] == 'assistant') %}{{'<|im_start|>assistant<|im_sep|>' + message['content'] + '<|im_end|>'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant<|im_sep|>' }}{% endif %}"};}

        protected override string PlayerPrefix(string playerName) { return $"<|im_start|>user<|im_sep|>"; }
        protected override string AIPrefix(string AIName) { return $"<|im_start|>assistant<|im_sep|>"; }
        protected override string RequestSuffix() { return "<|im_end|>"; }
        protected override string PairSuffix() { return "<|im_end|>"; }
        protected override string SystemPrefix() { return "<|im_start|>system<|im_sep|>"; }
        protected override string SystemSuffix() { return "<|im_end|>"; }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { "<|im_end|>", "<|im_start|>" });
        }
    }

    /// @ingroup template
    /// <summary>
    /// Class implementing the Zephyr template
    /// </summary>
    public class ZephyrTemplate : ChatTemplate
    {
        public override string GetName() { return "zephyr"; }
        public override string GetDescription() { return "zephyr"; }
        public override string[] GetNameMatches() { return new string[] {"zephyr"}; }
        public override string[] GetChatTemplateMatches() { return new string[] {"{% for message in messages %}\n{% if message['role'] == 'user' %}\n{{ '<|user|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'system' %}\n{{ '<|system|>\n' + message['content'] + eos_token }}\n{% elif message['role'] == 'assistant' %}\n{{ '<|assistant|>\n'  + message['content'] + eos_token }}\n{% endif %}\n{% if loop.last and add_generation_prompt %}\n{{ '<|assistant|>' }}\n{% endif %}\n{% endfor %}"}; }

        protected override string SystemPrefix() { return "<|system|>\n"; }
        protected override string SystemSuffix() { return "</s>\n"; }
        protected override string PlayerPrefix(string playerName) { return $"<|user|>\n"; }
        protected override string AIPrefix(string AIName) { return $"<|assistant|>\n"; }
        protected override string RequestSuffix() { return "</s>\n"; }
        protected override string PairSuffix() { return "</s>\n"; }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { $"<|user|>", $"<|assistant|>" });
        }
    }

    /// @ingroup template
    /// <summary>
    /// Class implementing the DeepSeek V2 template
    /// </summary>
    public class DeepSeekV2Template : ChatTemplate
    {
        public override string GetName() { return "deepseek-v2"; }
        public override string GetDescription() { return "deepseek-v2"; }
        public override string[] GetNameMatches() { return new string[] {"deepseek-v2", "deepseek-llm"}; }
        public override string[] GetChatTemplateMatches() { return new string[] {"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{{ bos_token }}{% for message in messages %}{% if message['role'] == 'user' %}{{ 'User: ' + message['content'] + '\n\n' }}{% elif message['role'] == 'assistant' %}{{ 'Assistant: ' + message['content'] + eos_token }}{% elif message['role'] == 'system' %}{{ message['content'] + '\n\n' }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ 'Assistant:' }}{% endif %}"}; }

        protected override string PrefixMessageSeparator() { return " "; }
        protected override string PromptPrefix() { return "<ÔΩúbegin‚ñÅof‚ñÅsentenceÔΩú>"; }
        protected override string PlayerPrefix(string playerName) { return "User:"; }
        protected override string AIPrefix(string AIName) { return "Assistant:"; }
        protected override string PairSuffix() { return "<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>"; }
        protected override string RequestSuffix() { return "\n\n"; }
        protected override string SystemSuffix() { return "\n\n"; }

        // protected override bool SystemPromptSupported() { return false; }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { "<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>", "User:", "Assistant:" });
        }
    }

    /// @ingroup template
    /// <summary>
    /// Class implementing the DeepSeek V3 template
    /// </summary>
    public class DeepSeekV3Template : DeepSeekV2Template
    {
        public override string GetName() { return "deepseek-v3"; }
        public override string GetDescription() { return "deepseek-v3"; }
        public override string[] GetNameMatches() { return new string[] {"deepseek-v2.5", "deepseek-v3"}; }
        public override string[] GetChatTemplateMatches()
        {
            return new string[]
            {
                "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<ÔΩúUserÔΩú>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>' + tool['type'] + '<ÔΩútool‚ñÅsepÔΩú>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ÔΩútool‚ñÅcall‚ñÅendÔΩú>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>' + tool['type'] + '<ÔΩútool‚ñÅsepÔΩú>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ÔΩútool‚ñÅcall‚ñÅendÔΩú>'}}{{'<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>' + message['content'] + '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- set ns.is_tool = false -%}{%- else %}{{'<ÔΩúAssistantÔΩú>' + message['content'] + '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>' + message['content'] + '<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>' + message['content'] + '<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<ÔΩúAssistantÔΩú>'}}{% endif %}",
                "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\n\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<ÔΩúUserÔΩú>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>' + tool['type'] + '<ÔΩútool‚ñÅsepÔΩú>' + tool['function']['name'] + '\n' + '```json' + '\n' + tool['function']['arguments'] + '\n' + '```' + '<ÔΩútool‚ñÅcall‚ñÅendÔΩú>'}}{%- set ns.is_first = true -%}{%- else %}{{'\n' + '<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>' + tool['type'] + '<ÔΩútool‚ñÅsepÔΩú>' + tool['function']['name'] + '\n' + '```json' + '\n' + tool['function']['arguments'] + '\n' + '```' + '<ÔΩútool‚ñÅcall‚ñÅendÔΩú>'}}{{'<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>' + message['content'] + '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- set ns.is_tool = false -%}{%- else %}{{'<ÔΩúAssistantÔΩú>' + message['content'] + '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>' + message['content'] + '<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>' + message['content'] + '<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<ÔΩúAssistantÔΩú>'}}{% endif %}"
            };
        }

        protected override string PrefixMessageSeparator() { return ""; }
        protected override string PlayerPrefix(string playerName) { return "<ÔΩúUserÔΩú>"; }
        protected override string AIPrefix(string AIName) { return "<ÔΩúAssistantÔΩú>"; }
        protected override string RequestSuffix() { return ""; }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { "<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>", "<ÔΩúUserÔΩú>", "<ÔΩúAssistantÔΩú>" });
        }
    }

    /// @ingroup template
    /// <summary>
    /// Class implementing the DeepSeek R1 template
    /// </summary>
    public class DeepSeekR1Template : DeepSeekV3Template
    {
        public override string GetName() { return "deepseek-r1"; }
        public override string GetDescription() { return "deepseek-r1"; }
        public override string[] GetNameMatches() { return new string[] {"deepseek-r1"}; }
        public override string[] GetChatTemplateMatches()
        {
            return new string[]
            {
                "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='', is_first_sp=true) %}{%- for message in messages %}{%- if message['role'] == 'system' %}{%- if ns.is_first_sp %}{% set ns.system_prompt = ns.system_prompt + message['content'] %}{% set ns.is_first_sp = false %}{%- else %}{% set ns.system_prompt = ns.system_prompt + '\\n\\n' + message['content'] %}{%- endif %}{%- endif %}{%- endfor %}{{ bos_token }}{{ ns.system_prompt }}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<ÔΩúUserÔΩú>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' in message %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls'] %}{%- if not ns.is_first %}{%- if message['content'] is none %}{{'<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>' + tool['type'] + '<ÔΩútool‚ñÅsepÔΩú>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ÔΩútool‚ñÅcall‚ñÅendÔΩú>'}}{%- else %}{{'<ÔΩúAssistantÔΩú>' + message['content'] + '<ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>' + tool['type'] + '<ÔΩútool‚ñÅsepÔΩú>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ÔΩútool‚ñÅcall‚ñÅendÔΩú>'}}{%- endif %}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>' + tool['type'] + '<ÔΩútool‚ñÅsepÔΩú>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ÔΩútool‚ñÅcall‚ñÅendÔΩú>'}}{%- endif %}{%- endfor %}{{'<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- endif %}{%- if message['role'] == 'assistant' and 'tool_calls' not in message %}{%- if ns.is_tool %}{{'<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>' + message['content'] + '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<ÔΩúAssistantÔΩú>' + content + '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>' + message['content'] + '<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>'}}{%- set ns.is_output_first = false %}{%- else %}{{'<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>' + message['content'] + '<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<ÔΩúAssistantÔΩú><think>\\n'}}{% endif %}",
                "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set ns = namespace(is_first=false, is_tool=false, is_output_first=true, system_prompt='') %}{%- for message in messages %}{%- if message['role'] == 'system' %}{% set ns.system_prompt = message['content'] %}{%- endif %}{%- endfor %}{{bos_token}}{{ns.system_prompt}}{%- for message in messages %}{%- if message['role'] == 'user' %}{%- set ns.is_tool = false -%}{{'<ÔΩúUserÔΩú>' + message['content']}}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is none %}{%- set ns.is_tool = false -%}{%- for tool in message['tool_calls']%}{%- if not ns.is_first %}{{'<ÔΩúAssistantÔΩú><ÔΩútool‚ñÅcalls‚ñÅbeginÔΩú><ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>' + tool['type'] + '<ÔΩútool‚ñÅsepÔΩú>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ÔΩútool‚ñÅcall‚ñÅendÔΩú>'}}{%- set ns.is_first = true -%}{%- else %}{{'\\n' + '<ÔΩútool‚ñÅcall‚ñÅbeginÔΩú>' + tool['type'] + '<ÔΩútool‚ñÅsepÔΩú>' + tool['function']['name'] + '\\n' + '```json' + '\\n' + tool['function']['arguments'] + '\\n' + '```' + '<ÔΩútool‚ñÅcall‚ñÅendÔΩú>'}}{{'<ÔΩútool‚ñÅcalls‚ñÅendÔΩú><ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- endif %}{%- endfor %}{%- endif %}{%- if message['role'] == 'assistant' and message['content'] is not none %}{%- if ns.is_tool %}{{'<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>' + message['content'] + '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- set ns.is_tool = false -%}{%- else %}{% set content = message['content'] %}{% if '</think>' in content %}{% set content = content.split('</think>')[-1] %}{% endif %}{{'<ÔΩúAssistantÔΩú>' + content + '<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>'}}{%- endif %}{%- endif %}{%- if message['role'] == 'tool' %}{%- set ns.is_tool = true -%}{%- if ns.is_output_first %}{{'<ÔΩútool‚ñÅoutputs‚ñÅbeginÔΩú><ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>' + message['content'] + '<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>'}}{%- set ns.is_output_first = false %}{%- else %}{{'\\n<ÔΩútool‚ñÅoutput‚ñÅbeginÔΩú>' + message['content'] + '<ÔΩútool‚ñÅoutput‚ñÅendÔΩú>'}}{%- endif %}{%- endif %}{%- endfor -%}{% if ns.is_tool %}{{'<ÔΩútool‚ñÅoutputs‚ñÅendÔΩú>'}}{% endif %}{% if add_generation_prompt and not ns.is_tool %}{{'<ÔΩúAssistantÔΩú><think>\\n'}}{% endif %}"
            };
        }

        public override string ComputePrompt(List<ChatMessage> chatMessages, string playerName, string AIName, bool endWithPrefix = true)
        {
            string prompt = base.ComputePrompt(chatMessages, playerName, AIName, endWithPrefix);
            if (endWithPrefix) prompt += "<think>\n\n</think>\n\n";
            return prompt;
        }

        public override string[] GetStop(string playerName, string AIName)
        {
            return AddStopNewlines(new string[] { "<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>", "<ÔΩúUserÔΩú>", "<ÔΩúAssistantÔΩú>", "</think>" });
        }
    }
}


# -------------------- LLMEmbedder.cs --------------------

/// @file
/// @brief File implementing the LLM embedder.
using UnityEngine;

namespace LLMUnity
{
    [DefaultExecutionOrder(-2)]
    /// @ingroup llm
    /// <summary>
    /// Class implementing the LLM embedder.
    /// </summary>
    public class LLMEmbedder : LLMCaller
    {
        protected override void SetLLM(LLM llmSet)
        {
            base.SetLLM(llmSet);
            if (llmSet != null && !llmSet.embeddingsOnly)
            {
                LLMUnitySetup.LogWarning($"The LLM {llmSet.name} set for LLMEmbeddings {gameObject.name} is not an embeddings-only model, accuracy may be sub-optimal");
            }
        }

        public override bool IsAutoAssignableLLM(LLM llmSet)
        {
            return llmSet.embeddingsOnly;
        }
    }
}


# -------------------- LLMGGUF.cs --------------------

/// @file
/// @brief File implementing the GGUF reader.
using System;
using System.Collections.Generic;
using System.IO;
using System.Runtime.InteropServices;

namespace LLMUnity
{
    /// \cond HIDE
    public enum GGUFValueType
    {
        UINT8 = 0,
        INT8 = 1,
        UINT16 = 2,
        INT16 = 3,
        UINT32 = 4,
        INT32 = 5,
        FLOAT32 = 6,
        BOOL = 7,
        STRING = 8,
        ARRAY = 9,
        UINT64 = 10,
        INT64 = 11,
        FLOAT64 = 12
    }

    public class ReaderField
    {
        public int offset;
        public string name;
        public List<Array> parts = new List<Array>();
        public List<int> data = new List<int>();
        public List<GGUFValueType> types = new List<GGUFValueType>();
    }

    public class ReaderTensor
    {
        public string name;
        public GGUFValueType tensor_type;
        public uint[] shape;
        public int n_elements;
        public int n_bytes;
        public int data_offset;
        public Array data;
        public ReaderField field;
    }
    /// \endcond

    /// @ingroup utils
    /// <summary>
    /// Class implementing the GGUF reader.
    /// </summary>
    public class GGUFReader
    {
        private const uint GGUF_MAGIC = 0x46554747; // "GGUF"
        private const int GGUF_VERSION = 3;
        private readonly List<int> READER_SUPPORTED_VERSIONS = new List<int> { 2, GGUF_VERSION };
        private Dictionary<GGUFValueType, Type> gguf_scalar_to_np = new Dictionary<GGUFValueType, Type>
        {
            { GGUFValueType.UINT8, typeof(byte) },
            { GGUFValueType.INT8, typeof(sbyte) },
            { GGUFValueType.UINT16, typeof(ushort) },
            { GGUFValueType.INT16, typeof(short) },
            { GGUFValueType.UINT32, typeof(uint) },
            { GGUFValueType.INT32, typeof(int) },
            { GGUFValueType.FLOAT32, typeof(float) },
            { GGUFValueType.UINT64, typeof(ulong) },
            { GGUFValueType.INT64, typeof(long) },
            { GGUFValueType.FLOAT64, typeof(double) },
            { GGUFValueType.BOOL, typeof(bool) }
        };

        // private MemoryStream data;
        private FileStream data;
        /// <summary> Dictionary of GGUF fields to location info </summary>
        public Dictionary<string, ReaderField> fields = new Dictionary<string, ReaderField>();

        /// <summary>
        /// Constructor of the GGUF reader that parses a GGUF file and retrieves the fields.
        /// </summary>
        /// <param name="path">GGUF file path to read</param>
        public GGUFReader(string path)
        {
            // data = new MemoryStream(File.ReadAllBytes(path));
            data = new FileStream(path, FileMode.Open, FileAccess.Read);
            int offs = 0;

            if (BitConverter.ToUInt32(ReadBytes(offs, 4), 0) != GGUF_MAGIC)
                throw new ArgumentException("GGUF magic invalid");
            offs += 4;

            uint temp_version = BitConverter.ToUInt32(ReadBytes(offs, 4));
            if ((temp_version & 65535) == 0)
            {
                byte[] tempBytes = ReadBytes(offs, 4);
                Array.Reverse(tempBytes);
                temp_version = BitConverter.ToUInt32(tempBytes, 0);
            }
            uint version = temp_version;

            if (!READER_SUPPORTED_VERSIONS.Contains((int)version))
                throw new ArgumentException($"Sorry, file appears to be version {version} which we cannot handle");

            offs += PushField(new ReaderField { offset = offs, name = "GGUF.version", parts = new List<Array> { new uint[] { temp_version } }, data = new List<int> { 0 }, types = new List<GGUFValueType> { GGUFValueType.UINT32 } });
            ulong[] temp_counts = new ulong[2];
            Buffer.BlockCopy(ReadBytes(offs, 16), 0, temp_counts, 0, 16);
            offs += PushField(new ReaderField { offset = offs, name = "GGUF.tensor_count", parts = new List<Array> { new ulong[] { temp_counts[0] } }, data = new List<int> { 0 }, types = new List<GGUFValueType> { GGUFValueType.UINT64 } });
            offs += PushField(new ReaderField { offset = offs, name = "GGUF.kv_count", parts = new List<Array> { new ulong[] { temp_counts[1] } }, data = new List<int> { 0 }, types = new List<GGUFValueType> { GGUFValueType.UINT64 } });
            ulong tensor_count = temp_counts[0];
            ulong kv_count = temp_counts[1];
            offs = BuildFields(offs, (int)kv_count);
            data.Close();
        }

        /// <summary>
        /// Allows to retrieve location info for a GGUF field.
        /// </summary>
        /// <param name="key"> GGUF field to retrieve </param>
        /// <returns> Retrieved location info as ReaderField </returns>
        public ReaderField GetField(string key)
        {
            if (fields.TryGetValue(key, out ReaderField value))
                return value;
            return null;
        }

        /// <summary>
        /// Allows to retrieve a single-valued GGUF field.
        /// </summary>
        /// <param name="key"> GGUF field to retrieve </param>
        /// <returns> Retrieved location info as ReaderField </returns>
        public byte[] GetGenericField(string key)
        {
            ReaderField field = GetField(key);
            if (field == null || field.parts.Count == 0) return null;
            return (byte[])field.parts[field.parts.Count - 1];
        }

        /// <summary>
        /// Allows to retrieve a string GGUF field.
        /// </summary>
        /// <param name="key"> GGUF field to retrieve </param>
        /// <returns> Retrieved GGUF value </returns>
        public string GetStringField(string key)
        {
            byte[] value = GetGenericField(key);
            if (value == null) return null;
            return System.Text.Encoding.UTF8.GetString(value);
        }

        /// <summary>
        /// Allows to retrieve an integer GGUF field.
        /// </summary>
        /// <param name="key"> GGUF field to retrieve </param>
        /// <returns> Retrieved GGUF value </returns>
        public int GetIntField(string key)
        {
            byte[] value = GetGenericField(key);
            if (value == null) return -1;
            return BitConverter.ToInt32(value, 0);
        }

        private byte[] ReadBytes(int offset, int count)
        {
            byte[] buffer = new byte[count];
            data.Seek(offset, SeekOrigin.Begin);
            data.Read(buffer, 0, count);
            return buffer;
        }

        private int PushField(ReaderField field, bool skip_sum = false)
        {
            if (fields.ContainsKey(field.name))
                throw new ArgumentException($"Duplicate {field.name} already in list at offset {field.offset}");
            fields[field.name] = field;
            if (skip_sum)
                return 0;
            int sum = 0;
            for (int i = 0; i < field.parts.Count; i++)
            {
                Type partType = gguf_scalar_to_np[field.types[i]];
                sum += Marshal.SizeOf(partType) *  field.parts[i].Length;
            }
            return sum;
        }

        private (ulong[], byte[]) GetStr(int offset)
        {
            ulong slen = BitConverter.ToUInt64(ReadBytes(offset, 8));
            byte[] sdata = ReadBytes(offset + 8, (int)slen);
            return (new ulong[] { slen }, sdata);
        }

        private (int, List<Array>, List<int>, List<GGUFValueType>) GetFieldParts(int orig_offs, int raw_type)
        {
            int offs = orig_offs;
            List<GGUFValueType> types = new List<GGUFValueType>();
            types.Add((GGUFValueType)raw_type);
            // Handle strings.
            if ((GGUFValueType)raw_type == GGUFValueType.STRING)
            {
                (ulong[] slen, byte[] sdata) = GetStr(offs);
                List<Array> sparts = new List<Array> { slen, sdata };
                int size = slen.Length * sizeof(ulong) + sdata.Length;
                return (size, sparts, new List<int> { 1 }, types);
            }

            // Check if it's a simple scalar type.
            if (gguf_scalar_to_np.TryGetValue((GGUFValueType)raw_type, out Type nptype))
            {
                Array val = ReadBytes(offs, Marshal.SizeOf(nptype));
                int size = nptype == typeof(bool) ? 1 : Marshal.SizeOf(nptype);
                return (size, new List<Array> { val }, new List<int> { 0 }, types);
            }

            // Handle arrays.
            if ((GGUFValueType)raw_type == GGUFValueType.ARRAY)
            {
                int raw_itype = BitConverter.ToInt32(ReadBytes(offs, 4));
                offs += Marshal.SizeOf(typeof(int));

                ulong alen = BitConverter.ToUInt64(ReadBytes(offs, 8));
                offs += Marshal.SizeOf(typeof(ulong));

                List<Array> aparts = new List<Array> { BitConverter.GetBytes(raw_itype), BitConverter.GetBytes(alen) };
                List<int> data_idxs = new List<int>();

                for (int idx = 0; idx < (int)alen; idx++)
                {
                    (int curr_size, List<Array> curr_parts, List<int> curr_idxs, List<GGUFValueType> curr_types) = GetFieldParts(offs, raw_itype);
                    if (idx == 0)
                        types.AddRange(curr_types);

                    int idxs_offs = aparts.Count;
                    aparts.AddRange(curr_parts);
                    data_idxs.AddRange(new List<int>(curr_idxs.ConvertAll(i => i + idxs_offs)));
                    offs += curr_size;
                }
                return (offs - orig_offs, aparts, data_idxs, types);
            }
            // We can't deal with this one.
            throw new ArgumentException($"Unknown/unhandled field type {(GGUFValueType)raw_type}");
        }

        private int BuildFields(int offs, int count)
        {
            for (int i = 0; i < count; i++)
            {
                int orig_offs = offs;
                (ulong[] kv_klen, byte[] kv_kdata) = GetStr(offs);
                offs += Marshal.SizeOf(typeof(ulong)) + kv_kdata.Length;

                int raw_kv_type = BitConverter.ToInt32(ReadBytes(offs, 4));
                offs += Marshal.SizeOf(typeof(int));
                List<Array> parts = new List<Array> { kv_klen, kv_kdata, BitConverter.GetBytes(raw_kv_type) };
                List<int> idxs_offs = new List<int> { parts.Count };

                (int field_size, List<Array> field_parts, List<int> field_idxs, List<GGUFValueType> field_types) = GetFieldParts(offs, raw_kv_type);
                if (field_size == -1)
                    continue;

                parts.AddRange(field_parts);
                ReaderField readerField = new ReaderField
                {
                    offset = orig_offs,
                    name = System.Text.Encoding.UTF8.GetString(kv_kdata),
                    parts = parts,
                    data = new List<int>(field_idxs.ConvertAll(idx => idx + idxs_offs[0])),
                    types = field_types
                };
                PushField(readerField, skip_sum: true);
                offs += field_size;
            }
            return offs;
        }
    }
}


# -------------------- LLMInterface.cs --------------------

/// @file
/// @brief File implementing the LLM server interfaces.
using System;
using System.Collections.Generic;

/// \cond HIDE
namespace LLMUnity
{
    [Serializable]
    public struct ChatRequest
    {
        public string prompt;
        public int id_slot;
        public float temperature;
        public int top_k;
        public float top_p;
        public float min_p;
        public int n_predict;
        public int n_keep;
        public bool stream;
        public List<string> stop;
        public float tfs_z;
        public float typical_p;
        public float repeat_penalty;
        public int repeat_last_n;
        public bool penalize_nl;
        public float presence_penalty;
        public float frequency_penalty;
        public string penalty_prompt;
        public int mirostat;
        public float mirostat_tau;
        public float mirostat_eta;
        public string grammar;
        public int seed;
        public bool ignore_eos;
        public Dictionary<int, string> logit_bias;
        public int n_probs;
        public bool cache_prompt;
        public List<ChatMessage> messages;
    }

    [Serializable]
    public struct SystemPromptRequest
    {
        public string prompt;
        public string system_prompt;
        public int n_predict;
    }

    [Serializable]
    public struct ChatResult
    {
        public int id_slot;
        public string content;
        public bool stop;
        public string generation_settings;
        public string model;
        public string prompt;
        public bool stopped_eos;
        public bool stopped_limit;
        public bool stopped_word;
        public string stopping_word;
        public string timings;
        public int tokens_cached;
        public int tokens_evaluated;
        public bool truncated;
        public bool cache_prompt;
        public bool system_prompt;
    }

    [Serializable]
    public struct MultiChatResult
    {
        public List<ChatResult> data;
    }

    [Serializable]
    public struct ChatMessage
    {
        public string role;
        public string content;
    }

    [Serializable]
    public struct TokenizeRequest
    {
        public string content;
    }

    [Serializable]
    public struct TokenizeResult
    {
        public List<int> tokens;
    }

    [Serializable]
    public struct EmbeddingsResult
    {
        public List<float> embedding;
    }

    [Serializable]
    public struct LoraWeightRequest
    {
        public int id;
        public float scale;
    }

    [Serializable]
    public struct LoraWeightRequestList
    {
        public List<LoraWeightRequest> loraWeights;
    }

    [Serializable]
    public struct LoraWeightResult
    {
        public int id;
        public string path;
        public float scale;
    }

    [Serializable]
    public struct LoraWeightResultList
    {
        public List<LoraWeightResult> loraWeights;
    }

    [Serializable]
    public struct TemplateResult
    {
        public string template;
    }

    [Serializable]
    public struct SlotRequest
    {
        public int id_slot;
        public string action;
        public string filepath;
    }

    [Serializable]
    public struct SlotResult
    {
        public int id_slot;
        public string filename;
    }
}
/// \endcond


# -------------------- LLMLib.cs --------------------

/// @file
/// @brief File implementing the LLM library calls.
/// \cond HIDE
using System;
using System.Collections.Generic;
using System.IO;
using System.Runtime.InteropServices;
using UnityEngine;

namespace LLMUnity
{
    /// @ingroup utils
    /// <summary>
    /// Class implementing a wrapper for a communication stream between Unity and the llama.cpp library (mainly for completion calls and logging).
    /// </summary>
    public class StreamWrapper
    {
        LLMLib llmlib;
        Callback<string> callback;
        IntPtr stringWrapper;
        string previousString = "";
        string previousCalledString = "";
        int previousBufferSize = 0;
        bool clearOnUpdate;

        public StreamWrapper(LLMLib llmlib, Callback<string> callback, bool clearOnUpdate = false)
        {
            this.llmlib = llmlib;
            this.callback = callback;
            this.clearOnUpdate = clearOnUpdate;
            stringWrapper = (llmlib?.StringWrapper_Construct()).GetValueOrDefault();
        }

        /// <summary>
        /// Retrieves the content of the stream
        /// </summary>
        /// <param name="clear">whether to clear the stream after retrieving the content</param>
        /// <returns>stream content</returns>
        public string GetString(bool clear = false)
        {
            string result;
            int bufferSize = (llmlib?.StringWrapper_GetStringSize(stringWrapper)).GetValueOrDefault();
            if (bufferSize <= 1)
            {
                result = "";
            }
            else if (previousBufferSize != bufferSize)
            {
                IntPtr buffer = Marshal.AllocHGlobal(bufferSize);
                try
                {
                    llmlib?.StringWrapper_GetString(stringWrapper, buffer, bufferSize, clear);
                    result = Marshal.PtrToStringAnsi(buffer);
                }
                finally
                {
                    Marshal.FreeHGlobal(buffer);
                }
                previousString = result;
            }
            else
            {
                result = previousString;
            }
            previousBufferSize = bufferSize;
            return result;
        }

        /// <summary>
        /// Unity Update implementation that retrieves the content and calls the callback if it has changed.
        /// </summary>
        public void Update()
        {
            if (stringWrapper == IntPtr.Zero) return;
            string result = GetString(clearOnUpdate);
            if (result != "" && previousCalledString != result)
            {
                callback?.Invoke(result);
                previousCalledString = result;
            }
        }

        /// <summary>
        /// Gets the stringWrapper object to pass to the library.
        /// </summary>
        /// <returns>stringWrapper object</returns>
        public IntPtr GetStringWrapper()
        {
            return stringWrapper;
        }

        /// <summary>
        /// Deletes the stringWrapper object.
        /// </summary>
        public void Destroy()
        {
            if (stringWrapper != IntPtr.Zero) llmlib?.StringWrapper_Delete(stringWrapper);
        }
    }

    /// @ingroup utils
    /// <summary>
    /// Class implementing a library loader for Unity.
    /// Adapted from SkiaForUnity:
    /// https://github.com/ammariqais/SkiaForUnity/blob/f43322218c736d1c41f3a3df9355b90db4259a07/SkiaUnity/Assets/SkiaSharp/SkiaSharp-Bindings/SkiaSharp.HarfBuzz.Shared/HarfBuzzSharp.Shared/LibraryLoader.cs
    /// </summary>
    static class LibraryLoader
    {
        /// <summary>
        /// Allows to retrieve a function delegate for the library
        /// </summary>
        /// <typeparam name="T">type to cast the function</typeparam>
        /// <param name="library">library handle</param>
        /// <param name="name">function name</param>
        /// <returns>function delegate</returns>
        public static T GetSymbolDelegate<T>(IntPtr library, string name) where T : Delegate
        {
            var symbol = GetSymbol(library, name);
            if (symbol == IntPtr.Zero)
                throw new EntryPointNotFoundException($"Unable to load symbol '{name}'.");

            return Marshal.GetDelegateForFunctionPointer<T>(symbol);
        }

        /// <summary>
        /// Loads the provided library in a cross-platform manner
        /// </summary>
        /// <param name="libraryName">library path</param>
        /// <returns>library handle</returns>
        public static IntPtr LoadLibrary(string libraryName)
        {
            if (string.IsNullOrEmpty(libraryName))
                throw new ArgumentNullException(nameof(libraryName));

            IntPtr handle;
            if (Application.platform == RuntimePlatform.WindowsEditor || Application.platform == RuntimePlatform.WindowsPlayer || Application.platform == RuntimePlatform.WindowsServer)
                handle = Win32.LoadLibrary(libraryName);
            else if (Application.platform == RuntimePlatform.LinuxEditor || Application.platform == RuntimePlatform.LinuxPlayer || Application.platform == RuntimePlatform.LinuxServer)
                handle = Linux.dlopen(libraryName);
            else if (Application.platform == RuntimePlatform.OSXEditor || Application.platform == RuntimePlatform.OSXPlayer || Application.platform == RuntimePlatform.OSXServer)
                handle = Mac.dlopen(libraryName);
            else if (Application.platform == RuntimePlatform.Android || Application.platform == RuntimePlatform.IPhonePlayer || Application.platform == RuntimePlatform.VisionOS)
                handle = Mobile.dlopen(libraryName);
            else
                throw new PlatformNotSupportedException($"Current platform is unknown, unable to load library '{libraryName}'.");

            return handle;
        }

        /// <summary>
        /// Retrieve a function delegate for the library in a cross-platform manner
        /// </summary>
        /// <param name="library">library handle</param>
        /// <param name="symbolName">function name</param>
        /// <returns>function handle</returns>
        public static IntPtr GetSymbol(IntPtr library, string symbolName)
        {
            if (string.IsNullOrEmpty(symbolName))
                throw new ArgumentNullException(nameof(symbolName));

            IntPtr handle;
            if (Application.platform == RuntimePlatform.WindowsEditor || Application.platform == RuntimePlatform.WindowsPlayer || Application.platform == RuntimePlatform.WindowsServer)
                handle = Win32.GetProcAddress(library, symbolName);
            else if (Application.platform == RuntimePlatform.LinuxEditor || Application.platform == RuntimePlatform.LinuxPlayer || Application.platform == RuntimePlatform.LinuxServer)
                handle = Linux.dlsym(library, symbolName);
            else if (Application.platform == RuntimePlatform.OSXEditor || Application.platform == RuntimePlatform.OSXPlayer || Application.platform == RuntimePlatform.OSXServer)
                handle = Mac.dlsym(library, symbolName);
            else if (Application.platform == RuntimePlatform.Android || Application.platform == RuntimePlatform.IPhonePlayer || Application.platform == RuntimePlatform.VisionOS)
                handle = Mobile.dlsym(library, symbolName);
            else
                throw new PlatformNotSupportedException($"Current platform is unknown, unable to load symbol '{symbolName}' from library {library}.");

            return handle;
        }

        /// <summary>
        /// Frees up the library
        /// </summary>
        /// <param name="library">library handle</param>
        public static void FreeLibrary(IntPtr library)
        {
            if (library == IntPtr.Zero)
                return;

            if (Application.platform == RuntimePlatform.WindowsEditor || Application.platform == RuntimePlatform.WindowsPlayer || Application.platform == RuntimePlatform.WindowsServer)
                Win32.FreeLibrary(library);
            else if (Application.platform == RuntimePlatform.LinuxEditor || Application.platform == RuntimePlatform.LinuxPlayer || Application.platform == RuntimePlatform.LinuxServer)
                Linux.dlclose(library);
            else if (Application.platform == RuntimePlatform.OSXEditor || Application.platform == RuntimePlatform.OSXPlayer || Application.platform == RuntimePlatform.OSXServer)
                Mac.dlclose(library);
            else if (Application.platform == RuntimePlatform.Android || Application.platform == RuntimePlatform.IPhonePlayer || Application.platform == RuntimePlatform.VisionOS)
                Mobile.dlclose(library);
            else
                throw new PlatformNotSupportedException($"Current platform is unknown, unable to close library '{library}'.");
        }

        private static class Mac
        {
            private const string SystemLibrary = "/usr/lib/libSystem.dylib";

            private const int RTLD_LAZY = 1;
            private const int RTLD_NOW = 2;

            public static IntPtr dlopen(string path, bool lazy = true) =>
                dlopen(path, lazy ? RTLD_LAZY : RTLD_NOW);

            [DllImport(SystemLibrary)]
            public static extern IntPtr dlopen(string path, int mode);

            [DllImport(SystemLibrary)]
            public static extern IntPtr dlsym(IntPtr handle, string symbol);

            [DllImport(SystemLibrary)]
            public static extern void dlclose(IntPtr handle);
        }

        private static class Linux
        {
            private const string SystemLibrary = "libdl.so";
            private const string SystemLibrary2 = "libdl.so.2"; // newer Linux distros use this

            private const int RTLD_LAZY = 1;
            private const int RTLD_NOW = 2;

            private static bool UseSystemLibrary2 = true;

            public static IntPtr dlopen(string path, bool lazy = true)
            {
                try
                {
                    return dlopen2(path, lazy ? RTLD_LAZY : RTLD_NOW);
                }
                catch (DllNotFoundException)
                {
                    UseSystemLibrary2 = false;
                    return dlopen1(path, lazy ? RTLD_LAZY : RTLD_NOW);
                }
            }

            public static IntPtr dlsym(IntPtr handle, string symbol)
            {
                return UseSystemLibrary2 ? dlsym2(handle, symbol) : dlsym1(handle, symbol);
            }

            public static void dlclose(IntPtr handle)
            {
                if (UseSystemLibrary2)
                    dlclose2(handle);
                else
                    dlclose1(handle);
            }

            [DllImport(SystemLibrary, EntryPoint = "dlopen")]
            private static extern IntPtr dlopen1(string path, int mode);

            [DllImport(SystemLibrary, EntryPoint = "dlsym")]
            private static extern IntPtr dlsym1(IntPtr handle, string symbol);

            [DllImport(SystemLibrary, EntryPoint = "dlclose")]
            private static extern void dlclose1(IntPtr handle);

            [DllImport(SystemLibrary2, EntryPoint = "dlopen")]
            private static extern IntPtr dlopen2(string path, int mode);

            [DllImport(SystemLibrary2, EntryPoint = "dlsym")]
            private static extern IntPtr dlsym2(IntPtr handle, string symbol);

            [DllImport(SystemLibrary2, EntryPoint = "dlclose")]
            private static extern void dlclose2(IntPtr handle);
        }

        private static class Win32
        {
            private const string SystemLibrary = "Kernel32.dll";

            [DllImport(SystemLibrary, SetLastError = true, CharSet = CharSet.Ansi)]
            public static extern IntPtr LoadLibrary(string lpFileName);

            [DllImport(SystemLibrary, SetLastError = true, CharSet = CharSet.Ansi)]
            public static extern IntPtr GetProcAddress(IntPtr hModule, string lpProcName);

            [DllImport(SystemLibrary, SetLastError = true, CharSet = CharSet.Ansi)]
            public static extern void FreeLibrary(IntPtr hModule);
        }

        private static class Mobile
        {
            public static IntPtr dlopen(string path) => dlopen(path, 1);

#if UNITY_ANDROID || UNITY_IOS || UNITY_VISIONOS
            [DllImport("__Internal")]
            public static extern IntPtr dlopen(string filename, int flags);

            [DllImport("__Internal")]
            public static extern IntPtr dlsym(IntPtr handle, string symbol);

            [DllImport("__Internal")]
            public static extern int dlclose(IntPtr handle);
#else
            public static IntPtr dlopen(string filename, int flags)
            {
                return default;
            }

            public static IntPtr dlsym(IntPtr handle, string symbol)
            {
                return default;
            }

            public static int dlclose(IntPtr handle)
            {
                return default;
            }

#endif
        }
    }

    /// @ingroup utils
    /// <summary>
    /// Class implementing the LLM library handling
    /// </summary>
    public class LLMLib
    {
        public string architecture { get; private set; }
        IntPtr libraryHandle = IntPtr.Zero;
        static bool has_avx = false;
        static bool has_avx2 = false;
        static bool has_avx512 = false;
        List<IntPtr> dependencyHandles = new List<IntPtr>();

#if (UNITY_ANDROID || UNITY_IOS || UNITY_VISIONOS) && !UNITY_EDITOR

        public LLMLib(string arch)
        {
            architecture = arch;
        }

#if UNITY_ANDROID
        public const string LibraryName = "libundreamai_android";
#else
        public const string LibraryName = "__Internal";
#endif

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "Logging")]
        public static extern void LoggingStatic(IntPtr stringWrapper);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "StopLogging")]
        public static extern void StopLoggingStatic();
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_Construct")]
        public static extern IntPtr LLM_ConstructStatic(string command);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_Delete")]
        public static extern void LLM_DeleteStatic(IntPtr LLMObject);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_StartServer")]
        public static extern void LLM_StartServerStatic(IntPtr LLMObject);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_StopServer")]
        public static extern void LLM_StopServerStatic(IntPtr LLMObject);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_Start")]
        public static extern void LLM_StartStatic(IntPtr LLMObject);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_Started")]
        public static extern bool LLM_StartedStatic(IntPtr LLMObject);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_Stop")]
        public static extern void LLM_StopStatic(IntPtr LLMObject);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_SetTemplate")]
        public static extern void LLM_SetTemplateStatic(IntPtr LLMObject, string chatTemplate);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_SetSSL")]
        public static extern void LLM_SetSSLStatic(IntPtr LLMObject, string SSLCert, string SSLKey);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_Tokenize")]
        public static extern void LLM_TokenizeStatic(IntPtr LLMObject, string jsonData, IntPtr stringWrapper);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_Detokenize")]
        public static extern void LLM_DetokenizeStatic(IntPtr LLMObject, string jsonData, IntPtr stringWrapper);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_Embeddings")]
        public static extern void LLM_EmbeddingsStatic(IntPtr LLMObject, string jsonData, IntPtr stringWrapper);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_Lora_Weight")]
        public static extern void LLM_LoraWeightStatic(IntPtr LLMObject, string jsonData, IntPtr stringWrapper);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_Lora_List")]
        public static extern void LLM_LoraListStatic(IntPtr LLMObject, IntPtr stringWrapper);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_Completion")]
        public static extern void LLM_CompletionStatic(IntPtr LLMObject, string jsonData, IntPtr stringWrapper);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_Slot")]
        public static extern void LLM_SlotStatic(IntPtr LLMObject, string jsonData, IntPtr stringWrapper);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_Cancel")]
        public static extern void LLM_CancelStatic(IntPtr LLMObject, int idSlot);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "LLM_Status")]
        public static extern int LLM_StatusStatic(IntPtr LLMObject, IntPtr stringWrapper);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "StringWrapper_Construct")]
        public static extern IntPtr StringWrapper_ConstructStatic();
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "StringWrapper_Delete")]
        public static extern void StringWrapper_DeleteStatic(IntPtr instance);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "StringWrapper_GetStringSize")]
        public static extern int StringWrapper_GetStringSizeStatic(IntPtr instance);
        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl, EntryPoint = "StringWrapper_GetString")]
        public static extern void StringWrapper_GetStringStatic(IntPtr instance, IntPtr buffer, int bufferSize, bool clear = false);

        public void Logging(IntPtr stringWrapper) { LoggingStatic(stringWrapper); }
        public void StopLogging() { StopLoggingStatic(); }
        public IntPtr LLM_Construct(string command) { return LLM_ConstructStatic(command); }
        public void LLM_Delete(IntPtr LLMObject) { LLM_DeleteStatic(LLMObject); }
        public void LLM_StartServer(IntPtr LLMObject) { LLM_StartServerStatic(LLMObject); }
        public void LLM_StopServer(IntPtr LLMObject) { LLM_StopServerStatic(LLMObject); }
        public void LLM_Start(IntPtr LLMObject) { LLM_StartStatic(LLMObject); }
        public bool LLM_Started(IntPtr LLMObject) { return LLM_StartedStatic(LLMObject); }
        public void LLM_Stop(IntPtr LLMObject) { LLM_StopStatic(LLMObject); }
        public void LLM_SetTemplate(IntPtr LLMObject, string chatTemplate) { LLM_SetTemplateStatic(LLMObject, chatTemplate); }
        public void LLM_SetSSL(IntPtr LLMObject, string SSLCert, string SSLKey) { LLM_SetSSLStatic(LLMObject, SSLCert, SSLKey); }
        public void LLM_Tokenize(IntPtr LLMObject, string jsonData, IntPtr stringWrapper) { LLM_TokenizeStatic(LLMObject, jsonData, stringWrapper); }
        public void LLM_Detokenize(IntPtr LLMObject, string jsonData, IntPtr stringWrapper) { LLM_DetokenizeStatic(LLMObject, jsonData, stringWrapper); }
        public void LLM_Embeddings(IntPtr LLMObject, string jsonData, IntPtr stringWrapper) { LLM_EmbeddingsStatic(LLMObject, jsonData, stringWrapper); }
        public void LLM_LoraWeight(IntPtr LLMObject, string jsonData, IntPtr stringWrapper) { LLM_LoraWeightStatic(LLMObject, jsonData, stringWrapper); }
        public void LLM_LoraList(IntPtr LLMObject, IntPtr stringWrapper) { LLM_LoraListStatic(LLMObject, stringWrapper); }
        public void LLM_Completion(IntPtr LLMObject, string jsonData, IntPtr stringWrapper) { LLM_CompletionStatic(LLMObject, jsonData, stringWrapper); }
        public void LLM_Slot(IntPtr LLMObject, string jsonData, IntPtr stringWrapper) { LLM_SlotStatic(LLMObject, jsonData, stringWrapper); }
        public void LLM_Cancel(IntPtr LLMObject, int idSlot) { LLM_CancelStatic(LLMObject, idSlot); }
        public int LLM_Status(IntPtr LLMObject, IntPtr stringWrapper) { return LLM_StatusStatic(LLMObject, stringWrapper); }
        public IntPtr StringWrapper_Construct() { return StringWrapper_ConstructStatic(); }
        public void StringWrapper_Delete(IntPtr instance) { StringWrapper_DeleteStatic(instance); }
        public int StringWrapper_GetStringSize(IntPtr instance) { return StringWrapper_GetStringSizeStatic(instance); }
        public void StringWrapper_GetString(IntPtr instance, IntPtr buffer, int bufferSize, bool clear = false) { StringWrapper_GetStringStatic(instance, buffer, bufferSize, clear); }

#else

        static bool has_avx_set = false;
        static readonly object staticLock = new object();

        static LLMLib()
        {
            lock (staticLock)
            {
                if (has_avx_set) return;
                string archCheckerPath = GetArchitectureCheckerPath();
                if (archCheckerPath != null)
                {
                    IntPtr archCheckerHandle = LibraryLoader.LoadLibrary(archCheckerPath);
                    if (archCheckerHandle == IntPtr.Zero)
                    {
                        LLMUnitySetup.LogError($"Failed to load library {archCheckerPath}.");
                    }
                    else
                    {
                        try
                        {
                            has_avx = LibraryLoader.GetSymbolDelegate<HasArchDelegate>(archCheckerHandle, "has_avx")();
                            has_avx2 = LibraryLoader.GetSymbolDelegate<HasArchDelegate>(archCheckerHandle, "has_avx2")();
                            has_avx512 = LibraryLoader.GetSymbolDelegate<HasArchDelegate>(archCheckerHandle, "has_avx512")();
                            LibraryLoader.FreeLibrary(archCheckerHandle);
                        }
                        catch (Exception e)
                        {
                            LLMUnitySetup.LogError($"{e.GetType()}: {e.Message}");
                        }
                    }
                }
                has_avx_set = true;
            }
        }

        /// <summary>
        /// Loads the library and function handles for the defined architecture
        /// </summary>
        /// <param name="arch">archtecture</param>
        /// <exception cref="Exception"></exception>
        public LLMLib(string arch)
        {
            architecture = arch;
            foreach (string dependency in GetArchitectureDependencies(arch))
            {
                LLMUnitySetup.Log($"Loading {dependency}");
                dependencyHandles.Add(LibraryLoader.LoadLibrary(dependency));
            }

            libraryHandle = LibraryLoader.LoadLibrary(GetArchitecturePath(arch));
            if (libraryHandle == IntPtr.Zero)
            {
                throw new Exception($"Failed to load library {arch}.");
            }

            LLM_Construct = LibraryLoader.GetSymbolDelegate<LLM_ConstructDelegate>(libraryHandle, "LLM_Construct");
            LLM_Delete = LibraryLoader.GetSymbolDelegate<LLM_DeleteDelegate>(libraryHandle, "LLM_Delete");
            LLM_StartServer = LibraryLoader.GetSymbolDelegate<LLM_StartServerDelegate>(libraryHandle, "LLM_StartServer");
            LLM_StopServer = LibraryLoader.GetSymbolDelegate<LLM_StopServerDelegate>(libraryHandle, "LLM_StopServer");
            LLM_Start = LibraryLoader.GetSymbolDelegate<LLM_StartDelegate>(libraryHandle, "LLM_Start");
            LLM_Started = LibraryLoader.GetSymbolDelegate<LLM_StartedDelegate>(libraryHandle, "LLM_Started");
            LLM_Stop = LibraryLoader.GetSymbolDelegate<LLM_StopDelegate>(libraryHandle, "LLM_Stop");
            LLM_SetTemplate = LibraryLoader.GetSymbolDelegate<LLM_SetTemplateDelegate>(libraryHandle, "LLM_SetTemplate");
            LLM_SetSSL = LibraryLoader.GetSymbolDelegate<LLM_SetSSLDelegate>(libraryHandle, "LLM_SetSSL");
            LLM_Tokenize = LibraryLoader.GetSymbolDelegate<LLM_TokenizeDelegate>(libraryHandle, "LLM_Tokenize");
            LLM_Detokenize = LibraryLoader.GetSymbolDelegate<LLM_DetokenizeDelegate>(libraryHandle, "LLM_Detokenize");
            LLM_Embeddings = LibraryLoader.GetSymbolDelegate<LLM_EmbeddingsDelegate>(libraryHandle, "LLM_Embeddings");
            LLM_LoraWeight = LibraryLoader.GetSymbolDelegate<LLM_LoraWeightDelegate>(libraryHandle, "LLM_Lora_Weight");
            LLM_LoraList = LibraryLoader.GetSymbolDelegate<LLM_LoraListDelegate>(libraryHandle, "LLM_Lora_List");
            LLM_Completion = LibraryLoader.GetSymbolDelegate<LLM_CompletionDelegate>(libraryHandle, "LLM_Completion");
            LLM_Slot = LibraryLoader.GetSymbolDelegate<LLM_SlotDelegate>(libraryHandle, "LLM_Slot");
            LLM_Cancel = LibraryLoader.GetSymbolDelegate<LLM_CancelDelegate>(libraryHandle, "LLM_Cancel");
            LLM_Status = LibraryLoader.GetSymbolDelegate<LLM_StatusDelegate>(libraryHandle, "LLM_Status");
            StringWrapper_Construct = LibraryLoader.GetSymbolDelegate<StringWrapper_ConstructDelegate>(libraryHandle, "StringWrapper_Construct");
            StringWrapper_Delete = LibraryLoader.GetSymbolDelegate<StringWrapper_DeleteDelegate>(libraryHandle, "StringWrapper_Delete");
            StringWrapper_GetStringSize = LibraryLoader.GetSymbolDelegate<StringWrapper_GetStringSizeDelegate>(libraryHandle, "StringWrapper_GetStringSize");
            StringWrapper_GetString = LibraryLoader.GetSymbolDelegate<StringWrapper_GetStringDelegate>(libraryHandle, "StringWrapper_GetString");
            Logging = LibraryLoader.GetSymbolDelegate<LoggingDelegate>(libraryHandle, "Logging");
            StopLogging = LibraryLoader.GetSymbolDelegate<StopLoggingDelegate>(libraryHandle, "StopLogging");
        }

        /// <summary>
        /// Gets the path of a library that allows to detect the underlying CPU (Windows / Linux).
        /// </summary>
        /// <returns>architecture checker library path</returns>
        public static string GetArchitectureCheckerPath()
        {
            string filename;
            if (Application.platform == RuntimePlatform.WindowsEditor || Application.platform == RuntimePlatform.WindowsPlayer || Application.platform == RuntimePlatform.WindowsServer)
            {
                filename = $"windows-archchecker/archchecker.dll";
            }
            else if (Application.platform == RuntimePlatform.LinuxEditor || Application.platform == RuntimePlatform.LinuxPlayer || Application.platform == RuntimePlatform.LinuxServer)
            {
                filename = $"linux-archchecker/libarchchecker.so";
            }
            else
            {
                return null;
            }
            return Path.Combine(LLMUnitySetup.libraryPath, filename);
        }

        /// <summary>
        /// Gets additional dependencies for the specified architecture.
        /// </summary>
        /// <param name="arch">architecture</param>
        /// <returns>paths of dependency dlls</returns>
        public static List<string> GetArchitectureDependencies(string arch)
        {
            List<string> dependencies = new List<string>();
            if (arch == "cuda-cu12.2.0-full")
            {
                if (Application.platform == RuntimePlatform.WindowsEditor || Application.platform == RuntimePlatform.WindowsPlayer || Application.platform == RuntimePlatform.WindowsServer)
                {
                    dependencies.Add(Path.Combine(LLMUnitySetup.libraryPath, $"windows-{arch}/cudart64_12.dll"));
                    dependencies.Add(Path.Combine(LLMUnitySetup.libraryPath, $"windows-{arch}/cublasLt64_12.dll"));
                    dependencies.Add(Path.Combine(LLMUnitySetup.libraryPath, $"windows-{arch}/cublas64_12.dll"));
                }
            }
            else if (arch == "vulkan")
            {
                if (Application.platform == RuntimePlatform.WindowsEditor || Application.platform == RuntimePlatform.WindowsPlayer || Application.platform == RuntimePlatform.WindowsServer)
                {
                    dependencies.Add(Path.Combine(LLMUnitySetup.libraryPath, $"windows-{arch}/vulkan-1.dll"));
                }
                else if (Application.platform == RuntimePlatform.LinuxEditor || Application.platform == RuntimePlatform.LinuxPlayer || Application.platform == RuntimePlatform.LinuxServer)
                {
                    dependencies.Add(Path.Combine(LLMUnitySetup.libraryPath, $"linux-{arch}/libvulkan.so.1"));
                }
            }
            return dependencies;
        }

        /// <summary>
        /// Gets the path of the llama.cpp library for the specified architecture.
        /// </summary>
        /// <param name="arch">architecture</param>
        /// <returns>llama.cpp library path</returns>
        public static string GetArchitecturePath(string arch)
        {
            string filename;
            if (Application.platform == RuntimePlatform.WindowsEditor || Application.platform == RuntimePlatform.WindowsPlayer || Application.platform == RuntimePlatform.WindowsServer)
            {
                filename = $"windows-{arch}/undreamai_windows-{arch}.dll";
            }
            else if (Application.platform == RuntimePlatform.LinuxEditor || Application.platform == RuntimePlatform.LinuxPlayer || Application.platform == RuntimePlatform.LinuxServer)
            {
                filename = $"linux-{arch}/libundreamai_linux-{arch}.so";
            }
            else if (Application.platform == RuntimePlatform.OSXEditor || Application.platform == RuntimePlatform.OSXPlayer || Application.platform == RuntimePlatform.OSXServer)
            {
                filename = $"macos-{arch}/libundreamai_macos-{arch}.dylib";
            }
            else
            {
                string error = "Unknown OS";
                LLMUnitySetup.LogError(error);
                throw new Exception(error);
            }
            return Path.Combine(LLMUnitySetup.libraryPath, filename);
        }

        public delegate bool HasArchDelegate();
        public delegate void LoggingDelegate(IntPtr stringWrapper);
        public delegate void StopLoggingDelegate();
        public delegate IntPtr LLM_ConstructDelegate(string command);
        public delegate void LLM_DeleteDelegate(IntPtr LLMObject);
        public delegate void LLM_StartServerDelegate(IntPtr LLMObject);
        public delegate void LLM_StopServerDelegate(IntPtr LLMObject);
        public delegate void LLM_StartDelegate(IntPtr LLMObject);
        public delegate bool LLM_StartedDelegate(IntPtr LLMObject);
        public delegate void LLM_StopDelegate(IntPtr LLMObject);
        public delegate void LLM_SetTemplateDelegate(IntPtr LLMObject, string chatTemplate);
        public delegate void LLM_SetSSLDelegate(IntPtr LLMObject, string SSLCert, string SSLKey);
        public delegate void LLM_TokenizeDelegate(IntPtr LLMObject, string jsonData, IntPtr stringWrapper);
        public delegate void LLM_DetokenizeDelegate(IntPtr LLMObject, string jsonData, IntPtr stringWrapper);
        public delegate void LLM_EmbeddingsDelegate(IntPtr LLMObject, string jsonData, IntPtr stringWrapper);
        public delegate void LLM_LoraWeightDelegate(IntPtr LLMObject, string jsonData, IntPtr stringWrapper);
        public delegate void LLM_LoraListDelegate(IntPtr LLMObject, IntPtr stringWrapper);
        public delegate void LLM_CompletionDelegate(IntPtr LLMObject, string jsonData, IntPtr stringWrapper);
        public delegate void LLM_SlotDelegate(IntPtr LLMObject, string jsonData, IntPtr stringWrapper);
        public delegate void LLM_CancelDelegate(IntPtr LLMObject, int idSlot);
        public delegate int LLM_StatusDelegate(IntPtr LLMObject, IntPtr stringWrapper);
        public delegate IntPtr StringWrapper_ConstructDelegate();
        public delegate void StringWrapper_DeleteDelegate(IntPtr instance);
        public delegate int StringWrapper_GetStringSizeDelegate(IntPtr instance);
        public delegate void StringWrapper_GetStringDelegate(IntPtr instance, IntPtr buffer, int bufferSize, bool clear = false);

        public LoggingDelegate Logging;
        public StopLoggingDelegate StopLogging;
        public LLM_ConstructDelegate LLM_Construct;
        public LLM_DeleteDelegate LLM_Delete;
        public LLM_StartServerDelegate LLM_StartServer;
        public LLM_StopServerDelegate LLM_StopServer;
        public LLM_StartDelegate LLM_Start;
        public LLM_StartedDelegate LLM_Started;
        public LLM_StopDelegate LLM_Stop;
        public LLM_SetTemplateDelegate LLM_SetTemplate;
        public LLM_SetSSLDelegate LLM_SetSSL;
        public LLM_TokenizeDelegate LLM_Tokenize;
        public LLM_DetokenizeDelegate LLM_Detokenize;
        public LLM_CompletionDelegate LLM_Completion;
        public LLM_EmbeddingsDelegate LLM_Embeddings;
        public LLM_LoraWeightDelegate LLM_LoraWeight;
        public LLM_LoraListDelegate LLM_LoraList;
        public LLM_SlotDelegate LLM_Slot;
        public LLM_CancelDelegate LLM_Cancel;
        public LLM_StatusDelegate LLM_Status;
        public StringWrapper_ConstructDelegate StringWrapper_Construct;
        public StringWrapper_DeleteDelegate StringWrapper_Delete;
        public StringWrapper_GetStringSizeDelegate StringWrapper_GetStringSize;
        public StringWrapper_GetStringDelegate StringWrapper_GetString;

#endif

        /// <summary>
        /// Identifies the possible architectures that we can use based on the OS and GPU usage
        /// </summary>
        /// <param name="gpu">whether to allow GPU architectures</param>
        /// <returns>possible architectures</returns>
        public static List<string> PossibleArchitectures(bool gpu = false)
        {
            List<string> architectures = new List<string>();
            if (Application.platform == RuntimePlatform.WindowsEditor || Application.platform == RuntimePlatform.WindowsPlayer || Application.platform == RuntimePlatform.WindowsServer ||
                Application.platform == RuntimePlatform.LinuxEditor || Application.platform == RuntimePlatform.LinuxPlayer || Application.platform == RuntimePlatform.LinuxServer)
            {
                if (gpu)
                {
                    if (LLMUnitySetup.FullLlamaLib)
                    {
                        architectures.Add("cuda-cu12.2.0-full");
                    }
                    else
                    {
                        architectures.Add("cuda-cu12.2.0");
                    }
                    architectures.Add("hip");
                    architectures.Add("vulkan");
                }
                if (has_avx512) architectures.Add("avx512");
                if (has_avx2) architectures.Add("avx2");
                if (has_avx) architectures.Add("avx");
                architectures.Add("noavx");
            }
            else if (Application.platform == RuntimePlatform.OSXEditor || Application.platform == RuntimePlatform.OSXPlayer)
            {
                string arch = RuntimeInformation.ProcessArchitecture.ToString().ToLower();
                if (arch.Contains("arm"))
                {
                    architectures.Add("arm64-acc");
                    architectures.Add("arm64-no_acc");
                }
                else
                {
                    if (arch != "x86" && arch != "x64") LLMUnitySetup.LogWarning($"Unknown architecture of processor {arch}! Falling back to x86_64");
                    architectures.Add("x64-acc");
                    architectures.Add("x64-no_acc");
                }
            }
            else if (Application.platform == RuntimePlatform.Android)
            {
                architectures.Add("android");
            }
            else if (Application.platform == RuntimePlatform.IPhonePlayer)
            {
                architectures.Add("ios");
            }
            else if (Application.platform == RuntimePlatform.VisionOS)
            {
                architectures.Add("visionos");
            }
            else
            {
                string error = "Unknown OS";
                LLMUnitySetup.LogError(error);
                throw new Exception(error);
            }
            return architectures;
        }

        /// <summary>
        /// Allows to retrieve a string from the library (Unity only allows marshalling of chars)
        /// </summary>
        /// <param name="stringWrapper">string wrapper pointer</param>
        /// <returns>retrieved string</returns>
        public string GetStringWrapperResult(IntPtr stringWrapper)
        {
            string result = "";
            int bufferSize = StringWrapper_GetStringSize(stringWrapper);
            if (bufferSize > 1)
            {
                IntPtr buffer = Marshal.AllocHGlobal(bufferSize);
                try
                {
                    StringWrapper_GetString(stringWrapper, buffer, bufferSize);
                    result = Marshal.PtrToStringAnsi(buffer);
                }
                finally
                {
                    Marshal.FreeHGlobal(buffer);
                }
            }
            return result;
        }

        /// <summary>
        /// Destroys the LLM library
        /// </summary>
        public void Destroy()
        {
            if (libraryHandle != IntPtr.Zero) LibraryLoader.FreeLibrary(libraryHandle);
            foreach (IntPtr dependencyHandle in dependencyHandles) LibraryLoader.FreeLibrary(dependencyHandle);
        }
    }
}
/// \endcond


# -------------------- LLMManager.cs --------------------

/// @file
/// @brief File implementing the LLM model manager
using System;
using System.Collections.Generic;
using System.IO;
using System.Threading.Tasks;
using UnityEditor;
using UnityEngine;

namespace LLMUnity
{
    [Serializable]
    /// @ingroup utils
    /// <summary>
    /// Class implementing a LLM model entry
    /// </summary>
    public class ModelEntry
    {
        public string label;
        public string filename;
        public string path;
        public bool lora;
        public string chatTemplate;
        public string url;
        public bool embeddingOnly;
        public int embeddingLength;
        public bool includeInBuild;
        public int contextLength;

        static List<string> embeddingOnlyArchs = new List<string> {"bert", "nomic-bert", "jina-bert-v2", "t5", "t5encoder"};

        /// <summary>
        /// Returns the relative asset path if it is in the AssetPath folder (StreamingAssets or persistentPath), otherwise the filename.
        /// </summary>
        /// <param name="path">asset path</param>
        /// <returns>relative asset path or filename</returns>
        public static string GetFilenameOrRelativeAssetPath(string path)
        {
            string assetPath = LLMUnitySetup.GetAssetPath(path); // Note: this will return the full path if a full path is passed
            string basePath = LLMUnitySetup.GetAssetPath();
            if (File.Exists(assetPath) && LLMUnitySetup.IsSubPath(assetPath, basePath))
            {
                return LLMUnitySetup.RelativePath(assetPath, basePath);
            }
            return Path.GetFileName(path);
        }

        /// <summary>
        /// Constructs a LLM model entry
        /// </summary>
        /// <param name="path">model path</param>
        /// <param name="lora">if it is a LORA or LLM</param>
        /// <param name="label">label to show in the model manager in the Editor</param>
        /// <param name="url">model url</param>
        public ModelEntry(string path, bool lora = false, string label = null, string url = null)
        {
            filename = GetFilenameOrRelativeAssetPath(path);
            this.label = label == null ? Path.GetFileName(filename) : label;
            this.lora = lora;
            this.path = LLMUnitySetup.GetFullPath(path);
            this.url = url;
            includeInBuild = true;
            chatTemplate = null;
            contextLength = -1;
            embeddingOnly = false;
            embeddingLength = 0;
            if (!lora)
            {
                GGUFReader reader = new GGUFReader(this.path);
                string arch = reader.GetStringField("general.architecture");
                if (arch != null)
                {
                    contextLength = reader.GetIntField($"{arch}.context_length");
                    embeddingLength = reader.GetIntField($"{arch}.embedding_length");
                }
                embeddingOnly = embeddingOnlyArchs.Contains(arch);
                chatTemplate = embeddingOnly ? default : ChatTemplate.FromGGUF(reader, this.path);
            }
        }

        /// <summary>
        /// Returns only the required fields for bundling the model in the build
        /// </summary>
        /// <returns>Adapted model entry</returns>
        public ModelEntry OnlyRequiredFields()
        {
            ModelEntry entry = (ModelEntry)MemberwiseClone();
            entry.label = null;
            entry.path = entry.filename;
            return entry;
        }
    }

    /// \cond HIDE
    [Serializable]
    public class LLMManagerStore
    {
        public bool downloadOnStart;
        public List<ModelEntry> modelEntries;
        public int debugMode;
        public bool fullLlamaLib;
    }
    /// \endcond

    [DefaultExecutionOrder(-2)]
    /// @ingroup utils
    /// <summary>
    /// Class implementing the LLM model manager
    /// </summary>
    public class LLMManager
    {
        public static bool downloadOnStart = false;
        public static List<ModelEntry> modelEntries = new List<ModelEntry>();
        static List<LLM> llms = new List<LLM>();

        public static float downloadProgress = 1;
        public static List<Callback<float>> downloadProgressCallbacks = new List<Callback<float>>();
        static Task<bool> SetupTask;
        static readonly object lockObject = new object();
        static long totalSize;
        static long currFileSize;
        static long completedSize;

        /// <summary>
        /// Sets the model download progress in all registered callbacks
        /// </summary>
        /// <param name="progress">model download progress</param>
        public static void SetDownloadProgress(float progress)
        {
            downloadProgress = (completedSize + progress * currFileSize) / totalSize;
            foreach (Callback<float> downloadProgressCallback in downloadProgressCallbacks) downloadProgressCallback?.Invoke(downloadProgress);
        }

        /// <summary>
        /// Setup of the models
        /// </summary>
        /// <returns>bool specifying if the setup was successful</returns>
        public static Task<bool> Setup()
        {
            lock (lockObject)
            {
                if (SetupTask == null) SetupTask = SetupOnce();
            }
            return SetupTask;
        }

        /// <summary>
        /// Task performing the setup of the models
        /// </summary>
        /// <returns>bool specifying if the setup was successful</returns>
        public static async Task<bool> SetupOnce()
        {
            await LLMUnitySetup.AndroidExtractAsset(LLMUnitySetup.LLMManagerPath, true);
            LoadFromDisk();

            List<StringPair> downloads = new List<StringPair>();
            foreach (ModelEntry modelEntry in modelEntries)
            {
                string target = LLMUnitySetup.GetAssetPath(modelEntry.filename);
                if (File.Exists(target)) continue;

                if (!downloadOnStart || string.IsNullOrEmpty(modelEntry.url))
                {
                    await LLMUnitySetup.AndroidExtractFile(modelEntry.filename);
                    if (!File.Exists(target)) LLMUnitySetup.LogError($"Model {modelEntry.filename} could not be found!");
                }
                else
                {
                    target = LLMUnitySetup.GetDownloadAssetPath(modelEntry.filename);
                    downloads.Add(new StringPair {source = modelEntry.url, target = target});
                }
            }
            if (downloads.Count == 0) return true;

            try
            {
                downloadProgress = 0;
                totalSize = 0;
                completedSize = 0;

                ResumingWebClient client = new ResumingWebClient();
                Dictionary<string, long> fileSizes = new Dictionary<string, long>();
                foreach (StringPair pair in downloads)
                {
                    long size = client.GetURLFileSize(pair.source);
                    fileSizes[pair.source] = size;
                    totalSize += size;
                }

                foreach (StringPair pair in downloads)
                {
                    currFileSize = fileSizes[pair.source];
                    await LLMUnitySetup.DownloadFile(pair.source, pair.target, false, null, SetDownloadProgress);
                    await LLMUnitySetup.AndroidExtractFile(Path.GetFileName(pair.target));
                    completedSize += currFileSize;
                }

                completedSize = totalSize;
                SetDownloadProgress(0);
            }
            catch (Exception ex)
            {
                LLMUnitySetup.LogError($"Error downloading the models: {ex.Message}");
                return false;
            }
            return true;
        }

        /// <summary>
        /// Sets the chat template for a model and distributes it to all LLMs using it
        /// </summary>
        /// <param name="filename">model path</param>
        /// <param name="chatTemplate">chat template</param>
        public static void SetTemplate(string filename, string chatTemplate)
        {
            SetTemplate(Get(filename), chatTemplate);
        }

        /// <summary>
        /// Sets the chat template for a model and distributes it to all LLMs using it
        /// </summary>
        /// <param name="entry">model entry</param>
        /// <param name="chatTemplate">chat template</param>
        public static void SetTemplate(ModelEntry entry, string chatTemplate)
        {
            if (entry == null) return;
            entry.chatTemplate = chatTemplate;
            foreach (LLM llm in llms)
            {
                if (llm != null && llm.model == entry.filename) llm.SetTemplate(chatTemplate);
            }
#if UNITY_EDITOR
            Save();
#endif
        }

        /// <summary>
        /// Gets the model entry for a model path
        /// </summary>
        /// <param name="path">model path</param>
        /// <returns>model entry</returns>
        public static ModelEntry Get(string path)
        {
            string filename = Path.GetFileName(path);
            string fullPath = LLMUnitySetup.GetFullPath(path);
            foreach (ModelEntry entry in modelEntries)
            {
                if (entry.filename == filename || entry.path == fullPath) return entry;
            }
            return null;
        }

        /// <summary>
        /// Gets the asset path based on whether the application runs locally in the editor or in a build
        /// </summary>
        /// <param name="filename">model filename or relative path</param>
        /// <returns>asset path</returns>
        public static string GetAssetPath(string filename)
        {
            ModelEntry entry = Get(filename);
            if (entry == null) return "";
#if UNITY_EDITOR
            return entry.path;
#else
            return LLMUnitySetup.GetAssetPath(entry.filename);
#endif
        }

        /// <summary>
        /// Returns the number of LLM/LORA models
        /// </summary>
        /// <param name="lora">whether to return number of LORA or LLM models</param>
        /// <returns>number of LLM/LORA models</returns>
        public static int Num(bool lora)
        {
            int num = 0;
            foreach (ModelEntry entry in modelEntries)
            {
                if (entry.lora == lora) num++;
            }
            return num;
        }

        /// <summary>
        /// Returns the number of LLM models
        /// </summary>
        /// <returns>number of LLM models</returns>
        public static int NumModels()
        {
            return Num(false);
        }

        /// <summary>
        /// Returns the number of LORA models
        /// </summary>
        /// <returns>number of LORA models</returns>
        public static int NumLoras()
        {
            return Num(true);
        }

        /// <summary>
        /// Registers a LLM to the model manager
        /// </summary>
        /// <param name="llm">LLM</param>
        public static void Register(LLM llm)
        {
            llms.Add(llm);
        }

        /// <summary>
        /// Removes a LLM from the model manager
        /// </summary>
        /// <param name="llm">LLM</param>
        public static void Unregister(LLM llm)
        {
            llms.Remove(llm);
        }

        /// <summary>
        /// Loads the model manager from a file
        /// </summary>
        public static void LoadFromDisk()
        {
            if (!File.Exists(LLMUnitySetup.LLMManagerPath)) return;
            LLMManagerStore store = JsonUtility.FromJson<LLMManagerStore>(File.ReadAllText(LLMUnitySetup.LLMManagerPath));
            downloadOnStart = store.downloadOnStart;
            modelEntries = store.modelEntries;
            LLMUnitySetup.DebugMode = (LLMUnitySetup.DebugModeType)store.debugMode;
            LLMUnitySetup.FullLlamaLib = store.fullLlamaLib;
        }

#if UNITY_EDITOR
        static string LLMManagerPref = "LLMManager";

        [HideInInspector] public static float modelProgress = 1;
        [HideInInspector] public static float loraProgress = 1;

        [InitializeOnLoadMethod]
        static void InitializeOnLoad()
        {
            Load();
        }

        /// <summary>
        /// Adds a model entry to the model manager
        /// </summary>
        /// <param name="entry">model entry</param>
        /// <returns>model filename</returns>
        public static string AddEntry(ModelEntry entry)
        {
            int indexToInsert = modelEntries.Count;
            if (!entry.lora)
            {
                if (modelEntries.Count > 0 && modelEntries[0].lora) indexToInsert = 0;
                else
                {
                    for (int i = modelEntries.Count - 1; i >= 0; i--)
                    {
                        if (!modelEntries[i].lora)
                        {
                            indexToInsert = i + 1;
                            break;
                        }
                    }
                }
            }
            modelEntries.Insert(indexToInsert, entry);
            Save();
            return entry.filename;
        }

        /// <summary>
        /// Creates and adds a model entry to the model manager
        /// </summary>
        /// <param name="path">model path</param>
        /// <param name="lora">if it is a LORA or LLM</param>
        /// <param name="label">label to show in the model manager in the Editor</param>
        /// <param name="url">model url</param>
        /// <returns>model filename</returns>
        public static string AddEntry(string path, bool lora = false, string label = null, string url = null)
        {
            return AddEntry(new ModelEntry(path, lora, label, url));
        }

        /// <summary>
        /// Downloads a model and adds a model entry to the model manager
        /// </summary>
        /// <param name="url">model url</param>
        /// <param name="lora">if it is a LORA or LLM</param>
        /// <param name="log">whether to log</param>
        /// <param name="label">model label</param>
        /// <returns>model filename</returns>
        public static async Task<string> Download(string url, bool lora = false, bool log = false, string label = null)
        {
            foreach (ModelEntry entry in modelEntries)
            {
                if (entry.url == url)
                {
                    if (log) LLMUnitySetup.Log($"Found existing entry for {url}");
                    return entry.filename;
                }
            }

            string modelName = Path.GetFileName(url).Split("?")[0];
            ModelEntry entryPath = Get(modelName);
            if (entryPath != null)
            {
                if (log) LLMUnitySetup.Log($"Found existing entry for {modelName}");
                return entryPath.filename;
            }

            string modelPath = Path.Combine(LLMUnitySetup.modelDownloadPath, modelName);
            float preModelProgress = modelProgress;
            float preLoraProgress = loraProgress;
            try
            {
                if (!lora)
                {
                    modelProgress = 0;
                    await LLMUnitySetup.DownloadFile(url, modelPath, false, null, SetModelProgress);
                }
                else
                {
                    loraProgress = 0;
                    await LLMUnitySetup.DownloadFile(url, modelPath, false, null, SetLoraProgress);
                }
            }
            catch (Exception ex)
            {
                modelProgress = preModelProgress;
                loraProgress = preLoraProgress;
                LLMUnitySetup.LogError($"Error downloading the model from URL '{url}': " + ex.Message);
                return null;
            }
            return AddEntry(modelPath, lora, label, url);
        }

        /// <summary>
        /// Loads a model from disk and adds a model entry to the model manager
        /// </summary>
        /// <param name="path">model path</param>
        /// <param name="lora">if it is a LORA or LLM</param>
        /// <param name="log">whether to log</param>
        /// <param name="label">model label</param>
        /// <returns>model filename</returns>
        public static string Load(string path, bool lora = false, bool log = false, string label = null)
        {
            ModelEntry entry = Get(path);
            if (entry != null)
            {
                if (log) LLMUnitySetup.Log($"Found existing entry for {entry.filename}");
                return entry.filename;
            }
            return AddEntry(path, lora, label);
        }

        /// <summary>
        /// Downloads a LLM model from disk and adds a model entry to the model manager
        /// </summary>
        /// <param name="url">model url</param>
        /// <param name="log">whether to log</param>
        /// <param name="label">model label</param>
        /// <returns>model filename</returns>
        public static async Task<string> DownloadModel(string url, bool log = false, string label = null)
        {
            return await Download(url, false, log, label);
        }

        /// <summary>
        /// Downloads a Lora model from disk and adds a model entry to the model manager
        /// </summary>
        /// <param name="url">model url</param>
        /// <param name="log">whether to log</param>
        /// <param name="label">model label</param>
        /// <returns>model filename</returns>
        public static async Task<string> DownloadLora(string url, bool log = false, string label = null)
        {
            return await Download(url, true, log, label);
        }

        /// <summary>
        /// Loads a LLM model from disk and adds a model entry to the model manager
        /// </summary>
        /// <param name="path">model path</param>
        /// <param name="log">whether to log</param>
        /// <param name="label">model label</param>
        /// <returns>model filename</returns>
        public static string LoadModel(string path, bool log = false, string label = null)
        {
            return Load(path, false, log, label);
        }

        /// <summary>
        /// Loads a LORA model from disk and adds a model entry to the model manager
        /// </summary>
        /// <param name="path">model path</param>
        /// <param name="log">whether to log</param>
        /// <param name="label">model label</param>
        /// <returns>model filename</returns>
        public static string LoadLora(string path, bool log = false, string label = null)
        {
            return Load(path, true, log, label);
        }

        /// <summary>
        /// Sets the URL for a model
        /// </summary>
        /// <param name="filename">model filename</param>
        /// <param name="url">model URL</param>
        public static void SetURL(string filename, string url)
        {
            SetURL(Get(filename), url);
        }

        /// <summary>
        /// Sets the URL for a model
        /// </summary>
        /// <param name="entry">model entry</param>
        /// <param name="url">model URL</param>
        public static void SetURL(ModelEntry entry, string url)
        {
            if (entry == null) return;
            entry.url = url;
            Save();
        }

        /// <summary>
        /// Sets whether to include a model to the build
        /// </summary>
        /// <param name="filename">model filename</param>
        /// <param name="includeInBuild">whether to include it</param>
        public static void SetIncludeInBuild(string filename, bool includeInBuild)
        {
            SetIncludeInBuild(Get(filename), includeInBuild);
        }

        /// <summary>
        /// Sets whether to include a model to the build
        /// </summary>
        /// <param name="entry">model entry</param>
        /// <param name="includeInBuild">whether to include it</param>
        public static void SetIncludeInBuild(ModelEntry entry, bool includeInBuild)
        {
            if (entry == null) return;
            entry.includeInBuild = includeInBuild;
            Save();
        }

        /// <summary>
        /// Sets whether to download files on start
        /// </summary>
        /// <param name="value">whether to download files</param>
        public static void SetDownloadOnStart(bool value)
        {
            downloadOnStart = value;
            if (downloadOnStart)
            {
                bool warn = false;
                foreach (ModelEntry entry in modelEntries)
                {
                    if (entry.url == null || entry.url == "") warn = true;
                }
                if (warn) LLMUnitySetup.LogWarning("Some models do not have a URL and will be copied in the build. To resolve this fill in the URL field in the expanded view of the LLM Model list.");
            }
            Save();
        }

        /// <summary>
        /// Removes a model from the model manager
        /// </summary>
        /// <param name="filename">model filename</param>
        public static void Remove(string filename)
        {
            Remove(Get(filename));
        }

        /// <summary>
        /// Removes a model from the model manager
        /// </summary>
        /// <param name="filename">model entry</param>
        public static void Remove(ModelEntry entry)
        {
            if (entry == null) return;
            modelEntries.Remove(entry);
            Save();
            foreach (LLM llm in llms)
            {
                if (!entry.lora && llm.model == entry.filename) llm.model = "";
                else if (entry.lora) llm.RemoveLora(entry.filename);
            }
        }

        /// <summary>
        /// Sets the LLM download progress
        /// </summary>
        /// <param name="progress">download progress</param>
        public static void SetModelProgress(float progress)
        {
            modelProgress = progress;
        }

        /// <summary>
        /// Sets the LORA download progress
        /// </summary>
        /// <param name="progress">download progress</param>
        public static void SetLoraProgress(float progress)
        {
            loraProgress = progress;
        }

        /// <summary>
        /// Serialises and saves the model manager
        /// </summary>
        public static void Save()
        {
            string json = JsonUtility.ToJson(new LLMManagerStore
            {
                modelEntries = modelEntries,
                downloadOnStart = downloadOnStart,
            }, true);
            PlayerPrefs.SetString(LLMManagerPref, json);
            PlayerPrefs.Save();
        }

        /// <summary>
        /// Deserialises and loads the model manager
        /// </summary>
        public static void Load()
        {
            string pref = PlayerPrefs.GetString(LLMManagerPref);
            if (pref == null || pref == "") return;
            LLMManagerStore store = JsonUtility.FromJson<LLMManagerStore>(pref);
            downloadOnStart = store.downloadOnStart;
            modelEntries = store.modelEntries;
        }

        /// <summary>
        /// Saves the model manager to disk for the build
        /// </summary>
        public static void SaveToDisk()
        {
            List<ModelEntry> modelEntriesBuild = new List<ModelEntry>();
            foreach (ModelEntry modelEntry in modelEntries)
            {
                if (!modelEntry.includeInBuild) continue;
                modelEntriesBuild.Add(modelEntry.OnlyRequiredFields());
            }
            string json = JsonUtility.ToJson(new LLMManagerStore
            {
                modelEntries = modelEntriesBuild,
                downloadOnStart = downloadOnStart,
                debugMode = (int)LLMUnitySetup.DebugMode,
                fullLlamaLib = LLMUnitySetup.FullLlamaLib
            }, true);
            File.WriteAllText(LLMUnitySetup.LLMManagerPath, json);
        }

        /// <summary>
        /// Saves the model manager to disk along with models that are not (or can't) be downloaded for the build
        /// </summary>
        /// <param name="copyCallback">copy function</param>
        public static void Build(ActionCallback copyCallback)
        {
            SaveToDisk();

            foreach (ModelEntry modelEntry in modelEntries)
            {
                string target = LLMUnitySetup.GetAssetPath(modelEntry.filename);
                if (!modelEntry.includeInBuild || File.Exists(target)) continue;
                if (!downloadOnStart || string.IsNullOrEmpty(modelEntry.url)) copyCallback(modelEntry.path, target);
            }
        }

#endif
    }
}


# -------------------- LLMUnitySetup.cs --------------------

/// @file
/// @brief File implementing helper functions for setup and process management.
using UnityEditor;
using System.IO;
using UnityEngine;
using System.Threading.Tasks;
using System;
using System.IO.Compression;
using System.Collections.Generic;
using UnityEngine.Networking;
using System.Text.RegularExpressions;

/// @defgroup llm LLM
/// @defgroup template Chat Templates
/// @defgroup utils Utils
namespace LLMUnity
{
    /// \cond HIDE
    public sealed class FloatAttribute : PropertyAttribute
    {
        public float Min { get; private set; }
        public float Max { get; private set; }

        public FloatAttribute(float min, float max)
        {
            Min = min;
            Max = max;
        }
    }
    public sealed class IntAttribute : PropertyAttribute
    {
        public int Min { get; private set; }
        public int Max { get; private set; }

        public IntAttribute(int min, int max)
        {
            Min = min;
            Max = max;
        }
    }

    [AttributeUsage(AttributeTargets.Field, Inherited = true, AllowMultiple = false)]
    public class DynamicRangeAttribute : PropertyAttribute
    {
        public readonly string minVariable;
        public readonly string maxVariable;
        public bool intOrFloat;

        public DynamicRangeAttribute(string minVariable, string maxVariable, bool intOrFloat)
        {
            this.minVariable = minVariable;
            this.maxVariable = maxVariable;
            this.intOrFloat = intOrFloat;
        }
    }

    public class LLMAttribute : PropertyAttribute {}
    public class LocalRemoteAttribute : PropertyAttribute {}
    public class RemoteAttribute : PropertyAttribute {}
    public class LocalAttribute : PropertyAttribute {}
    public class ModelAttribute : PropertyAttribute {}
    public class ModelExtrasAttribute : PropertyAttribute {}
    public class ChatAttribute : PropertyAttribute {}
    public class LLMUnityAttribute : PropertyAttribute {}

    public class AdvancedAttribute : PropertyAttribute {}
    public class LLMAdvancedAttribute : AdvancedAttribute {}
    public class ModelAdvancedAttribute : AdvancedAttribute {}
    public class ChatAdvancedAttribute : AdvancedAttribute {}

    public class NotImplementedException : Exception
    {
        public NotImplementedException() : base("The method needs to be implemented by subclasses.") {}
    }

    public delegate void EmptyCallback();
    public delegate void Callback<T>(T message);
    public delegate Task TaskCallback<T>(T message);
    public delegate T2 ContentCallback<T, T2>(T message);
    public delegate void ActionCallback(string source, string target);

    [Serializable]
    public struct StringPair
    {
        public string source;
        public string target;
    }

    [Serializable]
    public class ListStringPair
    {
        public List<StringPair> pairs;
    }
    /// \endcond

    /// @ingroup utils
    /// <summary>
    /// Class implementing helper functions for setup and process management.
    /// </summary>
    public class LLMUnitySetup
    {
        // DON'T CHANGE! the version is autocompleted with a GitHub action
        /// <summary> LLM for Unity version </summary>
        public static string Version = "v2.5.0";
        /// <summary> LlamaLib version </summary>
        public static string LlamaLibVersion = "v1.2.4";
        /// <summary> LlamaLib release url </summary>
        public static string LlamaLibReleaseURL = $"https://github.com/undreamai/LlamaLib/releases/download/{LlamaLibVersion}";
        /// <summary> LlamaLib name </summary>
        public static string libraryName = GetLibraryName(LlamaLibVersion);
        /// <summary> LlamaLib path </summary>
        public static string libraryPath = GetAssetPath(libraryName);
        /// <summary> LlamaLib url </summary>
        public static string LlamaLibURL = $"{LlamaLibReleaseURL}/{libraryName}.zip";
        /// <summary> LlamaLib extension url </summary>
        public static string LlamaLibExtensionURL = $"{LlamaLibReleaseURL}/{libraryName}-full.zip";
        /// <summary> LLMnity store path </summary>
        public static string LLMUnityStore = Path.Combine(Environment.GetFolderPath(Environment.SpecialFolder.ApplicationData), "LLMUnity");
        /// <summary> Model download path </summary>
        public static string modelDownloadPath = Path.Combine(LLMUnityStore, "models");
        /// <summary> Path of file with build information for runtime </summary>
        public static string LLMManagerPath = GetAssetPath("LLMManager.json");

        /// <summary> Default models for download </summary>
        [HideInInspector] public static readonly Dictionary<string, (string, string, string)[]> modelOptions = new Dictionary<string, (string, string, string)[]>()
        {
            {"Large models (more than 10B)", new(string, string, string)[]
             {
                 ("Gemma 3 12B", "https://huggingface.co/lmstudio-community/gemma-3-12b-it-GGUF/resolve/main/gemma-3-12b-it-Q4_K_M.gguf", "https://ai.google.dev/gemma/terms"),
                 ("Phi 4 14B", "https://huggingface.co/bartowski/phi-4-GGUF/resolve/main/phi-4-Q4_K_M.gguf", null),
                 ("Qwen 2.5 14B", "https://huggingface.co/lmstudio-community/Qwen2.5-14B-Instruct-GGUF/resolve/main/Qwen2.5-14B-Instruct-Q4_K_M.gguf", null),
                 ("DeepSeek R1 Distill Qwen 14B", "https://huggingface.co/lmstudio-community/DeepSeek-R1-Distill-Qwen-14B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-14B-Q4_K_M.gguf", null),
             }},
            {"Medium models (up to 10B)", new(string, string, string)[]
             {
                 ("Llama 3.1 8B", "https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf", "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/LICENSE"),
                 ("Qwen 2.5 7B", "https://huggingface.co/lmstudio-community/Qwen2.5-7B-Instruct-GGUF/resolve/main/Qwen2.5-7B-Instruct-Q4_K_M.gguf", null),
                 ("DeepSeek R1 Distill Llama 8B", "https://huggingface.co/lmstudio-community/DeepSeek-R1-Distill-Llama-8B-GGUF/resolve/main/DeepSeek-R1-Distill-Llama-8B-Q4_K_M.gguf", null),
                 ("DeepSeek R1 Distill Qwen 7B", "https://huggingface.co/lmstudio-community/DeepSeek-R1-Distill-Qwen-7B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.gguf", null),
                 ("Gemma 2 9B it", "https://huggingface.co/bartowski/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q4_K_M.gguf", "https://ai.google.dev/gemma/terms"),
                 ("Mistral 7B Instruct v0.2", "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf", null),
                 ("OpenHermes 2.5 7B", "https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/resolve/main/openhermes-2.5-mistral-7b.Q4_K_M.gguf", null),
             }},
            {"Small models (up to 5B)", new(string, string, string)[]
             {
                 ("Llama 3.2 3B", "https://huggingface.co/hugging-quants/Llama-3.2-3B-Instruct-Q4_K_M-GGUF/resolve/main/llama-3.2-3b-instruct-q4_k_m.gguf", "https://huggingface.co/meta-llama/Llama-3.2-1B/blob/main/LICENSE.txt"),
                 ("Gemma 3 4B", "https://huggingface.co/lmstudio-community/gemma-3-4b-it-GGUF/resolve/main/gemma-3-4b-it-Q4_K_M.gguf", "https://ai.google.dev/gemma/terms"),
                 ("Phi 4 4B", "https://huggingface.co/bartowski/microsoft_Phi-4-mini-instruct-GGUF/resolve/main/microsoft_Phi-4-mini-instruct-Q4_K_M.gguf", null),
                 ("Qwen 2.5 3B", "https://huggingface.co/Qwen/Qwen2.5-3B-Instruct-GGUF/resolve/main/qwen2.5-3b-instruct-q4_k_m.gguf", null),
             }},
            {"Tiny models (up to 2B)", new(string, string, string)[]
             {
                 ("Llama 3.2 1B", "https://huggingface.co/hugging-quants/Llama-3.2-1B-Instruct-Q4_K_M-GGUF/resolve/main/llama-3.2-1b-instruct-q4_k_m.gguf", "https://huggingface.co/meta-llama/Llama-3.2-1B/blob/main/LICENSE.txt"),
                 ("Gemma 3 1B", "https://huggingface.co/lmstudio-community/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_K_M.gguf", "https://ai.google.dev/gemma/terms"),
                 ("Qwen 2.5 1.5B", "https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_k_m.gguf", null),
                 ("Qwen 2 0.5B", "https://huggingface.co/Qwen/Qwen2-0.5B-Instruct-GGUF/resolve/main/qwen2-0_5b-instruct-q4_k_m.gguf", null),
                 ("DeepSeek R1 Distill Qwen 1.5B", "https://huggingface.co/lmstudio-community/DeepSeek-R1-Distill-Qwen-1.5B-GGUF/resolve/main/DeepSeek-R1-Distill-Qwen-1.5B-Q4_K_M.gguf", null),
             }},
            {"RAG models", new(string, string, string)[]
             {
                 ("All MiniLM L12 v2", "https://huggingface.co/leliuga/all-MiniLM-L12-v2-GGUF/resolve/main/all-MiniLM-L12-v2.Q4_K_M.gguf", null),
                 ("BGE large en v1.5", "https://huggingface.co/CompendiumLabs/bge-large-en-v1.5-gguf/resolve/main/bge-large-en-v1.5-q4_k_m.gguf", null),
                 ("BGE base en v1.5", "https://huggingface.co/CompendiumLabs/bge-base-en-v1.5-gguf/resolve/main/bge-base-en-v1.5-q4_k_m.gguf", null),
                 ("BGE small en v1.5", "https://huggingface.co/CompendiumLabs/bge-small-en-v1.5-gguf/resolve/main/bge-small-en-v1.5-q4_k_m.gguf", null),
             }},
        };

        /// \cond HIDE
        [LLMUnity] public static DebugModeType DebugMode = DebugModeType.All;
        static string DebugModeKey = "DebugMode";
        public static bool FullLlamaLib = false;
        static string FullLlamaLibKey = "FullLlamaLib";
        static List<Callback<string>> errorCallbacks = new List<Callback<string>>();
        static readonly object lockObject = new object();
        static Dictionary<string, Task> androidExtractTasks = new Dictionary<string, Task>();

        public enum DebugModeType
        {
            All,
            Warning,
            Error,
            None
        }

        public static void Log(string message)
        {
            if ((int)DebugMode > (int)DebugModeType.All) return;
            Debug.Log(message);
        }

        public static void LogWarning(string message)
        {
            if ((int)DebugMode > (int)DebugModeType.Warning) return;
            Debug.LogWarning(message);
        }

        public static void LogError(string message)
        {
            if ((int)DebugMode > (int)DebugModeType.Error) return;
            Debug.LogError(message);
            foreach (Callback<string> errorCallback in errorCallbacks) errorCallback(message);
        }

        static void LoadPlayerPrefs()
        {
            DebugMode = (DebugModeType)PlayerPrefs.GetInt(DebugModeKey, (int)DebugModeType.All);
            FullLlamaLib = PlayerPrefs.GetInt(FullLlamaLibKey, 0) == 1;
        }

        public static void SetDebugMode(DebugModeType newDebugMode)
        {
            if (DebugMode == newDebugMode) return;
            DebugMode = newDebugMode;
            PlayerPrefs.SetInt(DebugModeKey, (int)DebugMode);
            PlayerPrefs.Save();
        }

#if UNITY_EDITOR
        public static void SetFullLlamaLib(bool value)
        {
            if (FullLlamaLib == value) return;
            FullLlamaLib = value;
            PlayerPrefs.SetInt(FullLlamaLibKey, value ? 1 : 0);
            PlayerPrefs.Save();
            _ = DownloadLibrary();
        }

#endif

        public static string GetLibraryName(string version)
        {
            return $"undreamai-{version}-llamacpp";
        }

        public static string GetAssetPath(string relPath = "")
        {
            string assetsDir = Application.platform == RuntimePlatform.Android ? Application.persistentDataPath : Application.streamingAssetsPath;
            return Path.Combine(assetsDir, relPath).Replace('\\', '/');
        }

        public static string GetDownloadAssetPath(string relPath = "")
        {
            string assetsDir = (Application.platform == RuntimePlatform.Android || Application.platform == RuntimePlatform.IPhonePlayer || Application.platform == RuntimePlatform.VisionOS) ? Application.persistentDataPath : Application.streamingAssetsPath;
            return Path.Combine(assetsDir, relPath).Replace('\\', '/');
        }

#if UNITY_EDITOR
        [InitializeOnLoadMethod]
        static async Task InitializeOnLoad()
        {
            LoadPlayerPrefs();
            await DownloadLibrary();
        }

#else
        [RuntimeInitializeOnLoadMethod(RuntimeInitializeLoadType.BeforeSceneLoad)]
        void InitializeOnLoad()
        {
            LoadPlayerPrefs();
        }

#endif

        static Dictionary<string, ResumingWebClient> downloadClients = new Dictionary<string, ResumingWebClient>();

        public static void CancelDownload(string savePath)
        {
            if (!downloadClients.ContainsKey(savePath)) return;
            downloadClients[savePath].CancelDownloadAsync();
            downloadClients.Remove(savePath);
        }

        public static async Task DownloadFile(
            string fileUrl, string savePath, bool overwrite = false,
            Callback<string> callback = null, Callback<float> progressCallback = null
        )
        {
            if (File.Exists(savePath) && !overwrite)
            {
                Log($"File already exists at: {savePath}");
            }
            else
            {
                Log($"Downloading {fileUrl} to {savePath}...");
                string tmpPath = Path.Combine(Application.temporaryCachePath, Path.GetFileName(savePath));

                ResumingWebClient client = new ResumingWebClient();
                downloadClients[savePath] = client;
                await client.DownloadFileTaskAsyncResume(new Uri(fileUrl), tmpPath, !overwrite, progressCallback);
                downloadClients.Remove(savePath);
#if UNITY_EDITOR
                AssetDatabase.StartAssetEditing();
#endif
                Directory.CreateDirectory(Path.GetDirectoryName(savePath));
                File.Move(tmpPath, savePath);
#if UNITY_EDITOR
                AssetDatabase.StopAssetEditing();
#endif
                Log($"Download complete!");
            }

            progressCallback?.Invoke(1f);
            callback?.Invoke(savePath);
        }

        public static async Task AndroidExtractFile(string assetName, bool overwrite = false, bool log = true, int chunkSize = 1024*1024)
        {
            Task extractionTask;
            lock (lockObject)
            {
                if (!androidExtractTasks.TryGetValue(assetName, out extractionTask))
                {
#if UNITY_ANDROID
                    extractionTask = AndroidExtractFileOnce(assetName, overwrite, log, chunkSize);
#else
                    extractionTask = Task.CompletedTask;
#endif
                    androidExtractTasks[assetName] = extractionTask;
                }
            }
            await extractionTask;
        }

        public static async Task AndroidExtractFileOnce(string assetName, bool overwrite = false, bool log = true, int chunkSize = 1024*1024)
        {
            string source = "jar:file://" + Application.dataPath + "!/assets/" + assetName;
            string target = GetAssetPath(assetName);
            if (!overwrite && File.Exists(target))
            {
                if (log) Log($"File {target} already exists");
                return;
            }

            Log($"Extracting {source} to {target}");

            // UnityWebRequest to read the file from StreamingAssets
            UnityWebRequest www = UnityWebRequest.Get(source);
            // Send the request and await its completion
            var operation = www.SendWebRequest();

            while (!operation.isDone) await Task.Delay(1);
            if (www.result != UnityWebRequest.Result.Success)
            {
                LogError("Failed to load file from StreamingAssets: " + www.error);
            }
            else
            {
                byte[] buffer = new byte[chunkSize];
                using (Stream responseStream = new MemoryStream(www.downloadHandler.data))
                using (FileStream fileStream = new FileStream(target, FileMode.Create, FileAccess.Write))
                {
                    int bytesRead;
                    while ((bytesRead = await responseStream.ReadAsync(buffer, 0, buffer.Length)) > 0)
                    {
                        await fileStream.WriteAsync(buffer, 0, bytesRead);
                    }
                }
            }
        }

        public static async Task AndroidExtractAsset(string path, bool overwrite = false)
        {
            if (Application.platform != RuntimePlatform.Android) return;
            await AndroidExtractFile(Path.GetFileName(path), overwrite);
        }

        public static string GetFullPath(string path)
        {
            return Path.GetFullPath(path).Replace('\\', '/');
        }

        public static bool IsSubPath(string childPath, string parentPath)
        {
            return GetFullPath(childPath).StartsWith(GetFullPath(parentPath), StringComparison.OrdinalIgnoreCase);
        }

        public static string RelativePath(string fullPath, string basePath)
        {
            // Get the full paths and replace backslashes with forward slashes (or vice versa)
            string fullParentPath = GetFullPath(basePath).TrimEnd('/');
            string fullChildPath = GetFullPath(fullPath);

            string relativePath = fullChildPath;
            if (fullChildPath.StartsWith(fullParentPath, StringComparison.OrdinalIgnoreCase))
            {
                relativePath = fullChildPath.Substring(fullParentPath.Length);
                while (relativePath.StartsWith("/")) relativePath = relativePath.Substring(1);
            }
            return relativePath;
        }

        public static string SearchDirectory(string directory, string targetFileName)
        {
            string[] files = Directory.GetFiles(directory, targetFileName);
            if (files.Length > 0) return files[0];
            string[] subdirectories = Directory.GetDirectories(directory);
            foreach (var subdirectory in subdirectories)
            {
                string result = SearchDirectory(subdirectory, targetFileName);
                if (result != null) return result;
            }
            return null;
        }

#if UNITY_EDITOR

        [HideInInspector] public static float libraryProgress = 1;

        public static void CreateEmptyFile(string path)
        {
            File.Create(path).Dispose();
        }

        static void ExtractInsideDirectory(string zipPath, string extractPath, bool overwrite = true)
        {
            using (ZipArchive archive = ZipFile.OpenRead(zipPath))
            {
                foreach (ZipArchiveEntry entry in archive.Entries)
                {
                    if (string.IsNullOrEmpty(entry.Name)) continue;
                    string destinationPath = Path.Combine(extractPath, entry.FullName);
                    Directory.CreateDirectory(Path.GetDirectoryName(destinationPath));
                    entry.ExtractToFile(destinationPath, overwrite);
                }
            }
        }

        static async Task DownloadAndExtractInsideDirectory(string url, string path, string setupDir)
        {
            string urlName = Path.GetFileName(url);
            string setupFile = Path.Combine(setupDir, urlName + ".complete");
            if (File.Exists(setupFile)) return;

            string zipPath = Path.Combine(Application.temporaryCachePath, urlName);
            await DownloadFile(url, zipPath, true, null, SetLibraryProgress);

            AssetDatabase.StartAssetEditing();
            ExtractInsideDirectory(zipPath, path);
            CreateEmptyFile(setupFile);
            AssetDatabase.StopAssetEditing();

            File.Delete(zipPath);
        }

        static void DeleteEarlierVersions()
        {
            List<string> assetPathSubDirs = new List<string>();
            foreach (string dir in new string[] {GetAssetPath(), Path.Combine(Application.dataPath, "Plugins", "Android")})
            {
                if (Directory.Exists(dir)) assetPathSubDirs.AddRange(Directory.GetDirectories(dir));
            }

            Regex regex = new Regex(GetLibraryName("(.+)"));
            foreach (string assetPathSubDir in assetPathSubDirs)
            {
                Match match = regex.Match(Path.GetFileName(assetPathSubDir));
                if (match.Success)
                {
                    string version = match.Groups[1].Value;
                    if (version != LlamaLibVersion)
                    {
                        Debug.Log($"Deleting other LLMUnity version folder: {assetPathSubDir}");
                        Directory.Delete(assetPathSubDir, true);
                        if (File.Exists(assetPathSubDir + ".meta")) File.Delete(assetPathSubDir + ".meta");
                    }
                }
            }
        }

        static async Task DownloadLibrary()
        {
            if (libraryProgress < 1) return;
            libraryProgress = 0;

            try
            {
                DeleteEarlierVersions();

                string setupDir = Path.Combine(libraryPath, "setup");
                Directory.CreateDirectory(setupDir);

                // setup LlamaLib in StreamingAssets
                await DownloadAndExtractInsideDirectory(LlamaLibURL, libraryPath, setupDir);

                // setup LlamaLib extras in StreamingAssets
                if (FullLlamaLib) await DownloadAndExtractInsideDirectory(LlamaLibExtensionURL, libraryPath, setupDir);
            }
            catch (Exception e)
            {
                LogError(e.Message);
            }

            libraryProgress = 1;
        }

        private static void SetLibraryProgress(float progress)
        {
            libraryProgress = Math.Min(0.99f, progress);
        }

        public static string AddAsset(string assetPath)
        {
            if (!File.Exists(assetPath))
            {
                LogError($"{assetPath} does not exist!");
                return null;
            }
            string assetDir = GetAssetPath();
            if (IsSubPath(assetPath, assetDir)) return RelativePath(assetPath, assetDir);

            string filename = Path.GetFileName(assetPath);
            string fullPath = GetAssetPath(filename);
            AssetDatabase.StartAssetEditing();
            foreach (string path in new string[] {fullPath, fullPath + ".meta"})
            {
                if (File.Exists(path)) File.Delete(path);
            }
            File.Copy(assetPath, fullPath);
            AssetDatabase.StopAssetEditing();
            return filename;
        }

#endif
        /// \endcond

        /// <summary> Add callback function to call for error logs </summary>
        public static void AddErrorCallBack(Callback<string> callback)
        {
            errorCallbacks.Add(callback);
        }

        /// <summary> Remove callback function added for error logs </summary>
        public static void RemoveErrorCallBack(Callback<string> callback)
        {
            errorCallbacks.Remove(callback);
        }

        /// <summary> Remove all callback function added for error logs </summary>
        public static void ClearErrorCallBacks()
        {
            errorCallbacks.Clear();
        }

        public static int GetMaxFreqKHz(int cpuId)
        {
            string[] paths = new string[]
            {
                $"/sys/devices/system/cpu/cpufreq/stats/cpu{cpuId}/time_in_state",
                $"/sys/devices/system/cpu/cpu{cpuId}/cpufreq/stats/time_in_state",
                $"/sys/devices/system/cpu/cpu{cpuId}/cpufreq/cpuinfo_max_freq"
            };

            foreach (var path in paths)
            {
                if (!File.Exists(path)) continue;

                int maxFreqKHz = 0;
                using (StreamReader sr = new StreamReader(path))
                {
                    string line;
                    while ((line = sr.ReadLine()) != null)
                    {
                        string[] parts = line.Split(' ');
                        if (parts.Length > 0 && int.TryParse(parts[0], out int freqKHz))
                        {
                            if (freqKHz > maxFreqKHz)
                            {
                                maxFreqKHz = freqKHz;
                            }
                        }
                    }
                }
                if (maxFreqKHz != 0) return maxFreqKHz;
            }
            return -1;
        }

        public static bool IsSmtCpu(int cpuId)
        {
            string[] paths = new string[]
            {
                $"/sys/devices/system/cpu/cpu{cpuId}/topology/core_cpus_list",
                $"/sys/devices/system/cpu/cpu{cpuId}/topology/thread_siblings_list"
            };

            foreach (var path in paths)
            {
                if (!File.Exists(path)) continue;
                using (StreamReader sr = new StreamReader(path))
                {
                    string line;
                    while ((line = sr.ReadLine()) != null)
                    {
                        if (line.Contains(",") || line.Contains("-"))
                        {
                            return true;
                        }
                    }
                }
            }
            return false;
        }

        /// <summary>
        /// Calculates the number of big cores in Android similarly to ncnn (https://github.com/Tencent/ncnn)
        /// </summary>
        /// <returns></returns>
        public static int AndroidGetNumBigCores()
        {
            int maxFreqKHzMin = int.MaxValue;
            int maxFreqKHzMax = 0;
            List<int> cpuMaxFreqKHz = new List<int>();
            List<bool> cpuIsSmtCpu = new List<bool>();

            try
            {
                string cpuPath = "/sys/devices/system/cpu/";
                int coreIndex;
                if (Directory.Exists(cpuPath))
                {
                    foreach (string cpuDir in Directory.GetDirectories(cpuPath))
                    {
                        string dirName = Path.GetFileName(cpuDir);
                        if (!dirName.StartsWith("cpu")) continue;
                        if (!int.TryParse(dirName.Substring(3), out coreIndex)) continue;

                        int maxFreqKHz = GetMaxFreqKHz(coreIndex);
                        cpuMaxFreqKHz.Add(maxFreqKHz);
                        if (maxFreqKHz > maxFreqKHzMax) maxFreqKHzMax = maxFreqKHz;
                        if (maxFreqKHz < maxFreqKHzMin)  maxFreqKHzMin = maxFreqKHz;
                        cpuIsSmtCpu.Add(IsSmtCpu(coreIndex));
                    }
                }
            }
            catch (Exception e)
            {
                LogError(e.Message);
            }

            int numBigCores = 0;
            int numCores = SystemInfo.processorCount;
            int maxFreqKHzMedium = (maxFreqKHzMin + maxFreqKHzMax) / 2;
            if (maxFreqKHzMedium == maxFreqKHzMax) numBigCores = numCores;
            else
            {
                for (int i = 0; i < cpuMaxFreqKHz.Count; i++)
                {
                    if (cpuIsSmtCpu[i] || cpuMaxFreqKHz[i] >= maxFreqKHzMedium) numBigCores++;
                }
            }

            if (numBigCores == 0) numBigCores = SystemInfo.processorCount / 2;
            else numBigCores = Math.Min(numBigCores, SystemInfo.processorCount);

            return numBigCores;
        }

        /// <summary>
        /// Calculates the number of big cores in Android similarly to Unity (https://docs.unity3d.com/2022.3/Documentation/Manual/android-thread-configuration.html)
        /// </summary>
        /// <returns></returns>
        public static int AndroidGetNumBigCoresCapacity()
        {
            List<int> capacities = new List<int>();
            int minCapacity = int.MaxValue;
            try
            {
                string cpuPath = "/sys/devices/system/cpu/";
                int coreIndex;
                if (Directory.Exists(cpuPath))
                {
                    foreach (string cpuDir in Directory.GetDirectories(cpuPath))
                    {
                        string dirName = Path.GetFileName(cpuDir);
                        if (!dirName.StartsWith("cpu")) continue;
                        if (!int.TryParse(dirName.Substring(3), out coreIndex)) continue;

                        string capacityPath = Path.Combine(cpuDir, "cpu_capacity");
                        if (!File.Exists(capacityPath)) break;

                        int capacity = int.Parse(File.ReadAllText(capacityPath).Trim());
                        capacities.Add(capacity);
                        if (minCapacity > capacity) minCapacity = capacity;
                    }
                }
            }
            catch (Exception e)
            {
                LogError(e.Message);
            }

            int numBigCores = 0;
            foreach (int capacity in capacities)
            {
                if (capacity >= 2 * minCapacity) numBigCores++;
            }

            if (numBigCores == 0 || numBigCores > SystemInfo.processorCount) numBigCores = SystemInfo.processorCount;
            return numBigCores;
        }
    }
}


# -------------------- LLMUtils.cs --------------------

/// @file
/// @brief File implementing LLM helper code.
using System;
using System.Collections.Generic;

namespace LLMUnity
{
    /// @ingroup utils
    /// <summary>
    /// Class implementing a basic LLM Exception
    /// </summary>
    public class LLMException : Exception
    {
        public int ErrorCode { get; private set; }

        public LLMException(string message, int errorCode) : base(message)
        {
            ErrorCode = errorCode;
        }
    }

    /// @ingroup utils
    /// <summary>
    /// Class implementing a basic LLM Destroy Exception
    /// </summary>
    public class DestroyException : Exception {}

    /// @ingroup utils
    /// <summary>
    /// Class representing a LORA asset
    /// </summary>
    public class LoraAsset
    {
        public string assetPath;
        public string fullPath;
        public float weight;

        public LoraAsset(string path, float weight = 1)
        {
            assetPath = LLM.GetLLMManagerAsset(path);
            fullPath = RuntimePath(path);
            this.weight = weight;
        }

        public static string RuntimePath(string path)
        {
            return LLMUnitySetup.GetFullPath(LLM.GetLLMManagerAssetRuntime(path));
        }
    }

    /// @ingroup utils
    /// <summary>
    /// Class representing the LORA manager allowing to convert and retrieve LORA assets to string (for serialisation)
    /// </summary>
    public class LoraManager
    {
        List<LoraAsset> loras = new List<LoraAsset>();
        public string delimiter = ",";

        /// <summary>
        /// Clears the LORA assets
        /// </summary>
        public void Clear()
        {
            loras.Clear();
        }

        /// <summary>
        /// Searches for a LORA based on the path
        /// </summary>
        /// <param name="path">LORA path</param>
        /// <returns>LORA index</returns>
        public int IndexOf(string path)
        {
            string fullPath = LoraAsset.RuntimePath(path);
            for (int i = 0; i < loras.Count; i++)
            {
                LoraAsset lora = loras[i];
                if (lora.assetPath == path || lora.fullPath == fullPath) return i;
            }
            return -1;
        }

        /// <summary>
        /// Checks if the provided LORA based on a path exists already in the LORA manager
        /// </summary>
        /// <param name="path">LORA path</param>
        /// <returns>whether the LORA manager contains the LORA</returns>
        public bool Contains(string path)
        {
            return IndexOf(path) != -1;
        }

        /// <summary>
        /// Adds a LORA with the defined weight
        /// </summary>
        /// <param name="path">LORA path</param>
        /// <param name="weight">LORA weight</param>
        public void Add(string path, float weight = 1)
        {
            if (Contains(path)) return;
            loras.Add(new LoraAsset(path, weight));
        }

        /// <summary>
        /// Removes a LORA based on its path
        /// </summary>
        /// <param name="path">LORA path</param>
        public void Remove(string path)
        {
            int index = IndexOf(path);
            if (index != -1) loras.RemoveAt(index);
        }

        /// <summary>
        /// Modifies the weight of a LORA
        /// </summary>
        /// <param name="path">LORA path</param>
        /// <param name="weight">LORA weight</param>
        public void SetWeight(string path, float weight)
        {
            int index = IndexOf(path);
            if (index == -1)
            {
                LLMUnitySetup.LogError($"LoRA {path} not loaded with the LLM");
                return;
            }
            loras[index].weight = weight;
        }

        /// <summary>
        /// Converts strings with the lora paths and weights to entries in the LORA manager
        /// </summary>
        /// <param name="loraString">lora paths</param>
        /// <param name="loraWeightsString">lora weights</param>
        public void FromStrings(string loraString, string loraWeightsString)
        {
            if (string.IsNullOrEmpty(loraString) && string.IsNullOrEmpty(loraWeightsString))
            {
                Clear();
                return;
            }

            try
            {
                List<string> loraStringArr = new List<string>(loraString.Split(delimiter));
                List<string> loraWeightsStringArr = new List<string>(loraWeightsString.Split(delimiter));
                if (loraStringArr.Count != loraWeightsStringArr.Count) throw new Exception($"LoRAs number ({loraString}) doesn't match the number of weights ({loraWeightsString})");

                List<LoraAsset> lorasNew = new List<LoraAsset>();
                for (int i = 0; i < loraStringArr.Count; i++) lorasNew.Add(new LoraAsset(loraStringArr[i].Trim(), float.Parse(loraWeightsStringArr[i])));
                loras = lorasNew;
            }
            catch (Exception e)
            {
                LLMUnitySetup.LogError($"Loras not set: {e.Message}");
            }
        }

        /// <summary>
        /// Converts the entries of the LORA manager to strings with the lora paths and weights
        /// </summary>
        /// <returns>strings with the lora paths and weights</returns>
        public (string, string) ToStrings()
        {
            string loraString = "";
            string loraWeightsString = "";
            for (int i = 0; i < loras.Count; i++)
            {
                if (i > 0)
                {
                    loraString += delimiter;
                    loraWeightsString += delimiter;
                }
                loraString += loras[i].assetPath;
                loraWeightsString += loras[i].weight;
            }
            return (loraString, loraWeightsString);
        }

        /// <summary>
        /// Gets the weights of the LORAs in the manager
        /// </summary>
        /// <returns>LORA weights</returns>
        public float[] GetWeights()
        {
            float[] weights = new float[loras.Count];
            for (int i = 0; i < loras.Count; i++) weights[i] = loras[i].weight;
            return weights;
        }

        /// <summary>
        /// Gets the paths of the LORAs in the manager
        /// </summary>
        /// <returns>LORA paths</returns>
        public string[] GetLoras()
        {
            string[] loraPaths = new string[loras.Count];
            for (int i = 0; i < loras.Count; i++) loraPaths[i] = loras[i].assetPath;
            return loraPaths;
        }
    }
    /// \endcond
}


# -------------------- NativeMethods.cs --------------------

using System.Runtime.InteropServices;

using usearch_index_t = System.IntPtr;
using usearch_key_t = System.UInt64;
using usearch_error_t = System.IntPtr;
using size_t = System.UIntPtr;
using void_ptr_t = System.IntPtr;
using usearch_distance_t = System.Single;

namespace Cloud.Unum.USearch
{
    public static class NativeMethods
    {
        private const string LibraryName = "libusearch_c";

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern usearch_index_t usearch_init(ref IndexOptions options, out usearch_error_t error);

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern void usearch_free(usearch_index_t index, out usearch_error_t error);

        [DllImport(LibraryName, CharSet = CharSet.Ansi, CallingConvention = CallingConvention.Cdecl)]
        public static extern void usearch_save(usearch_index_t index, [MarshalAs(UnmanagedType.LPStr)] string path, out usearch_error_t error);

        [DllImport(LibraryName, CharSet = CharSet.Ansi, CallingConvention = CallingConvention.Cdecl)]
        public static extern void usearch_load(usearch_index_t index, [MarshalAs(UnmanagedType.LPStr)] string path, out usearch_error_t error);

        [DllImport(LibraryName, CharSet = CharSet.Ansi, CallingConvention = CallingConvention.Cdecl)]
        public static extern void usearch_view(usearch_index_t index, [MarshalAs(UnmanagedType.LPStr)] string path, out usearch_error_t error);

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern size_t usearch_size(usearch_index_t index, out usearch_error_t error);

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern size_t usearch_capacity(usearch_index_t index, out usearch_error_t error);

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern size_t usearch_dimensions(usearch_index_t index, out usearch_error_t error);

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern size_t usearch_connectivity(usearch_index_t index, out usearch_error_t error);

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern void usearch_reserve(usearch_index_t index, size_t capacity, out usearch_error_t error);

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern void usearch_add(
            usearch_index_t index,
            usearch_key_t key,
            [In] float[] vector,
            ScalarKind vector_kind,
            out usearch_error_t error
        );

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern void usearch_add(
            usearch_index_t index,
            usearch_key_t key,
            [In] double[] vector,
            ScalarKind vector_kind,
            out usearch_error_t error
        );

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        [return : MarshalAs(UnmanagedType.I1)]
        public static extern bool usearch_contains(usearch_index_t index, usearch_key_t key, out usearch_error_t error);

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern size_t usearch_count(usearch_index_t index, usearch_key_t key, out usearch_error_t error);

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern size_t usearch_search(
            usearch_index_t index,
            void_ptr_t query_vector,
            ScalarKind query_kind,
            size_t count,
            [Out] usearch_key_t[] found_keys,
            [Out] usearch_distance_t[] found_distances,
            out usearch_error_t error
        );

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern size_t usearch_search(
            usearch_index_t index,
            [In] float[] query_vector,
            ScalarKind query_kind,
            size_t count,
            [Out] usearch_key_t[] found_keys,
            [Out] usearch_distance_t[] found_distances,
            out usearch_error_t error
        );

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern size_t usearch_search(
            usearch_index_t index,
            [In] double[] query_vector,
            ScalarKind query_kind,
            size_t count,
            [Out] usearch_key_t[] found_keys,
            [Out] usearch_distance_t[] found_distances,
            out usearch_error_t error
        );


        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern size_t usearch_get(
            usearch_index_t index,
            usearch_key_t key,
            size_t count,
            [Out] float[] vector,
            ScalarKind vector_kind,
            out usearch_error_t error
        );

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern size_t usearch_get(
            usearch_index_t index,
            usearch_key_t key,
            size_t count,
            [Out] double[] vector,
            ScalarKind vector_kind,
            out usearch_error_t error
        );

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern size_t usearch_remove(usearch_index_t index, usearch_key_t key, out usearch_error_t error);

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern size_t usearch_rename(usearch_index_t index, usearch_key_t key_from, usearch_key_t key_to, out usearch_error_t error);

        //========================== Additional methods from LLMUnity ==========================//

        [UnmanagedFunctionPointer(CallingConvention.Cdecl)]
        public delegate int FilterCallback(int key, void_ptr_t filterState);

        [DllImport(LibraryName, CharSet = CharSet.Ansi, CallingConvention = CallingConvention.Cdecl)]
        public static extern void usearch_load_buffer(usearch_index_t index, void_ptr_t buffer, size_t length, out usearch_error_t error);

        [DllImport(LibraryName, CharSet = CharSet.Ansi, CallingConvention = CallingConvention.Cdecl)]
        public static extern void usearch_view_buffer(usearch_index_t index, void_ptr_t buffer, size_t length, out usearch_error_t error);

        [DllImport(LibraryName, CallingConvention = CallingConvention.Cdecl)]
        public static extern size_t usearch_filtered_search(
            usearch_index_t index,
            void_ptr_t query_vector,
            ScalarKind query_kind,
            size_t count,
            FilterCallback filter,
            void_ptr_t filterState,
            [Out] usearch_key_t[] found_keys,
            [Out] usearch_distance_t[] found_distances,
            out usearch_error_t error
        );
    }
}


# -------------------- RAG.cs --------------------

/// @file
/// @brief File implementing the Retrieval Augmented Generation (RAG) system.
using System;
using System.IO.Compression;
using System.Threading.Tasks;
using UnityEditor;
using UnityEngine;

namespace LLMUnity
{
    /// <summary>
    /// Search methods implemented in LLMUnity
    /// </summary>
    public enum SearchMethods
    {
        DBSearch,
        SimpleSearch,
    }

    public class NoChunking {}

    /// <summary>
    /// Chunking methods implemented in LLMUnity
    /// </summary>
    public enum ChunkingMethods
    {
        NoChunking,
        TokenSplitter,
        WordSplitter,
        SentenceSplitter
    }

    /// @ingroup rag
    /// <summary>
    /// Class implementing a Retrieval Augmented Generation (RAG) system based on a search method and an optional chunking method.
    /// </summary>
    [Serializable]
    public class RAG : Searchable
    {
        /// <summary> Search method type to use for RAG. SimpleSearch is a simple brute-force search, while DBSearch is a fast Approximate Nearest Neighbor (ANN) method (recommended!). </summary>
        [Tooltip("Search method type to use for RAG. SimpleSearch is a simple brute-force search, while DBSearch is a fast Approximate Nearest Neighbor (ANN) method (recommended!).")]
        public SearchMethods searchType = SearchMethods.SimpleSearch;
        /// <summary> Search method GameObject. </summary>
        [Tooltip("Search method GameObject.")]
        public SearchMethod search;
        /// <summary> Chunking method type to use for RAG for splitting the inputs into chunks. This is useful to have a more consistent meaning within each data part. </summary>
        [Tooltip("Chunking method type to use for RAG for splitting the inputs into chunks. This is useful to have a more consistent meaning within each data part.")]
        public ChunkingMethods chunkingType = ChunkingMethods.NoChunking;
        /// <summary> Chunking method GameObject. </summary>
        [Tooltip("Chunking method GameObject.")]
        public Chunking chunking;

        /// <summary>
        /// Constructs the Retrieval Augmented Generation (RAG) system based on the provided search and chunking method.
        /// </summary>
        /// <param name="searchMethod">search method</param>
        /// <param name="chunkingMethod">chunking method for splitting the search entries</param>
        /// <param name="llm">LLM to use for the search method</param>
        public void Init(SearchMethods searchMethod = SearchMethods.SimpleSearch, ChunkingMethods chunkingMethod = ChunkingMethods.NoChunking, LLM llm = null)
        {
            searchType = searchMethod;
            chunkingType = chunkingMethod;
            UpdateGameObjects();
            search.SetLLM(llm);
        }

        /// <summary>
        /// Set to true to return chunks or the direct input with the Search function
        /// </summary>
        /// <param name="returnChunks">whether to return chunks</param>
        public void ReturnChunks(bool returnChunks)
        {
            if (chunking != null) chunking.ReturnChunks(returnChunks);
        }

        /// \cond HIDE
        protected void ConstructSearch()
        {
            search = ConstructComponent<SearchMethod>(Type.GetType("LLMUnity." + searchType.ToString()), (previous, current) => current.llmEmbedder.llm = previous.llmEmbedder.llm);
            if (chunking != null) chunking.SetSearch(search);
        }

        protected void ConstructChunking()
        {
            Type type = null;
            if (chunkingType != ChunkingMethods.NoChunking) type = Type.GetType("LLMUnity." + chunkingType.ToString());
            chunking = ConstructComponent<Chunking>(type);
            if (chunking != null) chunking.SetSearch(search);
        }

        public override void UpdateGameObjects()
        {
            if (this == null) return;
            ConstructSearch();
            ConstructChunking();
        }

        protected Searchable GetSearcher()
        {
            if (chunking != null) return chunking;
            if (search != null) return search;
            throw new Exception("The search GameObject is null");
        }

#if UNITY_EDITOR
        private void OnValidateUpdate()
        {
            EditorApplication.delayCall -= OnValidateUpdate;
            UpdateGameObjects();
        }

        public virtual void OnValidate()
        {
            if (!Application.isPlaying) EditorApplication.delayCall += OnValidateUpdate;
        }

#endif

        public override string Get(int key) { return GetSearcher().Get(key); }
        public override async Task<int> Add(string inputString, string group = "") { return await GetSearcher().Add(inputString, group); }
        public override int Remove(string inputString, string group = "") { return GetSearcher().Remove(inputString, group); }
        public override void Remove(int key) { GetSearcher().Remove(key); }
        public override int Count() { return GetSearcher().Count(); }
        public override int Count(string group) { return GetSearcher().Count(group); }
        public override void Clear() { GetSearcher().Clear(); }
        public override async Task<int> IncrementalSearch(string queryString, string group = "") { return await GetSearcher().IncrementalSearch(queryString, group);}
        public override (string[], float[], bool) IncrementalFetch(int fetchKey, int k) { return GetSearcher().IncrementalFetch(fetchKey, k);}
        public override (int[], float[], bool) IncrementalFetchKeys(int fetchKey, int k) { return GetSearcher().IncrementalFetchKeys(fetchKey, k);}
        public override void IncrementalSearchComplete(int fetchKey) { GetSearcher().IncrementalSearchComplete(fetchKey);}
        public override void Save(ZipArchive archive) { GetSearcher().Save(archive); }
        public override void Load(ZipArchive archive) { GetSearcher().Load(archive); }
        /// \endcond
    }
}


# -------------------- ResumingWebClient.cs --------------------

/// @file
/// @brief File implementing a resumable Web client
using System;
using System.Collections.Generic;
using System.IO;
using System.Net;
using System.Threading;
using System.Threading.Tasks;

namespace LLMUnity
{
    /// @ingroup utils
    /// <summary>
    /// Class implementing a resumable Web client
    /// </summary>
    public class ResumingWebClient : WebClient
    {
        private const int timeoutMs = 30 * 1000;
        private SynchronizationContext _context;
        private const int DefaultDownloadBufferLength = 65536;
        List<WebRequest> requests = new List<WebRequest>();

        public ResumingWebClient()
        {
            _context = SynchronizationContext.Current ?? new SynchronizationContext();
        }

        public long GetURLFileSize(string address)
        {
            return GetURLFileSize(new Uri(address));
        }

        public long GetURLFileSize(Uri address)
        {
            WebRequest request = GetWebRequest(address);
            request.Method = "HEAD";
            WebResponse response = request.GetResponse();
            return response.ContentLength;
        }

        public Task DownloadFileTaskAsyncResume(Uri address, string fileName, bool resume = false, Callback<float> progressCallback = null)
        {
            var tcs = new TaskCompletionSource<object>(address);
            FileStream fs = null;
            long bytesToSkip = 0;

            try
            {
                FileMode filemode = FileMode.Create;
                if (resume)
                {
                    var fileInfo = new FileInfo(fileName);
                    if (fileInfo.Exists) bytesToSkip = fileInfo.Length;
                }

                WebRequest request = GetWebRequest(address);
                if (request is HttpWebRequest webRequest && bytesToSkip > 0)
                {
                    long remoteFileSize = GetURLFileSize(address);
                    if (bytesToSkip >= remoteFileSize)
                    {
                        LLMUnitySetup.Log($"File is already fully downloaded: {fileName}");
                        tcs.TrySetResult(true);
                        return tcs.Task;
                    }

                    filemode = FileMode.Append;
                    LLMUnitySetup.Log($"File exists at {fileName}, skipping {bytesToSkip} bytes");
                    webRequest.AddRange(bytesToSkip);
                    webRequest.ReadWriteTimeout = timeoutMs;
                }

                fs = new FileStream(fileName, filemode, FileAccess.Write);
                DownloadBitsAsync(request, fs, bytesToSkip, progressCallback, tcs);
            }
            catch (Exception e)
            {
                fs?.Close();
                tcs.TrySetException(e);
            }

            return tcs.Task;
        }

        public void CancelDownloadAsync()
        {
            LLMUnitySetup.Log("Cancellation requested, aborting download.");
            foreach (WebRequest request in requests) AbortRequest(request);
            requests.Clear();
        }

        public void AbortRequest(WebRequest request)
        {
            try
            {
                request?.Abort();
            }
            catch (Exception e)
            {
                LLMUnitySetup.LogError($"Error aborting request: {e.Message}");
            }
        }

        private async void DownloadBitsAsync(WebRequest request, Stream writeStream, long bytesToSkip = 0, Callback<float> progressCallback = null, TaskCompletionSource<object> tcs = null)
        {
            try
            {
                requests.Add(request);
                WebResponse response = await request.GetResponseAsync().ConfigureAwait(false);

                long contentLength = response.ContentLength;
                byte[] copyBuffer = new byte[contentLength == -1 || contentLength > DefaultDownloadBufferLength ? DefaultDownloadBufferLength : contentLength];

                long TotalBytesToReceive = Math.Max(contentLength, 0) + bytesToSkip;
                long BytesReceived = bytesToSkip;

                using (writeStream)
                using (Stream readStream = response.GetResponseStream())
                {
                    if (readStream != null)
                    {
                        while (true)
                        {
                            int bytesRead = await readStream.ReadAsync(new Memory<byte>(copyBuffer)).ConfigureAwait(false);
                            if (bytesRead == 0)
                            {
                                break;
                            }

                            BytesReceived += bytesRead;
                            if (BytesReceived != TotalBytesToReceive)
                            {
                                PostProgressChanged(progressCallback, BytesReceived, TotalBytesToReceive);
                            }

                            await writeStream.WriteAsync(new ReadOnlyMemory<byte>(copyBuffer, 0, bytesRead)).ConfigureAwait(false);
                        }
                    }

                    if (TotalBytesToReceive < 0)
                    {
                        TotalBytesToReceive = BytesReceived;
                    }
                    PostProgressChanged(progressCallback, BytesReceived, TotalBytesToReceive);
                }
                tcs.TrySetResult(true);
            }
            catch (Exception e)
            {
                tcs.TrySetException(e);
                LLMUnitySetup.LogError(e.Message);
                AbortRequest(request);
                tcs.TrySetResult(false);
            }
            finally
            {
                writeStream?.Close();
                requests.Remove(request);
            }
        }

        private void PostProgressChanged(Callback<float> progressCallback, long BytesReceived, long TotalBytesToReceive)
        {
            if (progressCallback != null && BytesReceived > 0)
            {
                float progressPercentage = TotalBytesToReceive < 0 ? 0 : TotalBytesToReceive == 0 ? 1 : (float)BytesReceived / TotalBytesToReceive;
                _context.Post(_ => progressCallback?.Invoke(progressPercentage), null);
            }
        }
    }
}


# -------------------- Search.cs --------------------

/// @file
/// @brief File implementing the search functionality
using System;
using System.Collections.Generic;
using System.IO;
using System.IO.Compression;
using System.Linq;
using System.Runtime.Serialization.Formatters.Binary;
using System.Threading.Tasks;
using UnityEditor;
using UnityEngine;

/// @defgroup rag RAG
namespace LLMUnity
{
    /// @ingroup rag
    /// <summary>
    /// Class implementing the search template
    /// </summary>
    [DefaultExecutionOrder(-2)]
    public abstract class Searchable : MonoBehaviour
    {
        /// <summary>
        /// Retrieves the phrase with the specific id
        /// </summary>
        /// <param name="key">phrase id</param>
        /// <returns>phrase</returns>
        public abstract string Get(int key);

        /// <summary>
        /// Adds a phrase to the search.
        /// </summary>
        /// <param name="inputString">input phrase</param>
        /// <param name="group">data group to add it to </param>
        /// <returns>phrase id</returns>
        public abstract Task<int> Add(string inputString, string group = "");

        /// <summary>
        /// Removes a phrase from the search.
        /// </summary>
        /// <param name="inputString">input phrase</param>
        /// <param name="group">data group to remove it from </param>
        /// <returns>number of removed entries/returns>
        public abstract int Remove(string inputString, string group = "");

        /// <summary>
        /// Removes a phrase from the search.
        /// </summary>
        /// <param name="key">phrase id</param>
        public abstract void Remove(int key);

        /// <summary>
        /// Returns a count of the phrases
        /// </summary>
        /// <returns>phrase count</returns>
        public abstract int Count();

        /// <summary>
        /// Returns a count of the phrases in a specific data group
        /// </summary>
        /// <param name="group">data group</param>
        /// <returns>phrase count</returns>
        public abstract int Count(string group);

        /// <summary>
        /// Clears the search object
        /// </summary>
        public abstract void Clear();

        /// <summary>
        /// Allows to do search and retrieve results in batches (incremental search).
        /// </summary>
        /// <param name="queryString">search query</param>
        /// <param name="group">data group to search in</param>
        /// <returns>incremental search key</returns>
        public abstract Task<int> IncrementalSearch(string queryString, string group = "");

        /// <summary>
        /// Retrieves the most similar search results in batches (incremental search).
        /// The phrase keys and distances are retrieved, as well as a parameter that dictates whether the search is exhausted.
        /// </summary>
        /// <param name="fetchKey">incremental search key</param>
        /// <param name="k">number of results to retrieve</param>
        /// <returns>
        /// A tuple containing:
        /// <list type="bullet">
        /// <item><description>Array of retrieved keys (`int[]`).</description></item>
        /// <item><description>Array of distances for each result (`float[]`).</description></item>
        /// <item><description>`bool` indicating if the search is exhausted.</description></item>
        /// </list>
        /// </returns>
        public abstract ValueTuple<int[], float[], bool> IncrementalFetchKeys(int fetchKey, int k);

        /// <summary>
        /// Completes the search and clears the cached results for an incremental search
        /// </summary>
        /// <param name="fetchKey">incremental search key</param>
        public abstract void IncrementalSearchComplete(int fetchKey);

        /// <summary>
        /// Search for similar results to the provided query.
        /// The most similar results and their distances (dissimilarity) to the query are retrieved.
        /// </summary>
        /// <param name="queryString">query</param>
        /// <param name="k">number of results to retrieve</param>
        /// <param name="group">data group to search in</param>
        /// <returns>
        /// A tuple containing:
        /// <list type="bullet">
        /// <item><description>Array of retrieved results (`string[]`).</description></item>
        /// <item><description>Array of distances for each result (`float[]`).</description></item>
        /// <item><description>`bool` indicating if the search is exhausted.</description></item>
        /// </list>
        /// </returns>
        public async Task<(string[], float[])> Search(string queryString, int k, string group = "")
        {
            int fetchKey = await IncrementalSearch(queryString, group);
            (string[] phrases, float[] distances, bool completed) = IncrementalFetch(fetchKey, k);
            if (!completed) IncrementalSearchComplete(fetchKey);
            return (phrases, distances);
        }

        /// <summary>
        /// Retrieves the most similar search results in batches (incremental search).
        /// The most similar results and their distances (dissimilarity) to the query are retrieved as well as a parameter that dictates whether the search is exhausted.
        /// </summary>
        /// <param name="fetchKey">incremental search key</param>
        /// <param name="k">number of results to retrieve</param>
        /// <returns>
        /// A tuple containing:
        /// <list type="bullet">
        /// <item><description>Array of retrieved results (`string[]`).</description></item>
        /// <item><description>Array of distances for each result (`float[]`).</description></item>
        /// <item><description>`bool` indicating if the search is exhausted.</description></item>
        /// </list>
        /// </returns>
        public virtual ValueTuple<string[], float[], bool> IncrementalFetch(int fetchKey, int k)
        {
            (int[] resultKeys, float[] distances, bool completed) = IncrementalFetchKeys(fetchKey, k);
            string[] results = new string[resultKeys.Length];
            for (int i = 0; i < resultKeys.Length; i++) results[i] = Get(resultKeys[i]);
            return (results, distances, completed);
        }

        /// <summary>
        /// Saves the state of the search object.
        /// </summary>
        /// <param name="archive">file to save to</param>
        public void Save(string filePath)
        {
            try
            {
                string path = LLMUnitySetup.GetAssetPath(filePath);
                ArchiveSaver.Save(path, Save);
            }
            catch (Exception e)
            {
                LLMUnitySetup.LogError($"File {filePath} could not be saved due to {e.GetType()}: {e.Message}");
            }
        }

        /// <summary>
        /// Loads the state of the search object.
        /// </summary>
        /// <param name="archive">file to load from</param>
        public async Task<bool> Load(string filePath)
        {
            try
            {
                await LLMUnitySetup.AndroidExtractAsset(filePath, true);
                string path = LLMUnitySetup.GetAssetPath(filePath);
                if (!File.Exists(path)) return false;
                ArchiveSaver.Load(path, Load);
            }
            catch (Exception e)
            {
                LLMUnitySetup.LogError($"File {filePath} could not be loaded due to {e.GetType()}: {e.Message}");
                return false;
            }
            return true;
        }

        /// \cond HIDE
        public abstract void Save(ZipArchive archive);
        public abstract void Load(ZipArchive archive);
        public virtual string GetSavePath(string name)
        {
            return Path.Combine(GetType().Name, name);
        }

        public virtual void UpdateGameObjects() {}

        protected T ConstructComponent<T>(Type type, Action<T, T> copyAction = null) where T : Component
        {
            T Construct(Type type)
            {
                if (type == null) return null;
                T newComponent = (T)gameObject.AddComponent(type);
                if (newComponent is Searchable searchable) searchable.UpdateGameObjects();
                return newComponent;
            }

            T component = (T)gameObject.GetComponent(typeof(T));
            T newComponent;
            if (component == null)
            {
                newComponent = Construct(type);
            }
            else
            {
                if (component.GetType() == type)
                {
                    newComponent = component;
                }
                else
                {
                    newComponent = Construct(type);
                    if (type != null) copyAction?.Invoke(component, newComponent);
#if UNITY_EDITOR
                    DestroyImmediate(component);
#else
                    Destroy(component);
#endif
                }
            }
            return newComponent;
        }

        public virtual void Awake()
        {
            UpdateGameObjects();
        }

#if UNITY_EDITOR
        public virtual void Reset()
        {
            if (!Application.isPlaying) EditorApplication.update += UpdateGameObjects;
        }

        public virtual void OnDestroy()
        {
            if (!Application.isPlaying) EditorApplication.update -= UpdateGameObjects;
        }

#endif
        /// \endcond
    }

    /// @ingroup rag
    /// <summary>
    /// Class implementing the search method template
    /// </summary>
    public abstract class SearchMethod : Searchable
    {
        public LLMEmbedder llmEmbedder;

        protected int nextKey = 0;
        protected int nextIncrementalSearchKey = 0;
        protected SortedDictionary<int, string> data = new SortedDictionary<int, string>();
        protected SortedDictionary<string, List<int>> dataSplits = new SortedDictionary<string, List<int>>();

        protected LLM llm;

        protected abstract void AddInternal(int key, float[] embedding);
        protected abstract void RemoveInternal(int key);
        protected abstract void ClearInternal();
        protected abstract void SaveInternal(ZipArchive archive);
        protected abstract void LoadInternal(ZipArchive archive);

        /// <summary>
        /// Sets the LLM for encoding the search entries
        /// </summary>
        /// <param name="llm"></param>
        public void SetLLM(LLM llm)
        {
            this.llm = llm;
            if (llmEmbedder != null) llmEmbedder.llm = llm;
        }

        /// <summary>
        /// Orders the entries in the searchList according to their similarity to the provided query.
        /// The entries and distances (dissimilarity) to the query are returned in decreasing order of similarity.
        /// </summary>
        /// <param name="queryString">query</param>
        /// <param name="searchList">entries to order based on similarity</param>
        /// <returns>
        /// A tuple containing:
        /// <list type="bullet">
        /// <item><description>Array of entries (`string[]`).</description></item>
        /// <item><description>Array of distances for each result (`float[]`).</description></item>
        /// </list>
        /// </returns>
        public async Task<(string[], float[])> SearchFromList(string query, string[] searchList)
        {
            float[] embedding = await Encode(query);
            float[][] embeddingsList = new float[searchList.Length][];
            for (int i = 0; i < searchList.Length; i++) embeddingsList[i] = await Encode(searchList[i]);

            float[] unsortedDistances = InverseDotProduct(embedding, embeddingsList);
            List<(string, float)> sortedLists = searchList.Zip(unsortedDistances, (first, second) => (first, second))
                .OrderBy(item => item.Item2)
                .ToList();

            string[] results = new string[sortedLists.Count];
            float[] distances = new float[sortedLists.Count];
            for (int i = 0; i < sortedLists.Count; i++)
            {
                results[i] = sortedLists[i].Item1;
                distances[i] = sortedLists[i].Item2;
            }
            return (results.ToArray(), distances.ToArray());
        }

        /// \cond HIDE
        public static float DotProduct(float[] vector1, float[] vector2)
        {
            if (vector1 == null || vector2 == null) throw new ArgumentNullException("Vectors cannot be null");
            if (vector1.Length != vector2.Length) throw new ArgumentException("Vector lengths must be equal for dot product calculation");
            float result = 0;
            for (int i = 0; i < vector1.Length; i++)
            {
                result += vector1[i] * vector2[i];
            }
            return result;
        }

        public static float InverseDotProduct(float[] vector1, float[] vector2)
        {
            return 1 - DotProduct(vector1, vector2);
        }

        public static float[] InverseDotProduct(float[] vector1, float[][] vector2)
        {
            float[] results = new float[vector2.Length];
            for (int i = 0; i < vector2.Length; i++)
            {
                results[i] = InverseDotProduct(vector1, vector2[i]);
            }
            return results;
        }

        public virtual async Task<float[]> Encode(string inputString)
        {
            return (await llmEmbedder.Embeddings(inputString)).ToArray();
        }

        public virtual async Task<List<int>> Tokenize(string query, Callback<List<int>> callback = null)
        {
            return await llmEmbedder.Tokenize(query, callback);
        }

        public async Task<string> Detokenize(List<int> tokens, Callback<string> callback = null)
        {
            return await llmEmbedder.Detokenize(tokens, callback);
        }

        public override string Get(int key)
        {
            if (data.TryGetValue(key, out string result)) return result;
            return null;
        }

        public override async Task<int> Add(string inputString, string group = "")
        {
            int key = nextKey++;
            AddInternal(key, await Encode(inputString));

            data[key] = inputString;
            if (!dataSplits.ContainsKey(group)) dataSplits[group] = new List<int>(){key};
            else dataSplits[group].Add(key);
            return key;
        }

        public override void Clear()
        {
            data.Clear();
            dataSplits.Clear();
            ClearInternal();
            nextKey = 0;
            nextIncrementalSearchKey = 0;
        }

        protected bool RemoveEntry(int key)
        {
            bool removed = data.Remove(key);
            if (removed) RemoveInternal(key);
            return removed;
        }

        public override void Remove(int key)
        {
            if (RemoveEntry(key))
            {
                foreach (var dataSplit in dataSplits.Values) dataSplit.Remove(key);
            }
        }

        public override int Remove(string inputString, string group = "")
        {
            if (!dataSplits.TryGetValue(group, out List<int> dataSplit)) return 0;
            List<int> removeIds = new List<int>();
            foreach (int key in dataSplit)
            {
                if (Get(key) == inputString) removeIds.Add(key);
            }
            foreach (int key in removeIds)
            {
                if (RemoveEntry(key)) dataSplit.Remove(key);
            }
            return removeIds.Count;
        }

        public override int Count()
        {
            return data.Count;
        }

        public override int Count(string group)
        {
            if (!dataSplits.TryGetValue(group, out List<int> dataSplit)) return 0;
            return dataSplit.Count;
        }

        public override async Task<int> IncrementalSearch(string queryString, string group = "")
        {
            return IncrementalSearch(await Encode(queryString), group);
        }

        public override void Save(ZipArchive archive)
        {
            ArchiveSaver.Save(archive, data, GetSavePath("data"));
            ArchiveSaver.Save(archive, dataSplits, GetSavePath("dataSplits"));
            ArchiveSaver.Save(archive, nextKey, GetSavePath("nextKey"));
            ArchiveSaver.Save(archive, nextIncrementalSearchKey, GetSavePath("nextIncrementalSearchKey"));
            SaveInternal(archive);
        }

        public override void Load(ZipArchive archive)
        {
            data = ArchiveSaver.Load<SortedDictionary<int, string>>(archive, GetSavePath("data"));
            dataSplits = ArchiveSaver.Load<SortedDictionary<string, List<int>>>(archive, GetSavePath("dataSplits"));
            nextKey = ArchiveSaver.Load<int>(archive, GetSavePath("nextKey"));
            nextIncrementalSearchKey = ArchiveSaver.Load<int>(archive, GetSavePath("nextIncrementalSearchKey"));
            LoadInternal(archive);
        }

        public override void UpdateGameObjects()
        {
            if (this == null || llmEmbedder != null) return;
            llmEmbedder = ConstructComponent<LLMEmbedder>(typeof(LLMEmbedder), (previous, current) => current.llm = previous.llm);
        }

        public abstract int IncrementalSearch(float[] embedding, string group = "");
        /// \endcond
    }

    /// @ingroup rag
    /// <summary>
    /// Class implementing the search plugin template used e.g. in chunking
    /// </summary>
    public abstract class SearchPlugin : Searchable
    {
        protected SearchMethod search;

        /// <summary>
        /// Sets the search method of the plugin
        /// </summary>
        /// <param name="llm"></param>
        public void SetSearch(SearchMethod search)
        {
            this.search = search;
        }

        /// \cond HIDE
        protected abstract void SaveInternal(ZipArchive archive);
        protected abstract void LoadInternal(ZipArchive archive);

        public override void Save(ZipArchive archive)
        {
            search.Save(archive);
            SaveInternal(archive);
        }

        public override void Load(ZipArchive archive)
        {
            search.Load(archive);
            LoadInternal(archive);
        }

        /// \endcond
    }

    /// \cond HIDE
    public class ArchiveSaver
    {
        public delegate void ArchiveSaverCallback(ZipArchive archive);

        public static void Save(string filePath, ArchiveSaverCallback callback)
        {
            using (FileStream stream = new FileStream(filePath, FileMode.Create))
            using (ZipArchive archive = new ZipArchive(stream, ZipArchiveMode.Create))
            {
                callback(archive);
            }
        }

        public static void Load(string filePath, ArchiveSaverCallback callback)
        {
            using (FileStream stream = new FileStream(filePath, FileMode.Open, FileAccess.Read))
            using (ZipArchive archive = new ZipArchive(stream, ZipArchiveMode.Read))
            {
                callback(archive);
            }
        }

        public static void Save(ZipArchive archive, object saveObject, string name)
        {
            ZipArchiveEntry mainEntry = archive.CreateEntry(name);
            using (Stream entryStream = mainEntry.Open())
            {
                BinaryFormatter formatter = new BinaryFormatter();
                formatter.Serialize(entryStream, saveObject);
            }
        }

        public static T Load<T>(ZipArchive archive, string name)
        {
            ZipArchiveEntry baseEntry = archive.GetEntry(name);
            if (baseEntry == null) throw new Exception($"No entry with name {name} was found");
            using (Stream entryStream = baseEntry.Open())
            {
                BinaryFormatter formatter = new BinaryFormatter();
                return (T)formatter.Deserialize(entryStream);
            }
        }
    }
    /// \endcond
}


# -------------------- SentenceSplitter.cs --------------------

/// @file
/// @brief File implementing a sentence-based splitter
using System;
using System.Collections.Generic;
using System.Threading.Tasks;
using System.Linq;
using UnityEngine;

namespace LLMUnity
{
    /// @ingroup rag
    /// <summary>
    /// Class implementing a sentence-based splitter
    /// </summary>
    [Serializable]
    public class SentenceSplitter : Chunking
    {
        public const string DefaultDelimiters = ".!:;?\n\r";
        /// <summary> delimiters used to split the phrases </summary>
        [Tooltip("delimiters used to split the phrases")]
        public char[] delimiters = DefaultDelimiters.ToCharArray();

        /// <summary>
        /// Splits the provided phrase into chunks according to delimiters (defined in the delimiters variable)
        /// </summary>
        /// <param name="input">phrase</param>
        /// <returns>List of start/end indices of the split chunks</returns>
        public override async Task<List<(int, int)>> Split(string input)
        {
            List<(int, int)> indices = new List<(int, int)>();
            await Task.Run(() => {
                int startIndex = 0;
                bool seenChar = false;
                for (int i = 0; i < input.Length; i++)
                {
                    bool isDelimiter = delimiters.Contains(input[i]);
                    if (isDelimiter)
                    {
                        while ((i < input.Length - 1) && (delimiters.Contains(input[i + 1]) || char.IsWhiteSpace(input[i + 1]))) i++;
                    }
                    else
                    {
                        if (!seenChar) seenChar = !char.IsWhiteSpace(input[i]);
                    }
                    if ((i == input.Length - 1) || (isDelimiter && seenChar))
                    {
                        indices.Add((startIndex, i));
                        startIndex = i + 1;
                    }
                }
            });
            return indices;
        }
    }
}


# -------------------- SimpleSearch.cs --------------------

using System;
using System.Collections.Generic;
using System.Linq;
using System.IO.Compression;
using UnityEngine;

namespace LLMUnity
{
    /// @ingroup rag
    /// <summary>
    /// Class implementing a simple search that compares the enconding of the search query with all the search entries (brute-force).
    /// </summary>
    [DefaultExecutionOrder(-2)]
    public class SimpleSearch : SearchMethod
    {
        /// \cond HIDE
        protected SortedDictionary<int, float[]> embeddings = new SortedDictionary<int, float[]>();
        protected Dictionary<int, List<(int, float)>> incrementalSearchCache = new Dictionary<int, List<(int, float)>>();

        protected override void AddInternal(int key, float[] embedding)
        {
            embeddings[key] = embedding;
        }

        protected override void RemoveInternal(int key)
        {
            embeddings.Remove(key);
        }

        public override int IncrementalSearch(float[] embedding, string group = "")
        {
            int key = nextIncrementalSearchKey++;

            List<(int, float)> sortedLists = new List<(int, float)>();
            if (dataSplits.TryGetValue(group, out List<int> dataSplit))
            {
                if (dataSplit.Count >= 0)
                {
                    float[][] embeddingsSplit = new float[dataSplit.Count][];
                    for (int i = 0; i < dataSplit.Count; i++) embeddingsSplit[i] = embeddings[dataSplit[i]];

                    float[] unsortedDistances = InverseDotProduct(embedding, embeddingsSplit);
                    sortedLists = dataSplit.Zip(unsortedDistances, (first, second) => (first, second))
                        .OrderBy(item => item.Item2)
                        .ToList();
                }
            }
            incrementalSearchCache[key] = sortedLists;
            return key;
        }

        public override ValueTuple<int[], float[], bool> IncrementalFetchKeys(int fetchKey, int k)
        {
            if (!incrementalSearchCache.ContainsKey(fetchKey)) throw new Exception($"There is no IncrementalSearch cached with this key: {fetchKey}");

            bool completed;
            List<(int, float)> sortedLists;
            if (k == -1)
            {
                sortedLists = incrementalSearchCache[fetchKey];
                completed = true;
            }
            else
            {
                int getK = Math.Min(k, incrementalSearchCache[fetchKey].Count);
                sortedLists = incrementalSearchCache[fetchKey].GetRange(0, getK);
                incrementalSearchCache[fetchKey].RemoveRange(0, getK);
                completed = incrementalSearchCache[fetchKey].Count == 0;
            }
            if (completed) IncrementalSearchComplete(fetchKey);

            int[] results = new int[sortedLists.Count];
            float[] distances = new float[sortedLists.Count];
            for (int i = 0; i < sortedLists.Count; i++)
            {
                results[i] = sortedLists[i].Item1;
                distances[i] = sortedLists[i].Item2;
            }
            return (results.ToArray(), distances.ToArray(), completed);
        }

        public override void IncrementalSearchComplete(int fetchKey)
        {
            incrementalSearchCache.Remove(fetchKey);
        }

        protected override void ClearInternal()
        {
            embeddings.Clear();
            incrementalSearchCache.Clear();
        }

        protected override void SaveInternal(ZipArchive archive)
        {
            ArchiveSaver.Save(archive, embeddings, GetSavePath("embeddings"));
            ArchiveSaver.Save(archive, incrementalSearchCache, GetSavePath("incrementalSearchCache"));
        }

        protected override void LoadInternal(ZipArchive archive)
        {
            embeddings = ArchiveSaver.Load<SortedDictionary<int, float[]>>(archive, GetSavePath("embeddings"));
            incrementalSearchCache = ArchiveSaver.Load<Dictionary<int, List<(int, float)>>>(archive, GetSavePath("incrementalSearchCache"));
        }

        /// \endcond
    }
}


# -------------------- TokenSplitter.cs --------------------

/// @file
/// @brief File implementing a token-based splitter
using System;
using System.Collections.Generic;
using System.Threading.Tasks;
using UnityEngine;

namespace LLMUnity
{
    /// @ingroup rag
    /// <summary>
    /// Class implementing a token-based splitter
    /// </summary>
    [Serializable]
    public class TokenSplitter : Chunking
    {
        /// <summary> number of tokens by which to split phrases into chunks </summary>
        [Tooltip("number of tokens by which to split phrases into chunks")]
        public int numTokens = 10;

        protected int DetermineEndIndex(string input, string detokenised, int startIndex, int searchRange = 5, int charsFromEnd = 3)
        {
            int endIndex = Math.Min(input.Length - 1, startIndex + detokenised.Length - 1);
            if (endIndex == input.Length - 1) return endIndex;

            for (int lastCharI = 0; lastCharI < charsFromEnd; lastCharI++)
            {
                int charI = detokenised.Length - 1 - lastCharI;
                if (charI < 0) break;
                char lastChar = detokenised[charI];

                for (int i = 0; i < searchRange; i++)
                {
                    foreach (int mul in new int[] {-1, 1})
                    {
                        int inputCharI = endIndex + mul * i;
                        if (inputCharI < 0 || inputCharI > input.Length - 1) continue;
                        if (input[inputCharI] == lastChar) return inputCharI;
                    }
                }
            }
            return endIndex;
        }

        /// <summary>
        /// Splits the provided phrase into chunks of a specific number of tokens (defined by the numTokens variable)
        /// </summary>
        /// <param name="input">phrase</param>
        /// <returns>List of start/end indices of the split chunks</returns>
        public override async Task<List<(int, int)>> Split(string input)
        {
            List<(int, int)> indices = new List<(int, int)>();
            List<int> tokens = await search.Tokenize(input);
            if (tokens.Count == 0) return indices;

            int startIndex = 0;
            for (int i = 0; i < tokens.Count; i += numTokens)
            {
                int batchTokens = Math.Min(tokens.Count, i + numTokens) - i;
                string detokenised = await search.Detokenize(tokens.GetRange(i, batchTokens));
                int endIndex = DetermineEndIndex(input, detokenised, startIndex);
                indices.Add((startIndex, endIndex));
                startIndex = endIndex + 1;
                if (endIndex == input.Length - 1) break;
            }
            if (startIndex <= input.Length - 1) indices.Add((startIndex, input.Length - 1));
            return indices;
        }
    }
}


# -------------------- USearchException.cs --------------------

using System;

namespace Cloud.Unum.USearch
{
    public class USearchException : Exception
    {
        public USearchException(string message) : base(message) {}
    }
}


# -------------------- USearchIndex.cs --------------------

using System;
using System.IO;
using System.IO.Compression;
using System.Runtime.InteropServices;
using AOT;
using UnityEngine;
using static Cloud.Unum.USearch.NativeMethods;

namespace Cloud.Unum.USearch
{
    /// <summary>
    /// USearchIndex class provides a managed wrapper for the USearch library's index functionality.
    /// </summary>
    public class USearchIndex : IDisposable
    {
        private IntPtr _index;
        private bool _disposedValue = false;
        private ulong _cachedDimensions;

        public USearchIndex(
            MetricKind metricKind,
            ScalarKind quantization,
            ulong dimensions,
            ulong connectivity = 0,
            ulong expansionAdd = 0,
            ulong expansionSearch = 0,
            bool multi = false
                //CustomDistanceFunction? customMetric = null
        )
        {
            IndexOptions initOptions = new()
            {
                metric_kind = metricKind,
                metric = default,
                quantization = quantization,
                dimensions = dimensions,
                connectivity = connectivity,
                expansion_add = expansionAdd,
                expansion_search = expansionSearch,
                multi = multi
            };

            this._index = usearch_init(ref initOptions, out IntPtr error);
            HandleError(error);
            this._cachedDimensions = dimensions;
        }

        public USearchIndex(IndexOptions options)
        {
            this._index = usearch_init(ref options, out IntPtr error);
            HandleError(error);
            this._cachedDimensions = options.dimensions;
        }

        public USearchIndex(string path, bool view = false)
        {
            IndexOptions initOptions = new();
            this._index = usearch_init(ref initOptions, out IntPtr error);
            HandleError(error);

            if (view)
            {
                usearch_view(this._index, path, out error);
            }
            else
            {
                usearch_load(this._index, path, out error);
            }

            HandleError(error);

            this._cachedDimensions = this.Dimensions();
        }

        public void Save(string path)
        {
            usearch_save(this._index, path, out IntPtr error);
            HandleError(error);
        }

        public ulong Size()
        {
            ulong size = (ulong)usearch_size(this._index, out IntPtr error);
            HandleError(error);
            return size;
        }

        public ulong Capacity()
        {
            ulong capacity = (ulong)usearch_capacity(this._index, out IntPtr error);
            HandleError(error);
            return capacity;
        }

        public ulong Dimensions()
        {
            ulong dimensions = (ulong)usearch_dimensions(this._index, out IntPtr error);
            HandleError(error);
            return dimensions;
        }

        public ulong Connectivity()
        {
            ulong connectivity = (ulong)usearch_connectivity(this._index, out IntPtr error);
            HandleError(error);
            return connectivity;
        }

        public bool Contains(ulong key)
        {
            bool result = usearch_contains(this._index, key, out IntPtr error);
            HandleError(error);
            return result;
        }

        public int Count(ulong key)
        {
            int count = checked((int)usearch_count(this._index, key, out IntPtr error));
            HandleError(error);
            return count;
        }

        private void IncreaseCapacity(ulong size)
        {
            usearch_reserve(this._index, (UIntPtr)(this.Size() + size), out IntPtr error);
            HandleError(error);
        }

        private void CheckIncreaseCapacity(ulong size_increase)
        {
            ulong size_demand = this.Size() + size_increase;
            if (this.Capacity() < size_demand)
            {
                this.IncreaseCapacity(size_increase);
            }
        }

        public void Add(ulong key, float[] vector)
        {
            this.CheckIncreaseCapacity(1);
            usearch_add(this._index, key, vector, ScalarKind.Float32, out IntPtr error);
            HandleError(error);
        }

        public void Add(ulong key, double[] vector)
        {
            this.CheckIncreaseCapacity(1);
            usearch_add(this._index, key, vector, ScalarKind.Float64, out IntPtr error);
            HandleError(error);
        }

        public void Add(ulong[] keys, float[][] vectors)
        {
            this.CheckIncreaseCapacity((ulong)vectors.Length);
            for (int i = 0; i < vectors.Length; i++)
            {
                usearch_add(this._index, keys[i], vectors[i], ScalarKind.Float32, out IntPtr error);
                HandleError(error);
            }
        }

        public void Add(ulong[] keys, double[][] vectors)
        {
            this.CheckIncreaseCapacity((ulong)vectors.Length);
            for (int i = 0; i < vectors.Length; i++)
            {
                usearch_add(this._index, keys[i], vectors[i], ScalarKind.Float64, out IntPtr error);
                HandleError(error);
            }
        }

        public int Get(ulong key, out float[] vector)
        {
            vector = new float[this._cachedDimensions];
            int foundVectorsCount = checked((int)usearch_get(this._index, key, (UIntPtr)1, vector, ScalarKind.Float32, out IntPtr error));
            HandleError(error);
            if (foundVectorsCount < 1)
            {
                vector = null;
            }

            return foundVectorsCount;
        }

        public int Get(ulong key, int count, out float[][] vectors)
        {
            var flattenVectors = new float[count * (int)this._cachedDimensions];
            int foundVectorsCount = checked((int)usearch_get(this._index, key, (UIntPtr)count, flattenVectors, ScalarKind.Float32, out IntPtr error));
            HandleError(error);
            if (foundVectorsCount < 1)
            {
                vectors = null;
            }
            else
            {
                vectors = new float[foundVectorsCount][];
                for (int i = 0; i < foundVectorsCount; i++)
                {
                    vectors[i] = new float[this._cachedDimensions];
                    Array.Copy(flattenVectors, i * (int)this._cachedDimensions, vectors[i], 0, (int)this._cachedDimensions);
                }
            }

            return foundVectorsCount;
        }

        public int Get(ulong key, out double[] vector)
        {
            vector = new double[this._cachedDimensions];
            int foundVectorsCount = checked((int)usearch_get(this._index, key, (UIntPtr)1, vector, ScalarKind.Float64, out IntPtr error));
            HandleError(error);
            if (foundVectorsCount < 1)
            {
                vector = null;
            }

            return foundVectorsCount;
        }

        public int Get(ulong key, int count, out double[][] vectors)
        {
            var flattenVectors = new double[count * (int)this._cachedDimensions];
            int foundVectorsCount = checked((int)usearch_get(this._index, key, (UIntPtr)count, flattenVectors, ScalarKind.Float64, out IntPtr error));
            HandleError(error);
            if (foundVectorsCount < 1)
            {
                vectors = null;
            }
            else
            {
                vectors = new double[foundVectorsCount][];
                for (int i = 0; i < foundVectorsCount; i++)
                {
                    vectors[i] = new double[this._cachedDimensions];
                    Array.Copy(flattenVectors, i * (int)this._cachedDimensions, vectors[i], 0, (int)this._cachedDimensions);
                }
            }

            return foundVectorsCount;
        }

        public int Remove(ulong key)
        {
            int removedCount = checked((int)usearch_remove(this._index, key, out IntPtr error));
            HandleError(error);
            return removedCount;
        }

        public int Rename(ulong keyFrom, ulong keyTo)
        {
            int foundVectorsCount = checked((int)usearch_rename(this._index, keyFrom, keyTo, out IntPtr error));
            HandleError(error);
            return foundVectorsCount;
        }

        private static void HandleError(IntPtr error)
        {
            if (error != IntPtr.Zero)
            {
                throw new USearchException($"USearch operation failed: {Marshal.PtrToStringAnsi(error)}");
            }
        }

        private void FreeIndex()
        {
            if (this._index != IntPtr.Zero)
            {
                usearch_free(this._index, out IntPtr error);
                HandleError(error);
                this._index = IntPtr.Zero;
            }
        }

        public void Dispose()
        {
            this.Dispose(true);
            GC.SuppressFinalize(this);
        }

        protected virtual void Dispose(bool disposing)
        {
            if (!this._disposedValue)
            {
                this.FreeIndex();
                this._disposedValue = true;
            }
        }

        ~USearchIndex() => this.Dispose(false);

        //========================== Additional methods from LLMUnity ==========================//

        public static Func<int, int> FilterFunction;

        private static readonly object filterLock = new object();

        [MonoPInvokeCallback(typeof(NativeMethods.FilterCallback))]
        public static int StaticFilter(int key, System.IntPtr filterState)
        {
            if (FilterFunction != null) return FilterFunction(key);
            return 1;
        }

        private int Search<T>(T[] queryVector, int count, out ulong[] keys, out float[] distances, ScalarKind scalarKind, Func<int, int> filter = null)
        {
            keys = new ulong[count];
            distances = new float[count];

            GCHandle handle = GCHandle.Alloc(queryVector, GCHandleType.Pinned);
            int matches = 0;
            try
            {
                IntPtr queryVectorPtr = handle.AddrOfPinnedObject();
                IntPtr error;
                if (filter == null)
                {
                    matches = checked((int)usearch_search(this._index, queryVectorPtr, scalarKind, (UIntPtr)count, keys, distances, out error));
                }
                else
                {
                    if (Application.platform == RuntimePlatform.Android)
                    {
                        lock (filterLock)
                        {
                            FilterFunction = filter;
                            matches = checked((int)usearch_filtered_search(this._index, queryVectorPtr, scalarKind, (UIntPtr)count, StaticFilter, IntPtr.Zero, keys, distances, out error));
                        }
                    }
                    else
                    {
                        matches = checked((int)usearch_filtered_search(this._index, queryVectorPtr, scalarKind, (UIntPtr)count, (int key, IntPtr state) => filter(key), IntPtr.Zero, keys, distances, out error));
                    }
                }
                HandleError(error);
            }
            finally
            {
                handle.Free();
            }

            if (matches < count)
            {
                Array.Resize(ref keys, (int)matches);
                Array.Resize(ref distances, (int)matches);
            }

            return matches;
        }

        public int Search(float[] queryVector, int count, out ulong[] keys, out float[] distances, Func<int, int> filter = null)
        {
            return this.Search(queryVector, count, out keys, out distances, ScalarKind.Float32, filter);
        }

        public int Search(double[] queryVector, int count, out ulong[] keys, out float[] distances, Func<int, int> filter = null)
        {
            return this.Search(queryVector, count, out keys, out distances, ScalarKind.Float64, filter);
        }

        protected virtual string GetIndexFilename()
        {
            return "usearch/index";
        }

        public void Save(ZipArchive zipArchive)
        {
            string indexPath = Path.Combine(Path.GetTempPath(), Path.GetRandomFileName());
            Save(indexPath);
            try
            {
                zipArchive.CreateEntryFromFile(indexPath, GetIndexFilename());
            }
            catch (Exception ex)
            {
                Debug.LogError($"Error adding file to the zip archive: {ex.Message}");
            }
            File.Delete(indexPath);
        }

        public void Load(ZipArchive zipArchive)
        {
            IndexOptions initOptions = new();
            this._index = usearch_init(ref initOptions, out IntPtr error);
            HandleError(error);

            try
            {
                ZipArchiveEntry entry = zipArchive.GetEntry(GetIndexFilename());
                using (Stream entryStream = entry.Open())
                using (MemoryStream memoryStream = new MemoryStream())
                {
                    entryStream.CopyTo(memoryStream);
                    // Access the length and create a buffer
                    byte[] managedBuffer = new byte[memoryStream.Length];
                    memoryStream.Position = 0;  // Reset the position to the beginning
                    memoryStream.Read(managedBuffer, 0, managedBuffer.Length);

                    GCHandle handle = GCHandle.Alloc(managedBuffer, GCHandleType.Pinned);
                    try
                    {
                        IntPtr unmanagedBuffer = handle.AddrOfPinnedObject();
                        usearch_load_buffer(_index, unmanagedBuffer, (UIntPtr)managedBuffer.Length, out error);
                        HandleError(error);
                    }
                    finally
                    {
                        handle.Free();
                    }
                }
            }
            catch (Exception ex)
            {
                Debug.LogError($"Error loading the search index: {ex.Message}");
            }

            this._cachedDimensions = this.Dimensions();
        }
    }
}


# -------------------- USearchTypes.cs --------------------

using System;
using System.Runtime.InteropServices;

namespace Cloud.Unum.USearch
{
    public enum MetricKind : uint
    {
        Unknown = 0,
        Cos,
        Ip,
        L2sq,
        Haversine,
        Pearson,
        Jaccard,
        Hamming,
        Tanimoto,
        Sorensen,
    }

    public enum ScalarKind : uint
    {
        Unknown = 0,
        Float32,
        Float64,
        Float16,
        Int8,
        Byte1,
    }

    // TODO: implement custom metric delegate
    // Microsoft guides links:
    // 1) https://learn.microsoft.com/en-us/dotnet/standard/native-interop/best-practices
    // 2) https://learn.microsoft.com/en-us/dotnet/framework/interop/marshalling-a-delegate-as-a-callback-method
    // public delegate float CustomMetricFunction(IntPtr a, IntPtr b);

    [Serializable]
    [StructLayout(LayoutKind.Sequential)]
    public struct IndexOptions
    {
        public MetricKind metric_kind;
        public IntPtr metric;
        public ScalarKind quantization;
        public ulong dimensions;
        public ulong connectivity;
        public ulong expansion_add;
        public ulong expansion_search;

        [MarshalAs(UnmanagedType.Bool)]
        public bool multi;

        public IndexOptions(
            MetricKind metricKind = MetricKind.Unknown,
            IntPtr metric = default,
            ScalarKind quantization = ScalarKind.Unknown,
            ulong dimensions = 0,
            ulong connectivity = 0,
            ulong expansionAdd = 0,
            ulong expansionSearch = 0,
            bool multi = false
        )
        {
            this.metric_kind = metricKind;
            this.metric = default; // TODO: Use actual metric param, when support is added for custom metric delegate
            this.quantization = quantization;
            this.dimensions = dimensions;
            this.connectivity = connectivity;
            this.expansion_add = expansionAdd;
            this.expansion_search = expansionSearch;
            this.multi = multi;
        }
    }
}


# -------------------- WordSplitter.cs --------------------

/// @file
/// @brief File implementing a word-based splitter
using System;
using System.Collections.Generic;
using System.Threading.Tasks;
using UnityEngine;

namespace LLMUnity
{
    /// @ingroup rag
    /// <summary>
    /// Class implementing a word-based splitter
    /// </summary>
    [Serializable]
    public class WordSplitter : Chunking
    {
        /// <summary> number of words by which to split phrases into chunks </summary>
        [Tooltip("number of words by which to split phrases into chunks")]
        public int numWords = 10;

        /// <summary>
        /// Splits the provided phrase into chunks of a specific number of words (defined by the numWords variable)
        /// </summary>
        /// <param name="input">phrase</param>
        /// <returns>List of start/end indices of the split chunks</returns>
        public override async Task<List<(int, int)>> Split(string input)
        {
            bool IsBoundary(char c)
            {
                return Char.IsPunctuation(c) || Char.IsWhiteSpace(c);
            }

            List<(int, int)> indices = new List<(int, int)>();
            await Task.Run(() => {
                List<(int, int)> wordIndices = new List<(int, int)>();
                int startIndex = 0;
                int endIndex;
                for (int i = 0; i < input.Length; i++)
                {
                    if (i == input.Length - 1 || IsBoundary(input[i]))
                    {
                        while (i < input.Length - 1 && IsBoundary(input[i + 1])) i++;
                        endIndex = i;
                        wordIndices.Add((startIndex, endIndex));
                        startIndex = i + 1;
                    }
                }

                for (int i = 0; i < wordIndices.Count; i += numWords)
                {
                    int iTo = Math.Min(wordIndices.Count - 1, i + numWords - 1);
                    indices.Add((wordIndices[i].Item1, wordIndices[iTo].Item2));
                }
            });
            return indices;
        }
    }
}


